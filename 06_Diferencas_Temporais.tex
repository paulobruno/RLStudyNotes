\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}
\usepackage{makecell}

\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}

\usepackage{graphicx}
\graphicspath{{figures/}}

\usepackage{amsmath,amssymb}
\DeclareMathOperator*{\argmax}{argmax}

% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        \vskip 1em
        {\large \textbf{\@author} \par}
        \vskip 1em
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother


\title{Notas de estudo - Aprendizado por Reforço}
\author{Parte 06 - Diferenças Temporais}
\date{Paulo Bruno de Sousa Serafim - Out/19 - Jan/20}

\begin{document}

\maketitle

\section{Introdução}

    \subsection{Equação de atualização}
    
        \begin{equation}
            V(S_t) \ \leftarrow V(S_t) + \alpha \Big[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \Big]
        \end{equation}
    
        TD-error:
        
        \begin{equation}
            \delta_t \ \dot{=} \ R_{t+1} + \gamma V(S_{t+1}) - V(S_t)
        \end{equation}
    
\section{Sarsa}

    \textit{On-policy}

    Nesse caso utilizamos a mesma política estimada para escolher a ação do estado seguinte. Para simplificar a notação, omitimos a política $\pi$ da equação de atualização da função de valor-ação do \textit{Sarsa}:

    \begin{equation}
        Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \Big]
    \end{equation}
    
\section{Q-Learning}

    \textit{Off-policy}
    
    A política estimada não é utilizada para escolher a ação no estado seguinte. Em especial, se for utilizada uma política gulosa, temos o algoritmo \textit{Q-Learning}:

    \begin{equation}
        Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \Big]
    \end{equation}

\section{Expected Sarsa}

    \begin{equation}
    \begin{split}
        Q(S_t, A_t) & \leftarrow Q(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma \mathbb{E} \big[ Q(S_{t+1}, A_{t+1}) \mid S_{t+1} \big] - Q(S_t, A_t) \Big] \\
        & \leftarrow Q(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma \sum_a \pi(a \mid S_{t+1}) Q(S_{t+1},a) - Q(S_t, A_t) \Big]
    \end{split}
    \end{equation}
    
\section{Double Learning}

    \begin{equation}
    \begin{split}
        Q_{1}(S_t, A_t) \leftarrow Q_{1}(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma Q_{2}(S_{t+1}, \argmax_a Q_{1}(S_{t+1}, a)) - Q_{1}(S_t, A_t) \Big] \\
        Q_{2}(S_t, A_t) \leftarrow Q_{2}(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma Q_{1}(S_{t+1}, \argmax_a Q_{2}(S_{t+1}, a)) - Q_{2}(S_t, A_t) \Big]
    \end{split}
    \end{equation}
    
\end{document}