\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}
\usepackage{makecell}

\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}

\usepackage{graphicx}
\graphicspath{{figures/}}

% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        \vskip 1em
        {\large \textbf{\@author} \par}
        \vskip 1em
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother

\title{Aprendizado por Reforço}
\author{Aula 03 - Processo de Decisão de Markov (MDP)}
\date{Paulo Bruno de Sousa Serafim - Outubro 2019}

\begin{document}

\maketitle

\section{Caracterização da interação agente-ambiente}

    \begin{center}
    \begin{tikzpicture}[->,>=stealth', auto, node distance=2.0cm, very thick]
        \tikzstyle{state}=[fill=none,shape=rectangle,draw=black,text=black,rounded corners=0.1cm, inner sep=0.3cm]
        \tikzstyle{action}=[fill=none, draw=none, text=black, shape=circle, inner sep=0,outer sep=0.05cm, minimum size=0.0cm]
        
        \node[state]  (S1)                             {Agent};
        \node[state]  (S2) [below of=S1]               {Environment};
        \node[action] (S3) [left of=S2, xshift=-0.5cm] {};
        \node[action] (S4) [left of=S1, xshift=-0.7cm, yshift=0.14cm] {};
        \node[action] (S5) [left of=S4, xshift=0.5cm] {};
        
        \draw[->, ultra thick] (S1) -- node[below right, pos=1.0, yshift=-0.5cm, align=center] {action\\$A_t$} ++(3,0)    |- (S2);
        \draw[->, ultra thick] (S3) -- node[below left, pos=1.0, yshift=1.5cm, align=center] {state\\$S_t$} ++(-1.0,0) |- (S1.190);
        \draw[->, ultra thick] (S2) to node[above] {$S_{t+1}$} (S3);
        \draw[->, very thick] (S4) to node[above, align=center] {reward $R_t$} (S1.170);
        \draw[->, very thick] (S5) to node[above] {$R_{t+1}$} (S4);
    \end{tikzpicture}
    \end{center}

\section{Definição de MDP}

    \subsection{Estado}
    
    \subsection{Ação}
    
    \subsection{Probabilidade}
    
    \subsection{Recompensa}
    
\section{Visão gráfica de um MDP}

    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm, semithick]
      \tikzstyle{every state}=[fill=blue,draw=none,text=white]
    
      \node[initial,state] (A)                    {$s_1$};
      \node[state]         (B) [above right of=A] {$s_2$};
      \node[state]         (D) [below right of=A] {$s_3$};
      \node[state]         (C) [below right of=B] {$s_4$};
      \node[state]         (E) [right of=C]       {$s_5$};
    
      \path (A) edge              node {$a_1$} (B)
                edge              node {$a_2$} (C)
            (B) edge [loop above] node {$a_3$} (B)
                edge              node {$a_4$} (C)
            (C) edge              node {$a_5$} (D)
                edge [bend left]  node {$a_6$} (E)
            (D) edge [loop below] node {$a_7$} (D)
                edge              node {$a_8$} (A)
            (E) edge [bend left]  node {$a_9$} (D);
    \end{tikzpicture}


\section{Objetivos}

    \subsection{Recompensa}
    
    \subsection{Retorno}
    
\section{Funções de valor}

    \subsection{Função estado-valor}
    
    \subsection{Função ação-valor}

\section{Política}

    Definição de política.

    \section{Política ótima}
    
\section{Processo de Decisão de Markov Parcialmente Observável}

\end{document}