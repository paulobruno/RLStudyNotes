\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}
\usepackage{makecell}

\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}

\usepackage{graphicx}
\graphicspath{{figures/}}

% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        \vskip 1em
        {\large \textbf{\@author} \par}
        \vskip 1em
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother


\tikzset{
    state-node/.style={
        fill=none, shape=circle, draw=black, thick, text=black, minimum size=0.6cm},
    action-node/.style={
        fill=black, draw=none, text=white, shape=circle, inner sep=0.05cm, minimum size=0.2cm},
    reward-node/.style={
        fill=none, draw=black, text=black},
    hidden-node/.style={
        fill=none, draw=none, text=white, shape=circle, inner sep=0,outer sep=0, minimum size=0.0cm},
    action-label/.style={
        shape=circle, text=white, draw=none, fill=black, inner sep=0.05cm, minimum size=0.2cm, align=center, yshift=0.0cm, anchor=center},
    reward-label/.style={
        shape=rectangle, text=black, draw=black, fill=white, minimum size=0.5cm, align=center, yshift=0.0cm, anchor=center},
    hidden-edge/.style={
        text=white, draw=none, fill=none, inner sep=0,outer sep=0, minimum size=0.0cm},
}


\title{Aprendizado por Reforço}
\author{Aula 03 - Processo de Decisão de Markov (MDP)}
\date{Paulo Bruno de Sousa Serafim - Outubro 2019}

\begin{document}

\maketitle

\section{Caracterização da interação agente-ambiente}

    \begin{center}
    \begin{tikzpicture}[->,>=stealth', auto, node distance=2.0cm, very thick]
        \tikzstyle{state}=[fill=none,shape=rectangle,draw=black,text=black,rounded corners=0.1cm, inner sep=0.3cm]
        \tikzstyle{action}=[fill=none, draw=none, text=black, shape=circle, inner sep=0,outer sep=0.05cm, minimum size=0.0cm]
        
        \node[state]  (S1)                             {Agent};
        \node[state]  (S2) [below of=S1]               {Environment};
        \node[action] (S3) [left of=S2, xshift=-0.5cm] {};
        \node[action] (S4) [left of=S1, xshift=-0.7cm, yshift=0.14cm] {};
        \node[action] (S5) [left of=S4, xshift=0.5cm] {};
        
        \draw[->, ultra thick] (S1) -- node[below right, pos=1.0, yshift=-0.5cm, align=center] {action\\$A_t$} ++(3,0)    |- (S2);
        \draw[->, ultra thick] (S3) -- node[below left, pos=1.0, yshift=1.5cm, align=center] {state\\$S_t$} ++(-1.0,0) |- (S1.190);
        \draw[->, ultra thick] (S2) to node[above] {$S_{t+1}$} (S3);
        \draw[->, very thick] (S4) to node[above, align=center] {reward $R_t$} (S1.170);
        \draw[->, very thick] (S5) to node[above] {$R_{t+1}$} (S4);
    \end{tikzpicture}
    \end{center}

\section{Definição de MDP}

    \subsection{Estado}
    
    \subsection{Ação}
    
    \subsection{Probabilidade}
    
    \subsection{Recompensa}
    
\section{Visão gráfica de um MDP}

    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm, semithick]
      \tikzstyle{every state}=[fill=blue,draw=none,text=white]
    
      \node[initial,state] (A)                    {$s_1$};
      \node[state]         (B) [above right of=A] {$s_2$};
      \node[state]         (D) [below right of=A] {$s_3$};
      \node[state]         (C) [below right of=B] {$s_4$};
      \node[state]         (E) [right of=C]       {$s_5$};
    
      \path (A) edge              node {$a_1$} (B)
                edge              node {$a_2$} (C)
            (B) edge [loop above] node {$a_3$} (B)
                edge              node {$a_4$} (C)
            (C) edge              node {$a_5$} (D)
                edge [bend left]  node {$a_6$} (E)
            (D) edge [loop below] node {$a_7$} (D)
                edge              node {$a_8$} (A)
            (E) edge [bend left]  node {$a_9$} (D);
    \end{tikzpicture}


    
    \begin{center}
    \begin{tikzpicture}[->,>=stealth', auto, node distance=4.0cm, thick]
        \node[state-node]  (S1) {$s_1$};
        \node[state-node]  (S2) [below of=S1, xshift=-3.0cm] {$s_2$};
        \node[state-node]  (S3) [below of=S1, xshift= 3.0cm] {$s_3$};
        \node[hidden-node] (A1) [above of=S2, yshift=-1.5cm] {};
        \node[hidden-node] (A2) [below of=S1, yshift=2.5cm]  {};
        
        \draw[bend right=20,-] (S1) to node[action-label]           {$a_1$} (A1);
        \draw[-]               (S1) to node[action-label, pos=0.4]  {$a_2$} (A2);
        \draw[bend left=40]    (S1) to node[action-label, pos=0.25] {$a_3$} (S3);
        
        \draw[bend right=30]   (A1) to node[left, pos=0.2]  {$p_1$} (S2);
        \draw[bend left=30]    (A1) to node[right, pos=0.2] {$p_2$} (S2);
        \draw[bend left=40]    (A2) to node[left, pos=0.3]  {$p_3$} (S2);
        \draw[bend right=40]   (A2) to node[right, pos=0.3] {$p_4$} (S3);
        
        \draw[hidden-edge, bend right=30] (A1) to node[reward-label]           {$r_1$} (S2);
        \draw[hidden-edge, bend left=30]  (A1) to node[reward-label]           {$r_2$} (S2);
        \draw[hidden-edge, bend left=40]  (A2) to node[reward-label]           {$r_3$} (S2);
        \draw[hidden-edge, bend right=40] (A2) to node[reward-label]           {$r_4$} (S3);
        \draw[hidden-edge, bend left=40]  (S1) to node[reward-label, pos=0.65] {$r_5$} (S3);
    \end{tikzpicture}
    \end{center}


\section{Objetivos}

    \subsection{Recompensa}
    
    \subsection{Retorno}
    
\section{Funções de valor}

    \subsection{Função estado-valor}
    
    \subsection{Função ação-valor}

\section{Política}

    Definição de política.

    \section{Política ótima}
    
\section{Processo de Decisão de Markov Parcialmente Observável}

\end{document}