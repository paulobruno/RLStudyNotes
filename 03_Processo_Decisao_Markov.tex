\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2cm, bottom=2.5cm, left=3cm, right=3cm]{geometry}
\usepackage{makecell}

\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}

\usepackage{graphicx}
\graphicspath{{figures/}}

\title{Aprendizado por Reforço - Notas de aula}
\author{Paulo Bruno de Sousa Serafim}
\date{Setembro 2019}

\begin{document}

\maketitle

\section{Caracterização da interação agente-ambiente}

\section{Definição de MDP}

    \subsection{Estado}
    
    \subsection{Ação}
    
    \subsection{Probabilidade}
    
    \subsection{Recompensa}
    
\section{Visão gráfica de um MDP}

    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm, semithick]
      \tikzstyle{every state}=[fill=blue,draw=none,text=white]
    
      \node[initial,state] (A)                    {$s_1$};
      \node[state]         (B) [above right of=A] {$s_2$};
      \node[state]         (D) [below right of=A] {$s_3$};
      \node[state]         (C) [below right of=B] {$s_4$};
      \node[state]         (E) [right of=C]       {$s_5$};
    
      \path (A) edge              node {$a_1$} (B)
                edge              node {$a_2$} (C)
            (B) edge [loop above] node {$a_3$} (B)
                edge              node {$a_4$} (C)
            (C) edge              node {$a_5$} (D)
                edge [bend left]  node {$a_6$} (E)
            (D) edge [loop below] node {$a_7$} (D)
                edge              node {$a_8$} (A)
            (E) edge [bend left]  node {$a_9$} (D);
    \end{tikzpicture}


\section{Objetivos}

    \subsection{Recompensa}
    
    \subsection{Retorno}
    
\section{Funções de valor}

    \subsection{Função estado-valor}
    
    \subsection{Função ação-valor}

\section{Política}

    Definição de política.

    \section{Política ótima}
    
\section{Processo de Decisão de Markov Parcialmente Observável}

\end{document}