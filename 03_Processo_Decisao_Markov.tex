\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}

\usepackage{indentfirst}

\usepackage{graphicx}
\graphicspath{{figures/}}

\input{pb_rlstyle_tikzgraph.tex}


% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        \vskip 1em
        {\large \textbf{\@author} \par}
        \vskip 1em
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother


\title{Notas de estudo - Aprendizado por Reforço}
\author{Parte 03 - Processo de Decisão de Markov (MDP)}
\date{Paulo Bruno de Sousa Serafim - Out/19 - Jan/20}


\begin{document}

\maketitle

    \section{Caracterização da interação agente-ambiente}
    
        Inicialmente, vamos recapitular o problema de Aprendizado por Reforço. Um agente interage com o ambiente. No começo ele encontra-se em um estado, então executa uma ação no ambiente, que lhe dá uma recompensa e o leva para outro estado. 
    
        \subsection{Definções}
            
            \subsubsection{Agente}
            
                Entidade 
            
            \subsubsection{Ambiente}
            
                
            
            \subsubsection{Estado}
            \subsubsection{Ação}
            \subsubsection{Recompensa}
            \subsubsection{Trajetória}
            
                
            
        \subsection{Diagrama de interação}
        
            \textcolor{red}{a recompensa $R_t+1$ deve ser recebida antes de $S_t$}
        
            \begin{center}
                \rlinteraction
            \end{center}
            
    \section{Definição de MDP}
    
        Esse tipo de interação apresentado acima é formalizado matematicamente como um \emph{Processo de decisão de Markov} (MDP). \textcolor{red}{Os papéis do agente e ambiente são diferentes}.
    
        \subsection{Elementos de um MDP}
    
            \subsubsection{Estado}
            
                \begin{tikzpicture}
                    \node[state-node] (R) {$s_1$};
                \end{tikzpicture}
                
            \subsubsection{Ação}
            
                \begin{tikzpicture}
                    \node[action-node] (R) {$a_1$};
                \end{tikzpicture}
                
            \subsubsection{\textcolor{red}{Probabilidade de Transição de Estado}}
            
                $p_1$ \\
            
                Dessa forma, a distribuição de probabilidade $p$ caracteriza a \emph{dinâmica} de um MDP.
            
                Nota: duas notações são comuns ao para se referir à probabilidade:
                
                \begin{enumerate}
                    \item $p(s', r \mid s, a)$, que pode ser lida como ``chegar no estado $s'$ recebendo uma recompensa $r$ dado que está no estado $s$ e executa a ação $a$'';
                    \item $p(s, a, r, s')$, que pode ser lida como ``está no estado $s$, executa a ação $a$, recebe a recompensa $r$ e chega no estado $s'$.
                \end{enumerate}
                
            \subsubsection{Recompensa}
        
                \begin{tikzpicture}
                    \node[reward-node] (R) {$r_1$};
                \end{tikzpicture}
                
        \subsection{Diagrama de interação mais adequado a outros casos práticos}
        
            \subsubsection{Nota sobre \emph{observabilidade parcial}}

                Normalmente, assumimos que as entradas recebidas do ambiente contêm no estado todas as informações necessárias. Nem sempre isso é verdade, alguns aspectos importantes do estado podem não ser observáveis. Por exemplo, em um jogo cuja visão da tela é considerada como o ``estado'', inimigos atrás da câmera não são observados e portanto não fazem parte do ``estado'' de entrada. Nesses casos, a \emph{Propriedade de Markov} não é completamente satisfeita. 
                
                Esse tipo de problema caracteriza-se como um \emph{Processo de Decisão de Markov Parcialmente Observável (POMDP)}. De fato um estado contém todas as informações sobre o sistema, por isso nesse tipo de problema consideramos uma variação do estado chamada de \emph{observação}. Uma observação é uma função do estado que podem ou não se sobrepor.
                
                Tarefas próximas ao mundo real são quase exclusivamente \emph{POMDPs} e portanto têm grande importância. A abordagem utilizada para resolver ou mitigar os problemas relacionados aos \emph{POMDPs} será tratada mais adiante.

            \subsubsection{Origem da recompensa}
            
                Embora o agente interaja primariamente com o ambiente, a recompensa recebida não necessariamente vem dele. Por exemplo, um outro personagem/entidade presente na tarefa pode dar recompensas ao agente. Dessa forma, é importante notar que embora a recompensa seja na maioria das vezes dadas pelo ambiente, isso não é necessariamente verdade.

            \subsubsection{Diagrama}

                \begin{center}
                \begin{tikzpicture}[->,>=latex, auto, node distance=2.0cm, very thick, font=\small]
                    \tikzstyle{rect-node}=[fill=none,shape=rectangle,draw=black,text=black,rounded corners=0.1cm, inner sep=0.4cm]
                    \tikzstyle{hidden-node}=[fill=none, draw=none, text=black, shape=rectangle, inner sep=0,outer sep=0.05cm, minimum height=0.75cm]
                    
                    \node[rect-node] (Agent) {\normalsize Agent};
                    \node[rect-node] (Env) [right of=Agent, xshift=4.0cm] {\normalsize Environment};
                    \node[hidden-node] (Hidden) [below of=Env, xshift=-2.0cm, yshift=0.5cm] {};
                    \node[hidden-node] (UpHid) [above of=Hidden, yshift=-1.0cm] {};
                    \node[hidden-node] (DownHid) [below of=Hidden, yshift=1.0cm] {};
                    \node[hidden-node] (Reward) [below of=Env, xshift=0.25cm, yshift=0.5cm] {};
                    
                    \draw[thick, transform canvas={yshift=-0.25cm}] (Reward) to node[below, yshift=-0.1cm] 
                        {$R_{t+1}$} (Hidden);
                        
                    \draw[ultra thick, transform canvas={xshift=0.10cm}] (Env.260) to node[xshift=-0.5cm, yshift=-0.4cm] {$O_{t+1}$} ++(0,0) |- (Hidden.105);
                        
                    \draw[ultra thick] (Hidden.105) to node[above, xshift=-1.5cm, align=center] 
                        {observation\\$O_t$}  ++(0,0) -| (Agent.290);
                    \draw[thick] (Hidden.255) to node[below, xshift=-1.5cm, align=center] 
                        {reward\\$R_t$} ++(0,0) -| (Agent.250);
                        
                    \draw[ultra thick] (Agent) to node[xshift=3.5cm, align=center] 
                        {action\\$A_t$} ++(0,1.5) -| (Env);
                        
                    \draw[-, thick, dashed] (UpHid) to node {} (DownHid);
                \end{tikzpicture}
                \end{center}

        \subsection{Características de um MDP}
        
            \subsubsection{MDP's finitos e infinitos}
            
            \subsubsection{Propriedade de Markov}
            
            \subsubsection{Cadeia de Markov}
        
    \section{Visão gráfica de um MDP}
    
        Graficamente, um MDP é representado como um grafo direcionado em que os nós são pares estados e ações, e as arestas indicam o estado a ser alcançado:
    
        \begin{center}
        \mdpbig
        \end{center}
        
    \section{Exemplo de como representar um problema via MDP}
    
        \textcolor{red}{Escadas de Hogwarts}
    
    \section{Objetivos}
    
        \subsection{Hipótese da recompensa}
    
        \subsection{Retorno esperado}
        
        \subsection{Desconto}
        
            Propósito intuitivo: recompensa imediatas são mais importantes do que as futuras.
            
            Propósito formal: transformar retornos infinitos em finitos.
        
    \section{Política}
    
        Definição de política.
    
        \section{Política ótima}
        
    
    \section{Funções de valor}
        
        \subsection{Função estado-valor}
        
        \subsection{Função ação-valor}
        
        \subsection{Otimalidade das funções de valor}
        
            \subsubsection{Valor estado}
            
            \subsubsection{Valor ação}
            
            \subsubsection{Equações de Bellman}
        
    \section{Processo de Decisão de Markov Parcialmente Observável}


\end{document}