\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2cm, bottom=2.5cm, left=3cm, right=3cm]{geometry}
\usepackage{makecell}

\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}

\usepackage{graphicx}
\graphicspath{{figures/}}

\title{Aprendizado por Reforço - Notas de aula}
\author{Paulo Bruno de Sousa Serafim}
\date{Setembro 2019}

\begin{document}

\maketitle

\section{Recapitulação de MDP}

    Mostrar grafos de MDP's.
    
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm, semithick]
      \tikzstyle{every state}=[fill=red,draw=none,text=white]
    
      \node[initial,state] (A)                    {$s_1$};
      \node[state]         (B) [above right of=A] {$s_2$};
      \node[state]         (D) [below right of=A] {$s_3$};
      \node[state]         (C) [below right of=B] {$s_4$};
      \node[state]         (E) [right of=C]       {$s_5$};
    
      \path (A) edge              node {$a_1$} (B)
                edge              node {$a_2$} (C)
            (B) edge [loop above] node {$a_3$} (B)
                edge              node {$a_4$} (C)
            (C) edge              node {$a_5$} (D)
                edge [bend left]  node {$a_6$} (E)
            (D) edge [loop below] node {$a_7$} (D)
                edge              node {$a_8$} (A)
            (E) edge [bend left]  node {$a_9$} (D);
    \end{tikzpicture}

\section{Equações de otimalidade de Bellman}

    \subsection{Função de estado-valor}
    
    \subsection{Função de ação-valor}

\section{Avaliação de política}

    Também conhecido como o \textit{problema da predição}.
    
\section{Melhora da política}

    Também conhecido como o \textit{problema do controle}.

\section{Iteração da política}

    União da avaliação e da melhora.
    
\section{Iteração do valor}

\section{Iteração de Política Generalizada (GPI)}

\end{document}