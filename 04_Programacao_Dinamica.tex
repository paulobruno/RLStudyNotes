\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}
\usepackage{makecell}

\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}

\usepackage{graphicx}
\graphicspath{{figures/}}

\usepackage{amsmath,amssymb}

% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        \vskip 1em
        {\large \textbf{\@author} \par}
        \vskip 1em
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother


\title{Notas de estudo - Aprendizado por Reforço}
\author{Parte 04 - Programação Dinâmica}
\date{Paulo Bruno de Sousa Serafim - Out/19 - Jan/20}

\begin{document}

\maketitle

\section{Recapitulação de MDP}

    Mostrar grafos de MDP's.
    
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm, semithick]
      \tikzstyle{every state}=[fill=none,draw=black,text=black, thick]
      \tikzstyle{every edge}=[font=\small, align=center, draw=black]
    
      \node[state] (A)                              {$s$};
      \node[state] (B) [below of=A, xshift=-3.5cm]  {$s_1$};
      \node[state] (C) [below of=A]                 {$s_2$};
      \node[state] (D) [below of=A, xshift=3.5cm]   {$s_3$};
    
      \path (A) edge [bend right=45] node[left] {$a_1, p_1$\\ $r_1$}  (B)
                edge [bend right]    node[left] {$a_2, p_2$\\ $r_2$} (C)
                edge [bend left]     node[right] {$a_3, p_3$\\ $r_3$}  (C)
                edge [bend left=45]  node[right] {$a_4, p_4$\\ $r_4$} (D)
                edge [bend left=10]  node[above] {$a_4, p_5$\\ $r_5$} (D);
    \end{tikzpicture}

\section{Equações de otimalidade de Bellman}

    \subsection{Função de estado-valor}
    
        \begin{equation}
            \begin{split}
                v_*(s) & = \max_a \mathbb{E} \left[ R_{t+1} + \gamma v_*(S_{t+1}) \mid S_t = s, A_t = a \right] \\
                & = \max_a \sum_{s', r} p(s',r \mid s,a) \left[ r + \gamma v_*(s') \right]
            \end{split}
        \end{equation}
    
    \subsection{Função de ação-valor}

        \begin{equation}
            \begin{split}
                q_*(s,a) & = \mathbb{E} \left[ R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') \bigm\vert S_t = s, A_t = a \right] \\
                & = \sum_{s', r} p(s',r \mid s,a) \left[ r + \gamma \max_{a'}q_*(s', a') \right]
            \end{split}
        \end{equation}
        
\section{Avaliação de política}

    Também conhecido como o \textit{problema da predição}.
    
    \begin{equation}
        \begin{split}
            v_{\pi}(s) & \ \dot{=} \, \mathbb{E}_{\pi} \left[ G_t \mid S_t = s \right] \\
            & = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right] \\
            & = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s \right] \\
            & = \sum_a \pi(a \mid s) \sum_{s',r} p(s',r \mid s,a) \Big[ r + \gamma v_{\pi}(s') \Big]
        \end{split}
    \end{equation}
    
    \begin{equation}
        \begin{split}
            v_{k+1}(s) & \ \dot{=} \, \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_k(S_{t+1}) \mid S_t = s \right] \\
            & = \sum_a \pi(a \mid s) \sum_{s',r} p(s',r \mid s,a) \Big[ r + \gamma v_k(s') \Big]
        \end{split}
    \end{equation}
    
\section{Melhora da política}

    Também conhecido como o \textit{problema do controle}.
    
    \begin{equation}
        \begin{split}
            q_{\pi}(s,a) & \ \dot{=} \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s, A_t = a \right] \\
            & = \sum_{s',r} p(s', r \mid s, a) \Big[ r + \gamma v_{\pi}(s') \Big]
        \end{split}
    \end{equation}

\section{Iteração da política}

    União da avaliação e da melhora.
    
    \begin{center}
        \begin{math}
            \pi_0 \xrightarrow{\ \textrm{E} \ } 
            v_{\pi_0} \xrightarrow{\ \textrm{I} \ } 
            \pi_1 \xrightarrow{\ \textrm{E} \ } 
            v_{\pi_1} \xrightarrow{\ \textrm{I} \ } 
            \pi_2 \xrightarrow{\ \textrm{E} \ } 
            \cdots \xrightarrow{\ \textrm{I} \ }
            \pi_* \xrightarrow{\ \textrm{E} \ } v_{*}
        \end{math}
    \end{center}
    
\section{Iteração do valor}

    \begin{equation}
        \begin{split}
            v_{k+1}(s) & \  \dot{=} \ \max_a \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) \mid S_t = s, A_t = a] \\
            & = \ \max_a \sum_{s',r} p(s', r \mid s, a) \Big[ r + \gamma v_k(s') \Big]
        \end{split}
    \end{equation}

\section{Iteração de Política Generalizada (GPI)}

    \begin{center}
        \begin{tikzpicture}[->,>=latex,shorten >=1pt,auto,node distance=2.8cm, thick]
            \tikzstyle{state}=[fill=none,draw=none,text=black]
            
            \node[state] (A) {\Large$\boldsymbol{\pi}$};
            \node[state] (B) [right of=A, xshift=0.75cm] {\Large$\mathbf{V}$};
            \node[state] (DOTS) [below of=A, xshift=1.75cm, yshift=0.5cm] {\textbf{\vdots}};
            \node[state] (A2) [below of=A, yshift=-0.5cm] {\Large$\boldsymbol{\pi_{*}}$};
            \node[state] (B2) [below of=B, yshift=-0.5cm] {\Large$\mathbf{v_{*}}$};
        
            \draw[bend left=70] (A) to node[above] {evaluation} (B);
            \draw[bend left=60, draw=none] (A) to node[below] {$V 	\rightsquigarrow v_{\pi}$} (B);
            \draw[bend left=70] (B) to node[below] {improvement} (A);
            \draw[bend left=60, draw=none] (B) to node[above] {\small$\pi \rightsquigarrow$ greedy($V$)} (A);
            
            \draw[transform canvas={yshift=0.15cm}] (A2) to node[] {} (B2);
            \draw[transform canvas={yshift=-0.15cm}] (B2) to node[] {} (A2);
        \end{tikzpicture}
    \end{center}
\end{document}