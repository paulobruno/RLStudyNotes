\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}

\usepackage{amsmath,amssymb}

\input{pb_rlstyle_tikzgraph.tex}


% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        \vskip 1em
        {\large \textbf{\@author} \par}
        \vskip 1em
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother


\title{Notas de estudo - Aprendizado por Reforço}
\author{Parte 04 - Programação Dinâmica}
\date{Paulo Bruno de Sousa Serafim - Out/19 - Jan/20}


\begin{document}

\maketitle

    \section{Recapitulação de MDP}
    
        Considere o MDP abaixo.
        
        \begin{center}
        \begin{tikzpicture}[->,>=stealth', auto, node distance=4.0cm, thick]
            \node[state-node]  (S1) {$s_1$};
            \node[state-node]  (S2) [below of=S1, xshift=-3.0cm] {$s_2$};
            \node[state-node]  (S3) [below of=S1, xshift= 3.0cm] {$s_3$};
            \node[hidden-node] (A1) [above of=S2, yshift=-1.5cm] {};
            \node[hidden-node] (A2) [below of=S1, yshift=2.5cm]  {};
            
            \draw[bend right=20,-] (S1) to node[action-label]           {$a_1$} (A1);
            \draw[-]               (S1) to node[action-label, pos=0.4]  {$a_2$} (A2);
            \draw[bend left=40]    (S1) to node[action-label, pos=0.25] {$a_3$} (S3);
            
            \draw[bend right=30]   (A1) to node[left, pos=0.2]  {$p_1$} (S2);
            \draw[bend left=30]    (A1) to node[right, pos=0.2] {$p_2$} (S2);
            \draw[bend left=40]    (A2) to node[left, pos=0.3]  {$p_3$} (S2);
            \draw[bend right=40]   (A2) to node[right, pos=0.3] {$p_4$} (S3);
            
            \draw[hidden-edge, bend right=30] (A1) to node[reward-label]           {$r_1$} (S2);
            \draw[hidden-edge, bend left=30]  (A1) to node[reward-label]           {$r_2$} (S2);
            \draw[hidden-edge, bend left=40]  (A2) to node[reward-label]           {$r_3$} (S2);
            \draw[hidden-edge, bend right=40] (A2) to node[reward-label]           {$r_4$} (S3);
            \draw[hidden-edge, bend left=40]  (S1) to node[reward-label, pos=0.65] {$r_5$} (S3);
        \end{tikzpicture}
        \end{center}
        
        Perceba que sabemos todas as informações necessárias sobre o modelo, i.e., todos os estados existentes, todas as ações possíveis e, principalmente as probabilidades de transição. Com isso, podemos estimar um valor para o estado $s_1$ de acordo com as equações de Bellman.
    
    \section{Equações de otimalidade de Bellman}
    
        Recaptulando dos estudos de MDP, conhecemos duas funções de valor ótimo, valor-estado e valor-ação.
    
        \subsection{Função de valor-estado}
        
            \begin{equation}
                \begin{split}
                    v_*(s) & = \max_a \mathbb{E} \left[ R_{t+1} + \gamma v_*(S_{t+1}) \mid S_t = s, A_t = a \right] \\
                    & = \max_a \sum_{s', r} p(s',r \mid s,a) \left[ r + \gamma v_*(s') \right]
                \end{split}
            \end{equation}
        
        \subsection{Função de valor-ação}
    
            \begin{equation}
                \begin{split}
                    q_*(s,a) & = \mathbb{E} \left[ R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') \bigm\vert S_t = s, A_t = a \right] \\
                    & = \sum_{s', r} p(s',r \mid s,a) \left[ r + \gamma \max_{a'}q_*(s', a') \right]
                \end{split}
            \end{equation}
            
        Com essas equações, podemos avaliar um estado e um par estado-ação, desde que tenhamos todas as informações necessárias do modelo. Note que nos casos ótimos consideramos a ação escolhida como sendo a que retorna o maior valor. Caso não tenhamos a política ótima ou estejamos seguindo outra política $\pi$ qualquer, poderíamos usar as funções de valor para avaliar essa política. É o que faremos a seguir.
        
    \section{Avaliação de política}
    
        Também conhecido como o \textit{problema da predição}, a avaliação da política consiste em computar o valor de um estado seguindo uma determinada política $\pi$. Embora esse valor possa ser definido a partir da resolução de um sistema linear nos estados, isso pode ser muito custoso, dessa forma, aqui estamos interessados em computá-lo iterativamente.
        
        O valor de um estado $s$ seguindo uma política $\pi$ é dado por:
        
        \begin{equation}
            \begin{split}
                v_{\pi}(s) & \ \dot{=} \, \mathbb{E}_{\pi} \left[ G_t \mid S_t = s \right] \\
                & = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right] \\
                & = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s \right] \\
                & = \sum_a \pi(a \mid s) \sum_{s',r} p(s',r \mid s,a) \Big[ r + \gamma v_{\pi}(s') \Big]
            \end{split}
        \end{equation}
                
        A sua versão iterativa é dada por:
        
        \begin{equation}
            \begin{split}
                v_{k+1}(s) & \ \dot{=} \, \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_k(S_{t+1}) \mid S_t = s \right] \\
                & = \sum_a \pi(a \mid s) \sum_{s',r} p(s',r \mid s,a) \Big[ r + \gamma v_k(s') \Big]
            \end{split}
        \end{equation}
        
        Em que a aproximação inicial $v_0(s)$ pode ser escolhida arbitrariamente e o valor de um estado terminal é $0$.
        
        \subsection{Exemplo aplicado ao MDP inicial}
        
            \begin{equation}
                \begin{split}
                    v_\pi(s_1) &= \pi(a_1 \mid s_1) \cdot p(s_2, r_1 \mid s_1, a_1) \cdot \Big[ r_1 + \gamma \cdot v_\pi(s_2) \Big] \\
                               &+ \pi(a_1 \mid s_1) \cdot p(s_2, r_2 \mid s_1, a_1) \cdot \Big[ r_2 + \gamma \cdot v_\pi(s_2) \Big] \\
                               &+ \pi(a_2 \mid s_1) \cdot p(s_2, r_3 \mid s_1, a_2) \cdot \Big[ r_3 + \gamma \cdot v_\pi(s_2) \Big] \\
                               &+ \pi(a_2 \mid s_1) \cdot p(s_3, r_4 \mid s_1, a_2) \cdot \Big[ r_4 + \gamma \cdot v_\pi(s_3) \Big] \\
                               &+ \pi(a_3 \mid s_1) \cdot p(s_3, r_5 \mid s_1, a_3) \cdot \Big[ r_5 + \gamma \cdot v_\pi(s_3) \Big]
                \end{split}
            \end{equation}
        
    \section{Melhora da política}
    
        Também conhecido como o \textit{problema do controle}.
        
        \begin{equation}
            \begin{split}
                q_{\pi}(s,a) & \ \dot{=} \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s, A_t = a \right] \\
                & = \sum_{s',r} p(s', r \mid s, a) \Big[ r + \gamma v_{\pi}(s') \Big]
            \end{split}
        \end{equation}
    
    \section{Iteração da política}
    
        União da avaliação e da melhora.
        
        \begin{center}
            \begin{math}
                \pi_0 \xrightarrow{\ \textrm{E} \ } 
                v_{\pi_0} \xrightarrow{\ \textrm{I} \ } 
                \pi_1 \xrightarrow{\ \textrm{E} \ } 
                v_{\pi_1} \xrightarrow{\ \textrm{I} \ } 
                \pi_2 \xrightarrow{\ \textrm{E} \ } 
                \cdots \xrightarrow{\ \textrm{I} \ }
                \pi_* \xrightarrow{\ \textrm{E} \ } v_{*}
            \end{math}
        \end{center}
        
    \section{Iteração do valor}
    
        \begin{equation}
            \begin{split}
                v_{k+1}(s) & \  \dot{=} \ \max_a \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) \mid S_t = s, A_t = a] \\
                & = \ \max_a \sum_{s',r} p(s', r \mid s, a) \Big[ r + \gamma v_k(s') \Big]
            \end{split}
        \end{equation}
    
    \section{Iteração de Política Generalizada (GPI)}
    
        \begin{center}
            \begin{tikzpicture}[->,>=latex,shorten >=1pt,auto,node distance=2.8cm, thick]
                \tikzstyle{state}=[fill=none,draw=none,text=black]
                
                \node[state] (A) {\Large$\boldsymbol{\pi}$};
                \node[state] (B) [right of=A, xshift=0.75cm] {\Large$\mathbf{V}$};
                \node[state] (DOTS) [below of=A, xshift=1.75cm, yshift=0.5cm] {\textbf{\vdots}};
                \node[state] (A2) [below of=A, yshift=-0.5cm] {\Large$\boldsymbol{\pi_{*}}$};
                \node[state] (B2) [below of=B, yshift=-0.5cm] {\Large$\mathbf{v_{*}}$};
            
                \draw[bend left=70] (A) to node[above] {evaluation} (B);
                \draw[bend left=60, draw=none] (A) to node[below] {$V 	\rightsquigarrow v_{\pi}$} (B);
                \draw[bend left=70] (B) to node[below] {improvement} (A);
                \draw[bend left=60, draw=none] (B) to node[above] {\small$\pi \rightsquigarrow$ greedy($V$)} (A);
                
                \draw[transform canvas={yshift=0.15cm}] (A2) to node[] {} (B2);
                \draw[transform canvas={yshift=-0.15cm}] (B2) to node[] {} (A2);
            \end{tikzpicture}
        \end{center}
        
        
\end{document}