\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}
\usepackage{makecell}

\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{subcaption}

\input{pb_rlstyle_tikzgraph.tex}


% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        \vskip 1em
        {\large \textbf{\@author} \par}
        \vskip 1em
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother


\title{Notas de estudo - Aprendizado por Reforço}
\author{Parte 01 - Introdução ao Aprendizado por Reforço}
\date{Paulo Bruno de Sousa Serafim - Out/19 - Fev/20}


\begin{document}

\maketitle

    \section{Bibliografia}
    
        Richard S. Sutton and Andrew G. Barto. \textbf{Reinforcement Learning: An Introduction}. 2nd Edition. MIT Press, Cambridge, MA, 2018.
    
    \section{O que é ``reforço''?}
    
        A introdução dos termos reforço e punição parte do \emph{Behaviorismo} .
    
        \subsection{Behaviorismo}
        
            \subsubsection{Condicionamento Clássico}
            
                Reação \textbf{involuntária} a um estímulo treinado.
            
            \subsubsection{Condicionamento Operante}
            
                Açao \textbf{voluntária} para obter uma resposta desejável.
            
        \subsection{Reforço vs. Punição}
        
            \subsubsection{Reforço}
            
                Processo em que a ocorrência de um comportamento é fortalecida por uma consequência de sua ocorrência. Portanto, aumenta a frequência do comportamento.
            
            \subsubsection{Punição}
            
                Processo em que a ocorrência de um comportamento é enfraquecida por uma consequência de sua ocorrência. Portanto, diminui a frequência do comportamento.
                
            \subsubsection{Positivo}
            
                \begin{itemize}
                    \item Relacionado ao acréscimo ou adição de algo.
                \end{itemize}
            
            \subsubsection{Negativo}
    
                \begin{itemize}
                    \item Relacionado à ausência ou subtração de algo.
                \end{itemize}
            
            \subsubsection{Combinações Reforço/Punição $\times$ Positivo/Negativo}
            
                Dessa forma, temos 4 combinações possíveis, que são resumidas na Tabela~\ref{tab:reforco-punicao} e ilustradas na Figura~\ref{fig:reforco-punicao}.
            
                \begin{table}[ht]
                    \centering
                    \begin{tabular}{|c|c|c|}
                        \hline
                         & \textbf{Positivo} & \textbf{Negativo} \\
                        \hline
                        \textbf{Reforço} & \makecell{Aumenta a chance de ocorrência do \\ comportamento pela adição de estímulo} & \makecell{Aumenta a chance de ocorrência do \\ comportamento pela remoção de estímulo} \\
                        \hline
                        \textbf{Punição} & \makecell{Diminui a chance de ocorrência do \\ comportamento pela adição de estímulo} & \makecell{Diminui a chance de ocorrência do \\ comportamento pela remoção de estímulo} \\
                        \hline
                    \end{tabular}
                    \caption{Relação entre estímulo e chance de ocorrência de um comportamento}
                    \label{tab:reforco-punicao}
                \end{table}
            
                \begin{figure}[ht]
                    \centering
                    \begin{subfigure}[b]{.45\textwidth}
                        \centering
                        \raisebox{-0.5\height}{\includegraphics[width=200pt]{positive-negative-reinforcement.png}}
                        \caption{Reforço Positivo/Negativo}
                    \end{subfigure}
                    \begin{subfigure}[b]{.45\textwidth}
                        \centering
                        \raisebox{-0.5\height}{\includegraphics[width=180pt]{positive-negative-punishment_0.png}}
                        \caption{Punição Positiva/Negativa}
                    \end{subfigure}
                    \caption{Fonte: Shrestha (2017)}
                    \label{fig:reforco-punicao}
                \end{figure}
            
    \section{Exemplos}
    
        \begin{itemize}
            \item Cachorro que toca campainha para receber petisco;
            \item Criança que pega na tomada;
            \item Jogador que aprende a jogar algum jogo.
        \end{itemize}
            
    \section{Características}
    
        \begin{itemize}
            \item Aprendizado através da interação com o ambiente;
            \item Caracterizado pelo aprendizado por tentativa e erro
            \item Recompensas futuras (delayed rewards);
            \item Problema de decisão sequencial
        \end{itemize}
        
        \subsection{Tempo de resposta}
        
            Outra característica do Aprendizado por Reforço é a possível demora para receber uma resposta significativa. Por exemplo, em problemas de aprendizado por reforço a resposta é conhecida a cada iteração; já em problemas de otimização sabemos a distância para a função objetivo em todos os instantes. No caso do aprendizado por reforço temos tarefa de recompensas imediatas e tadias.
        
            \subsubsection{Recompensas imediatas}
        
                Em algumas tarefas, recompensas significativas são recebidas após cada iteração. Chamamos esse caso de \emph{recompensas imediatas}. Nesse caso, o agente poderá encontrar estratégias de curto prazo e ainda assim ser bem sucedido.
        
            \subsubsection{Recompensas tardia}
        
                Na maioria dos problemas o agente interage várias vezes com o ambiente antes de receber uma recompensa significativa. Por exemplo, em um jogo de xadrez cada jogador realizará dezenas de ações antes de saber a resposta final, vitória, derrota ou empate. Essa característica é chamada de \emph{recompensas tardias} (do inglês \emph{delayed rewards}). Para resolver esse tipo de problema é necessária a descoberta de estratégias de longo prazo.
        
    \section{Elementos}
    
        \begin{itemize}
            \item Ambiente;
            \item Modelo (opcional);
            \item Política;
            \item Função de valor;
            \item Recompensa.
        \end{itemize}
        
    \section{Diagrama de interação}
    
        \begin{center}
            \rlinteraction
        \end{center}
    
    \section{Breve história}
    
        Três correntes separadas.
        
        \subsection{Aprendizado por tentativa e erro (learning by trial)}
            Ideias iniciais de aprendizado por tentativa e erro na década de 1850;
            
            Discussão mais explícita a partir da década de 1890;
            
            Continua sendo desenvolvida pela psicologia no início do século XX;
            
            Primeiras implementações na década de 1950;
            
            Durante muito tempo, RL foi confundido com SL;
            
            Os termos “reforço” e “aprendizado por reforço” em problemas de tentativa e erro foram utilizados pela primeira vez na década de 1960.
    
        
        \subsection{Problemas de controle ótimo}
            Iniciado na década de 1950;
            
            Modelados como Processos de Decisão de Markov (MDP);
            
            Resolvidos através de técnicas de Programação Dinâmica.
    
        
        \subsection{Diferença temporal}
            Mais recente, fim dos anos 1980, com Watkins     (1989);
            
            Foi o que uniu as outras duas correntes, formando uma área só.
            
        \subsection{Marcos \emph{recentes} utilizando Aprendizado por Reforço}
        
            \begin{itemize}
                \item \textbf{2013}: Publicação no ArXiv do artigo seminal \emph{Playing Atari with Deep Reinforcement Learning} pela então \emph{DeepMind Technologies} \textcolor{red}{referenciar mnih 2013};
                \item \textbf{2015}: Publicação na revista \emph{Nature} do artigo \emph{Human-level control through deep reinforcement learning} pelo \emph{Google DeepMind}; \textcolor{red}{referenciar mnih 2015}
                \item \textbf{2017}: \emph{AlphaGo} vence Lee Sedol, campão mundial de Go;  \textcolor{red}{referenciar documentario alphago e o paper}
                \item \textbf{2019}: \emph{OpenAI Five} vence a equipe OG no jogo Dota2;  \textcolor{red}{referenciar openai five}
                \item \textbf{2019}: AlphaStar vence MaNa, jogador profissional de StarCraft II. \textcolor{red}{referenciar alphastar}
            \end{itemize}

    
\end{document}