\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}
\usepackage{makecell}

\usepackage[sorting=none]{biblatex}
\bibliography{references}
\usepackage{csquotes}

\usepackage{hyperref}

\usepackage{amsmath,amssymb}
\newcommand*{\RandomVariable}[1]{\mathbf{#1}}
\newcommand*{\ExpectedValue}{\mathbb{E}}

\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{subcaption}

\input{pb_rlstyle_tikzgraph.tex}


% text spacing
\linespread{1.3}
\setlength{\parindent}{4em}
\setlength{\parskip}{0.75em}


\newcommand{\todo}[1]{ --\textcolor{red}{\textbf{#1}}--}
%\newcommand{\todo}[1]{}


% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        %\vskip 0.5em
        {\large \textbf{\@author} \par}
        %\vskip 0.5em
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother


\title{Notas de estudo - Aprendizado por Reforço}
\author{Parte 01 - Introdução ao Aprendizado por Reforço}
\date{Paulo Bruno de Sousa Serafim - Out/19 - Mar/21}


\begin{document}

\maketitle

    \section{Avisos}
    
        \subsection{Bibliografia}
        
            Antes de qualquer coisa, fica o aviso: esses estudos foram quase totalmente baseados no livro de introdução ao Aprendizado por Reforço \cite{Sutton2018}
        
            \begin{center}
            \noindent\fbox{%
                \parbox{40em}{%
                    Richard S. Sutton and Andrew G. Barto. \textbf{Reinforcement Learning: An Introduction}. 2nd Edition. MIT Press, Cambridge, MA, 2018.
                }%
            }%
            \end{center}
            
            Naturalmente que consultei outras referências, mas, no geral, essas notas são notas de estudo desse livro. Sugiro a leitura dessas notas como uma introdução ao livro, portanto leia-o, que é gratuito.
            
            Mais ainda, por considerá-lo como uma fonte básica, não o cito explicitamente ao longo das notas, mas considere que ele é citado implicitamente em todas elas. Portanto, caso queira usar algum trecho encontrado aqui, na maioria dos casos, cite o livro.
            
        \subsection{Rigor matemático}
        
            Reforçando: essas notas foram feitas com o objetivo de estudos pessoais. Eu tentei escrevê-las em uma linguagem mais acessível para mim e certamente fui descuidado em alguns momentos com certos termos e definições. Dito isso, eu tento garantir um certo rigor matemático nas notas, mas não tanto. Por exemplo, algumas vezes posso mencionar que tal coisa foi provada como verdade e não pôr a prova ou sequer pôr uma referência a ela. Ou descrever algumas técnicas, mas omitir conceitos importantes.
            
            Outra questão relacionada é a notação utilizada. Utilizo uma notação semelhante à do livro base, mas não exatamente igual. Apesar disso, tento manter a consistência ao longo de todas as notas. Caso ocorram diferenças de notação, muito provavelmente foi um equívoco meu e agradeço muito a quem puder apontá-las.
            
        \subsection{Comunicação, sugestões e críticas}
            
            Ficarei \textbf{muito} agradecido em receber qualquer feedback, em especial sugestões de melhoria, correção de erros e críticas de modo geral. Peço, por gentileza, que se você tiver alguma coisa para dizer, crie uma \emph{issue} no github das notas
            \begin{center}
                \url{https://github.com/paulobruno/RLStudyNotes} .
            \end{center}
            Caso isso não seja possível, você pode me contatar da forma como for mais conveniente: LinkedIn, e-mail ou de qualquer outra forma.
            
    \section{O que é ``reforço''?}
    
        Antes de começar os estudos de Aprendizado por Reforço, vamos primeiro entender de onde vem o \emph{reforço}. Esse termo vem da psicologia e juntamente com o termo \emph{punição} foi introduzido no \emph{Behaviorismo}. Ambos são conceitos centrais do Behaviorismo e encapsulam em si a relação entre um estímulo e a sua consequência. Nesse sentido, a adição ou a remoção de uma recompensa faz com que a frequência de um comportamento aumente ou diminua.
    
        \subsection{Reforço vs. Punição}
        
            \subsubsection{Reforço}
            
                Processo em que a ocorrência de um comportamento é fortalecida por uma consequência de sua ocorrência. Portanto, aumenta a frequência do comportamento.
            
            \subsubsection{Punição}
            
                Processo em que a ocorrência de um comportamento é enfraquecida por uma consequência de sua ocorrência. Portanto, diminui a frequência do comportamento.
                
            \subsubsection{Positivo}
            
                \begin{itemize}
                    \item Relacionado ao acréscimo ou adição de algo.
                \end{itemize}
            
            \subsubsection{Negativo}
    
                \begin{itemize}
                    \item Relacionado à ausência ou subtração de algo.
                \end{itemize}
            
            \subsubsection{Combinações Reforço/Punição \texorpdfstring{$\times$}{TEXT} Positivo/Negativo}
            
                Dessa forma, temos 4 combinações possíveis, que são resumidas na Tabela~\ref{tab:reforco-punicao} e ilustradas na Figura~\ref{fig:reforco-punicao}.
            
                \begin{table}[ht]
                    \centering
                    \caption{Relação entre estímulo e chance de ocorrência de um comportamento.}
                    \label{tab:reforco-punicao}
                    \begin{tabular}{|c|c|c|}
                        \hline
                         & \textbf{Positivo} & \textbf{Negativo} \\
                        \hline
                        \textbf{Reforço} & \makecell{Aumenta a chance de ocorrência do \\ comportamento pela adição de estímulo} & \makecell{Aumenta a chance de ocorrência do \\ comportamento pela remoção de estímulo} \\
                        \hline
                        \textbf{Punição} & \makecell{Diminui a chance de ocorrência do \\ comportamento pela adição de estímulo} & \makecell{Diminui a chance de ocorrência do \\ comportamento pela remoção de estímulo} \\
                        \hline
                    \end{tabular}
                \end{table}
            
                \begin{figure}[ht]
                    \centering
                    \begin{subfigure}[b]{.45\textwidth}
                        \centering
                        \raisebox{-0.5\height}{\includegraphics[width=200pt]{positive-negative-reinforcement.png}}
                        \caption{Reforço Positivo/Negativo}
                    \end{subfigure}
                    \begin{subfigure}[b]{.45\textwidth}
                        \centering
                        \raisebox{-0.5\height}{\includegraphics[width=180pt]{positive-negative-punishment_0.png}}
                        \caption{Punição Positiva/Negativa}
                    \end{subfigure}
                    \caption{Exemplos ilustrativos de reforço e punição positivas e negativas. Fonte: \citeauthor{Shrestha2017} (\citeyear{Shrestha2017}) \cite{Shrestha2017}.}
                    \label{fig:reforco-punicao}
                \end{figure}
            
    \section{Exemplos}
    
        Um exemplo tradicional é o de um cachorro que toca uma campainha, recebe um petisco e passa a tocar seguidamente para receber mais petiscos (\emph{reforço positivo}). Outro caso bastante conhecido, e muito utilizado pelos pais como exemplo, é o da suposta criança que pega na tomada, leva um choque e então aprende a não fazer isso novamente (\emph{punição positiva}). Por fim, uma situação que vai nos ajudar bastante a ilustrar o Aprendizado por Reforço, e também muito utilizado em pesquisas, é a de um jogador aprendendo a jogar um jogo qualquer. Claro que nós podemos extrapolar os conceitos de Aprendizado por Reforço para outras tarefas não cotidianas, como problemas de estatística, engenharia e economia. De modo geral, os exemplos com jogos nos ajudam a rapidamente descrever um problema e visualizar a maneira como as técnicas de Aprendizado por Reforço o resolve.
            
    \section{Características}
    
        Existe um agente que interage com um ambiente. A interação acontece assim: o agente encontra-se em um estado (que foi passado a ele pelo ambiente), executa uma ação no ambiente e recebe uma recompensa do ambiente. Essa é a interação tradicional em aprendizado por reforço e é ilustrada pelo diagrama de interação da figura ... .
        
        O aprendizado por tentativa e erro é uma característica estudada na área da psicologia desde o século 19. Ela está relacionada com o processo de aprendizado em que alguém executa uma ação e aprende se a ação executada foi boa ou ruim de acordo com a resposta que ele receber. Caso a resposta seja positiva, há uma tendência de que o comportamento se repita (reforço). Caso a resposta seja negativa, a tendência é de que o comportamento seja suprimido (punição).
        
        O problema de decisão sequencial refere-se principalmente ao fato de que certo estados só são atingidos a partir de certos estados. Portanto, temos uma noção de sequência de estados.
        
        Recompensas tardias refere-se ao fato de que muitas ações podem não dar nenhuma resposta significativa, ou seja, vc só sabe se uma ação levou a uma sequência de eventos boa no futuro. Outra maneira de dizer isso é que somente ao perceber que uma sequência de eventos é boa é que podemos dizer que o início dessa sequência foi uma ação boa.
    
        \begin{itemize}
            \item Aprendizado através da interação com o ambiente;
            \item Caracterizado pelo aprendizado por tentativa e erro
            \item Recompensas futuras (delayed rewards);
            \item Problema de decisão sequencial
        \end{itemize}
        
        \subsection{Diferença de outros paradigmas}
        
            \todo{Além disso, o Aprendizado por Reforço é um terceiro paradigma de aprendizado diferente dos aprendizados supervisionado e não-supervisionado.\\
            No caso não-supervisionado, o objetivo é encontrar estruturas escondidas em coleções de dados.\\
            No caso supervisionado, o processo de aprendizado ocorre através de respostas instrutivas (instructive feedback), que indicam qual a resposta esperada.\\
            Já no RL, o aprendizado ocorre através de respostas avaliativas, que indicam um valor associado à ação.}
            
        \subsection{Tempo de resposta}
        
            Outra característica do Aprendizado por Reforço é a possível demora para receber uma resposta significativa. Por exemplo, em problemas de Aprendizado por Reforço a resposta é conhecida a cada iteração; já em problemas de otimização sabemos a distância para a função objetivo em todos os instantes. No caso do Aprendizado por Reforço temos tarefa de recompensas imediatas e tadias.
        
            \subsubsection{Recompensas imediatas}
        
                Em algumas tarefas, recompensas significativas são recebidas após cada iteração. Chamamos esse caso de \emph{recompensas imediatas}. Nesse caso, o agente poderá encontrar estratégias de curto prazo e ainda assim ser bem sucedido.
        
            \subsubsection{Recompensas tardia}
        
                Na maioria dos problemas o agente interage várias vezes com o ambiente antes de receber uma recompensa significativa. Por exemplo, em um jogo de xadrez cada jogador realizará dezenas de ações antes de saber a resposta final, vitória, derrota ou empate. Essa característica é chamada de \emph{recompensas tardias} (do inglês \emph{delayed rewards}). Para resolver esse tipo de problema é necessária a descoberta de estratégias de longo prazo.
        
    \section{Elementos}
    
        Nós temos uma entidade que é responsável pelo processo de entender o estado em que se encontra e executar uma ação. Essa entidade é o agente. 
    
        \begin{itemize}
            \item Ambiente;
            \item Modelo (opcional);
            \item Política;
            \item Função de valor;
            \item Recompensa.
        \end{itemize}
        
    \section{Diagrama de interação}
    
        \begin{center}
            \rlinteraction
        \end{center}
    
    \section{Breve história}
    
        Três correntes separadas.
        
        \subsection{Aprendizado por tentativa e erro (learning by trial)}
            Ideias iniciais de aprendizado por tentativa e erro na década de 1850;
            
            Discussão mais explícita a partir da década de 1890;
            
            Continua sendo desenvolvida pela psicologia no início do século XX;
            
            Primeiras implementações na década de 1950;
            
            Durante muito tempo, RL foi confundido com SL;
            
            Os termos “reforço” e “Aprendizado por Reforço” em problemas de tentativa e erro foram utilizados pela primeira vez na década de 1960.
    
        
        \subsection{Problemas de controle ótimo}
            Iniciado na década de 1950;
            
            Modelados como Processos de Decisão de Markov (MDP);
            
            Resolvidos através de técnicas de Programação Dinâmica.
    
        
        \subsection{Diferença temporal}
            Mais recente, fim dos anos 1980, com Watkins     (1989);
            
            Foi o que uniu as outras duas correntes, formando uma área só.
        
        \subsection{Deep Reinforcement Learning}
        
            Na última década, as pesquisas em Aprendizado por Reforço receberam um novo ânimo. Em 2013, pesquisadores da então startup \emph{DeepMind Technologies} publicaram um artigo em que uniam um algoritmo de Aprendizado por Reforço, \emph{Q-Learning}, com o uso de \emph{Deep Neural Networks} para jogar jogos de \emph{Atari 2600}. Essa pesquisa resultou não somente em uma técnica nova, chamada de \emph{Deep Q-Networks} (DQN), mas também deu início a uma área de pesquisa totalmente nova: \emph{Deep Reinforcement Learning} (DRL). 
            
            Desde então, muitas pesquisas surgiram nessa área. Em notas futuras, falaremos mais sobre DRL. Nas próximas notas, no entanto, veremos conceitos elementares de Aprendizado por Reforço, de modo a conhecer as técnicas e conceitos básicos antes de avançar para os modelos mais complexos de DRL. Para quem tiver a curiosidade de já procurar mais sobre o assunto, listo a seguir alguns dos marcos na área.
            
            \subsubsection{Marcos \emph{recentes} utilizando DRL}
            
                \begin{itemize}
                
                    \item \textbf{2013}: Publicação no ArXiv do artigo seminal \emph{\citetitle{Mnih2013}} pela então \emph{DeepMind Technologies}, que deu início à área de \emph{Deep Reinforcement Learning} \cite{Mnih2013};
                    
                    \item \textbf{2015}: Publicação na revista \emph{Nature} do artigo \emph{\citetitle{Mnih2015}}, pelo \emph{Google DeepMind} \cite{Mnih2015};
                    
                    \item \textbf{2017}: \emph{AlphaGo} vence Lee Sedol, campão mundial de Go \cite{Silver2016,AlphaGo2017};
                    
                    \item \textbf{2019}: \emph{OpenAI Five} vence equipes campeãs mundiais do jogo Dota2 \cite{Openai2019};
                    
                    \item \textbf{2019}: AlphaStar vence dois jogadores profissionais de StarCraft II e jogadores humanos nos servidores oficias do jogo \cite{Vinyals2019}.
                    
                \end{itemize}

    \section{Esperança}
            
        Ao longo dos estudos, um conhecimento básico em estatística seria muito útil, mas é essencial conhecer desde o início o conceito o de \emph{valor esperado}, ou \emph{esperança}. Por sua importância, e até mesmo simplicidade, ele será, de maneira resumida, apresentado.
        
        Considere uma variável aleatória $\RandomVariable{X}$ que pode assumir $k$ valores $x_1$, $x_2$, \dots, $x_k$. De acordo com uma distribuição de probabilidade $p$, cada um desses valores ocorre com uma probabilidade $p(x_1)$, $p(x_2)$, \dots, $p(x_k)$, respectivamente. A \emph{esperança} (ou \emph{valor esperado}) de $\RandomVariable{X}$ sob a distribuição $p$ é definida como:
        \begin{equation}
        \begin{aligned}
            \label{eq:state-value}
            \ExpectedValue_p[\RandomVariable{X}] & = \sum_{i=1}^k p(x_i) \cdot x_i \\
            & = p(x_1) \cdot x_1 + p(x_2) \cdot x_2 + \dots + p(x_k) \cdot x_k\ .
        \end{aligned}
        \end{equation}
        Para simplificar a notação, na maioria dos casos, o símbolo subscrito da distribuição de probabilidade $p$ é omitido, de modo que usamos somente $\ExpectedValue[\RandomVariable{X}]$.

        Como exemplo, considere um dado de seis faces não viciado, $\RandomVariable{D}$. Ele pode assumir um dos valores $\{1, 2, 3, 4, 5, 6\}$, cada um deles com probabilidade $\frac{1}{6}$. Assim, o valor esperado de $\RandomVariable{D}$ é:
        \begin{equation*}
        \begin{aligned}
            \ExpectedValue[\RandomVariable{D}] & = \frac{1}{6} \cdot 1 + \frac{1}{6} \cdot 2 + \frac{1}{6} \cdot 3 + \frac{1}{6} \cdot 4 + \frac{1}{6} \cdot 5 + \frac{1}{6} \cdot 6 \\
            & = 3.5\ .
        \end{aligned}
        \end{equation*}        
        Note que o valor esperado é a média ponderada dos valores possíveis, em que a ponderação é dada de pela distribuição de probabilidade.
    
        \todo{O objetivo maior em tarefas de Aprendizado por Reforço é executar as ações cujas recompensas são máximas. Mais ainda, como há um conjunto de probabilidades envolvidas, desejamos executar as ações que deem a maior recompensa esperada. Para isso, utilizaremos bastante, entre outras coisas, o \emph{princípio da utilidade máxima esperada}.}

        O \emph{princípio da utilidade máxima esperada} diz que ... Ele foi desenvolvido por Von-Neumman e Morgenstern ...
        
        Formalmente, o teorema de Von-Neumman-Morgenstern nos diz que (equação) ... Podemos simplificar esse conceito da seguinte forma: considere que $x$ siga uma certa distribuição de probabilidade e para cada possibilidade $i$ teremos um valor $x_i$ associado, dizemos que o \emph{valor esperado} de $x$ é:
        \begin{equation}
            \mathbb{E} \left [ x \right ] = \sum_{i} p(i) \cdot x_i
        \end{equation}
        
        Por exemplo: (olhar o exemplo do caderno azul)
    
    \printbibliography
    
\end{document}