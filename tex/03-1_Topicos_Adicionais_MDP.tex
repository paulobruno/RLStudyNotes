\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}

\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{subcaption}

\usepackage{amsmath,amssymb}
\DeclareMathOperator*{\argmax}{argmax}

\input{pb_rlstyle_tikzgraph.tex}

% text spacing
\linespread{1.3}
\setlength{\parindent}{4em}
\setlength{\parskip}{0.75em}


\newcommand{\todo}[1]{ --\textcolor{red}{\textbf{#1}}--}
%\newcommand{\todo}[1]{}


% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        %\vskip 0.5em
        {\large \textbf{\@author} \par}
        %\vskip 0.5em
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother


\title{Notas de estudo - Aprendizado por Reforço}
\author{Parte 03.1 - Tópicos Adicionais em Processos de Decisão de Markov}
\date{Paulo Bruno Serafim - Mar/20}



\begin{document}

\maketitle

\subsection{Características de um MDP}

    \subsubsection{MDP's finitos e infinitos}
    
    \subsubsection{Propriedade de Markov}
    
    \subsubsection{Cadeia de Markov}
            
    \section{Cadeia de Markov}
    
        \begin{center}
        \begin{tikzpicture}[->, >=stealth', auto, node distance=4.0cm, thick]
            \node[state-node] (S1) {$s_1$};
            \node[state-node] (S2) [right of=S1] {$s_2$};
            
            \draw[loop left, min distance=15mm] (S1) to node[reward-label, left, pos=0.1] {$0$} (S1);
            \draw[bend left] (S1) to node[reward-label, pos=0.3] {$1$} (S2);
            \draw[loop right, min distance=15mm] (S2) to node[reward-label, right, pos=0.1] {$1$} (S2);
            \draw[bend left] (S2) to node[reward-label, pos=0.3] {$0$} (S1);
        \end{tikzpicture}
        \end{center}
        
        \begin{center}
        \begin{tikzpicture}[->, >=stealth', auto, node distance=4.0cm, thick]
            \node[state-node] (S1) {$s_1$};
            \node[state-node] (S2) [right of=S1] {$s_2$};
            
            \draw[loop left, min distance=15mm] (S1) to node[above, pos=0.8] {$0.5$} (S1);
            \draw[bend left] (S1) to node[above, pos=0.7] {$0.5$} (S2);
            \draw[loop right, min distance=15mm] (S2) to node[below, pos=0.8] {$0.75$} (S2);
            \draw[bend left] (S2) to node[below, pos=0.7] {$0.25$} (S1);
            
            \draw[hidden-edge, loop left, min distance=15mm] (S1) to node[reward-label, left, pos=0.1] {$0$} (S1);
            \draw[hidden-edge, bend left] (S1) to node[reward-label, pos=0.3] {$1$} (S2);
            \draw[hidden-edge, loop right, min distance=15mm] (S2) to node[reward-label, right, pos=0.1] {$1$} (S2);
            \draw[hidden-edge, bend left] (S2) to node[reward-label, pos=0.3] {$0$} (S1);
        \end{tikzpicture}
        \end{center}
        
    \section{Processos de Decisão de Markov Parcialmente Observáveis}
    
        \subsection{Nota sobre \emph{observabilidade parcial}}

            Normalmente, assumimos que as entradas recebidas do ambiente contêm no estado todas as informações necessárias. Nem sempre isso é verdade, alguns aspectos importantes do estado podem não ser observáveis. Por exemplo, em um jogo cuja visão da tela é considerada como o ``estado'', inimigos atrás da câmera não são observados e portanto não fazem parte do ``estado'' de entrada. Nesses casos, a \emph{Propriedade de Markov} não é completamente satisfeita. 
            
            Esse tipo de problema caracteriza-se como um \emph{Processo de Decisão de Markov Parcialmente Observável (POMDP)}. De fato um estado contém todas as informações sobre o sistema, por isso nesse tipo de problema consideramos uma variação do estado chamada de \emph{observação}. Uma observação é uma função do estado que podem ou não se sobrepor.
            
            Tarefas próximas ao mundo real são quase exclusivamente \emph{POMDPs} e portanto têm grande importância. A abordagem utilizada para resolver ou mitigar os problemas relacionados aos \emph{POMDPs} será tratada mais adiante.

        \subsection{Origem da recompensa}
        
            Embora o agente interaja primariamente com o ambiente, a recompensa recebida não necessariamente vem dele. Por exemplo, um outro personagem/entidade presente na tarefa pode dar recompensas ao agente. Dessa forma, é importante notar que embora a recompensa seja na maioria das vezes dadas pelo ambiente, isso não é necessariamente verdade.

        \subsection{Diagrama}

            \begin{figure}[ht]
                \centering
                \rlinteractionpomdp
                \caption{Diagrama de interação de Aprendizado por Reforço considerando observabilidade parcial}
                \label{diag:po-rl}
            \end{figure}
\end{document}