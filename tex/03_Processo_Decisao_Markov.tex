\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}

\setcounter{secnumdepth}{4}

\usepackage{hyperref}
\usepackage{indentfirst}

\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{subcaption}

\usepackage{amsmath,amssymb}

\input{pb_rlstyle_tikzgraph.tex}

% text spacing
\linespread{1.3}
\setlength{\parindent}{4em}
\setlength{\parskip}{0.75em}


\newcommand{\todo}[1]{ --\textcolor{red}{\textbf{#1}}--}
%\newcommand{\todo}[1]{}


% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        {\large \textbf{\@author} \par}
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother


\title{Notas de estudo - Aprendizado por Reforço}
\author{Parte 03 - Processo de Decisão Markoviano e Funções de Valor}
\date{Paulo Bruno Serafim - Mar/20}


\begin{document}

\maketitle

    \section{Caracterização da interação agente-ambiente}
    
        Inicialmente, vamos recapitular o problema de Aprendizado por Reforço, visto nos estudos introdutórios. O processo de interação do agente com o ambiente ocorre, simplificadamente, da seguinte forma: no começo o agente encontra-se em um estado $S_t$, então executa uma ação $A_t$ no ambiente, que lhe dá uma recompensa $R_{t+1}$ e o leva para outro estado $S_{t+1}$. Esse processo simplificado é ilustrado na Figura~\ref{diag:classical-rl}.
                
        \begin{figure}[ht]
            \centering
            \vspace*{5mm}
            \rlinteraction
            \vspace*{-5mm}
            \caption{Diagrama de interação clássico de aprendizado por reforço. Fonte: Sutton and Barto, 2018}
            \label{diag:classical-rl}
        \end{figure}
        
        \subsection{Entidades}
            
            \subsubsection{Agente}
            
                O \emph{agente} é o tomador de decisões. Ele é o responsável por avaliar as informações presentes no estado e as recompensas esperadas, e então decidir qual ação executar. Ainda que considerado como uma única entidade, o agente pode ser composto por módulos distintos, cada um responsável por uma tarefa do procesos de tomada de decisão. Apesar disso, nos nossos estudos nos referenciamos sempre ao ``agente'', sem considerar possíveis divisões internas.
            
            \subsubsection{Ambiente}
            
                O \emph{ambiente} é a entidade com a qual o agente interage. Na representação tradicional, ele é o responsável por apresentar o estado atual, dar as recompensas e realizar a transição de estados após o agente executar uma ação. Em representações mais complexas, o ambiente pode conter outras entidades em si, ou não ser o responsável por todas as tarefas mencionadas. Entretanto, nesse estudo vamos nos ater à representação tradicional.
            
        \subsection{Trajetória}
        
            A interação básica do agente com o ambiente acontece da seguinte forma: o agente encontra-se no estado $S_t$, executa uma ação $A_t$, recebe uma recompensa $R_{t+1}$, vai para um estado $S_{t+1}$, executa uma ação $A_{t+1}$ e assim por diante.

            $S_t,\ A_t,\ R_{t+1},\ S_{t+1},\ A_{t+1},\ R_{t+2},\ S_{t+2},\ A_{t+2},\ R_{t+3},\ S_{t+3},\ A_{t+3},\ \dots$
            
            \noindent
            Essa sequência que representa a interação de um agente é chamada de \emph{trajetória}, como ilustrada na Figura~\ref{fig:trajectory}.

            \begin{figure}[ht]
                \centering
                \begin{tikzpicture}[-,>=stealth', auto, node distance=2.0cm, thick]
                    \node[state-node]  (S1) {$s_1$};
                    \node[hidden-node] (H1) [above right of=S1] {};
                    \node[state-node]  (S2) [below right of=H1] {$s_2$};
                    \node[hidden-node] (H2) [above right of=S2] {};
                    \node[state-node]  (S3) [below right of=H2] {$s_3$};
                    \node[hidden-node] (H3) [above right of=S3] {};
                    \node[state-node]  (S4) [below right of=H3] {$s_4$};
                    
                    \draw[-, bend left, out=30, in=140]  (S1) to node[action-label, pos=0.6] {$a_1$} (H1);
                    \draw[->, bend left, out=40, in=150] (H1) to node[reward-label, pos=0.4] {$r_2$} (S2);
                    \draw[-, bend left, out=30, in=140]  (S2) to node[action-label, pos=0.6] {$a_2$} (H2);
                    \draw[->, bend left, out=40, in=150] (H2) to node[reward-label, pos=0.4] {$r_3$} (S3);
                    \draw[-, bend left, out=30, in=140]  (S3) to node[action-label, pos=0.6] {$a_3$} (H3);
                    \draw[->, bend left, out=40, in=150] (H3) to node[reward-label, pos=0.4] {$r_4$} (S4);
                \end{tikzpicture}
                \caption{Trajetória da interação do agente com o ambiente ao longo de quatro estados.}
                \label{fig:trajectory}
            \end{figure}
                
    \section{Definição de MDP}
    
        A interação apresentada acima é formalizada matematicamente como um \emph{Processo de decisão de Markov} (MDP). Um MDP é um processo estocástico de tomada de decisão.
    
        \subsection{Elementos de um MDP}
    
            Um MDP consiste de quatro elementos: um conjunto de estados $S$; um conjunto de ações $A$; as probabilidades de transição de cada estado para os outros; e as recompensas recebidas após essas transições. Veremos brevemente cada um deles.
    
            \subsubsection{Estado}

                O \emph{estado} representa a situação em que o agente se encontra e contém todas as informações conhecidas sobre o ambiente no dado momento. Assim, o agente pode avaliar as informações e decidir quais ações executar. Um MDP contém um conjunto de estados $S$, que pode ser finito ou inifinito. Entretanto, só veremos exemplos de MDPs finitos. 
                
                Em questão de notação, representamos um único estado como $s_n$, em que $n$ varia de $1$ ao número de estados no conjunto, ou seja, a cardinalidade de $S$, que denotamos por $\vert S \vert$. A Figura~\ref{diag:state-node} ilustra como indicaremos estados em nossas representações gráficas de MDPs.

                \begin{figure}[ht]
                    \centering
                    \begin{tikzpicture}
                        \node[state-node] (R) {$s_1$};
                    \end{tikzpicture}
                    \caption{Nos nossos diagramas, um estado é simbolizado por um círculo branco com borda preta}
                    \label{diag:state-node}
                \end{figure}
                
            \subsubsection{Ação}
            
                Uma \emph{ação} é uma opção de comportamento a ser adotado por um agente no estado dado. Em modelos tradicionais, ela é a única via na qual o agente pode interferir no ambiente. Para um conjunto de ações $A$ em um dado estado, com $\vert A \vert$ ações possíveis, o agente executará somente uma ação a cada iteração.

                A notação usada aqui para uma ação é $a_n$, em que $n$ varia de $1$ a $\vert A \vert$. Outra notação, menos utilizada, é $A(s)$, que indica as ações possíveis em um estado $s$. Nos nossos diagramas de MDPs, simbolizaremos uma ação da forma mostrada na Figura~\ref{diag:action-node}.

                \begin{figure}[ht]
                    \centering
                    \begin{tikzpicture}
                        \node[action-node] (R) {$a_1$};
                    \end{tikzpicture}
                    \caption{Nos nossos diagramas, uma ação é simbolizada por um círculo preto com texto branco}
                    \label{diag:action-node}
                \end{figure}
                
            \subsubsection{Probabilidade de Transição de Estado}

                Uma vez que uma ação tenha sido executada pelo agente, ela poderá acarretar em diferentes resultados, i.e., levar a diferentes estados e dar diferentes recompensas. Cada um desses possíveis resultados possui uma probabilidade diferente de acontecer. A distribuição de probabilidade dos resultados de uma ação $a$ a partir de um estado $s$ indica as chances do agente receber uma recompensa $r$ e chegar em um novo estado $s'$. Dessa forma, as distribuições de probabilidade caracterizam a dinâmica de um MDP.

                Utilizamos $p_n$, para representar a probabilidade de um certo resultado. Nas equações, a probabilidade é vista como uma função $p(s, a, r, s')$, que pode ser lida como: o agente encontra-se no estado $s$, executa a ação $a$, recebe a recompensa $r$ e chega no estado $s'$. Nos diagramas, ela é simbolizada como na Figura~\ref{diag:prob-label}.

                \begin{figure}[ht]
                    \centering
                    $p_1$\\
                    \caption{Nos nossos diagramas, uma probabilidade é simbolizada por um texto preto sem borda}
                    \label{diag:prob-label}
                \end{figure}
                
            \subsubsection{Recompensa}
            
                A \emph{recompensa} é um sinal escalar dado ao agente após a realização de uma ação. Tradicionalmente, a recompensa é passada pelo ambiente, mas em representações mais complexas podem haver subentidades dentro do ambiente responsáveis por dar a recompensa. 

                Uma recompensa é representada por $r_n$, de acordo com a transição $(s, a, r, s')$ da qual ela faz parte. A Figura~\ref{diag:reward-label} mostra o símbolo utilizado nos diagramas de MDPs.
        
                \begin{figure}[ht]
                    \centering
                    \begin{tikzpicture}
                        \node[reward-node] (R) {$r_1$};
                    \end{tikzpicture}
                    \caption{Nos nossos diagramas, uma recompensa é simbolizada por um retângulo branco com borda preta}
                    \label{diag:reward-label}
                \end{figure}
        
    \section{Visão gráfica de um MDP}
    
        Graficamente, um MDP é representado como um grafo direcionado em que os nós são pares estados e ações, e as arestas indicam transições de estado. Utilizaremos todos os símbolos apresentados anteriormente, como mostrado na Figura~\ref{diag:mdp-big}.
    
        \begin{figure}[ht]
            \centering
            \mdpbig
            \caption{Exemplo de um MDP com múltiplos estados, ações e transições}
            \label{diag:mdp-big}
        \end{figure}
        
    %\section{Exemplo de como representar um problema via MDP}
    
    %    \textcolor{red}{Escadas de Hogwarts}
    
    \section{Objetivo do agente}
    
        \subsection{Hipótese da recompensa}
    
            Informalmente, o propósito de um agente é receber o máximo de recompensa possível em uma tarefa, como afirmada na \emph{hipótese da recompensa}, em tradução livre:
            \begin{center}
            \noindent\fbox{%
                \parbox{45em}{%
                    \vskip 0.25em
                    \parindent=2em\indent
                    \emph{Tudo o que queremos dizer com objetivos e propósitos pode ser bem entendido como a maximização do valor esperado da soma cumulativa de um sinal escalar recebido (chamado de recompensa)}
                    \vskip 0.25em
                }%
            }%
            \end{center}
            
            A hipótese acima foi sugerida por Michael Littman para os autores Sutton e Barto em comunicação pessoal, como eles explicam nas notas do capítulo 3 do livro. Uma discussão inicial promovida por Sutton é encontrada no endereço:
            
            \url{http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html}
            
        \subsection{Distribuição de Recompensas}

            Em um problema, o agente pode receber uma recompensa após cada ação realizada. Em outro, o agente pode receber recompensa somente após certas ações. Essa diferença pode existir independentemente do número de estados e ações. Embora do ponto de vista do aprendizado por reforço os dois problemas sejam modelados por um MDP, eles diferem na \emph{distribuição de recompensas}, que é dividida em duas categorias: \emph{densas} e \emph{esparsas}.
        
            \emph{Observação: não confundir distribuição de recompensas com o tempo de resposta visto nos estudos introdutórios. Enquanto a distribuição de recompensa considera a presença ou ausência de recompensa após ações, o tempo de resposta é relativo ao momento em que o agente recebe uma resposta significativa.}

            \subsubsection{Recompensas densas}
                
                Em problemas que dão recompensas frequentemente, por exemplo com uma recompensa após cada ação, temos uma distribuição caracterizada como de recompensas \emph{densas}, ilustrada na Figura~\ref{diag:dense-rewards}.

                \begin{figure}[ht]
                    \centering
                    \begin{tikzpicture}[-,>=stealth', auto, node distance=2.0cm, thick]
                        \node[state-node]  (S1) {$s_1$};
                        \node[hidden-node] (H1) [above right of=S1] {};
                        \node[state-node]  (S2) [below right of=H1] {$s_2$};
                        \node[hidden-node] (H2) [above right of=S2] {};
                        \node[state-node]  (S3) [below right of=H2] {$s_3$};
                        \node[hidden-node] (H3) [above right of=S3] {};
                        \node[state-node]  (S4) [below right of=H3] {$s_4$};
                        \node[]            (HN) [above right of=S4] {$\cdots$};
                        \node[state-node]  (SN) [below right of=HN] {$s_n$};
                        \node[]            (SH) [below of=HN, yshift=0.5cm] {$\cdots$};
                        
                        \draw[-, bend left, out=30, in=140]  (S1) to node[action-label, pos=0.6] {$a_1$} (H1);
                        \draw[->, bend left, out=40, in=150] (H1) to node[reward-label, pos=0.4] {$r_2$} (S2);
                        \draw[-, bend left, out=30, in=140]  (S2) to node[action-label, pos=0.6] {$a_2$} (H2);
                        \draw[->, bend left, out=40, in=150] (H2) to node[reward-label, pos=0.4] {$r_3$} (S3);
                        \draw[-, bend left, out=30, in=140]  (S3) to node[action-label, pos=0.6] {$a_3$} (H3);
                        \draw[->, bend left, out=40, in=150] (H3) to node[reward-label, pos=0.4] {$r_4$} (S4);
                        \draw[-, bend left, out=30, in=135]  (S4) to node[action-label] {$a_4$} (HN);
                        \draw[->, bend left, out=45, in=150] (HN) to node[reward-label] {$r_n$} (SN);
                    \end{tikzpicture}
                    $s_1,\ a_1,\ \boxed{\mathbf{r_2}},\ s_2,\ a_2,\ \boxed{\mathbf{r_3}},\ s_3,\ a_3,\ \boxed{\mathbf{r_4}},\ s_4,\ a_4,\ \dots\,\ \boxed{\mathbf{r_n}},\ s_n$
                    \caption{Nesse problema, as recompensas são recebidas imediatamente após as ações serem realizadas}
                    \label{diag:dense-rewards}
                \end{figure}
                    
            \subsubsection{Recompensas esparsas}
                            
                Já os problemas que dão recompensas esporadicamente, por exemplo com uma recompensa somente ao fim do episódio, são caracterizados como de recompensas \emph{esparsas}, ilustrado na Figura~\ref{diag:sparse-rewards}.

                \begin{figure}[ht]
                    \centering
                    \begin{tikzpicture}[-,>=stealth', auto, node distance=1.5cm, thick]
                        \node[state-node]  (S1) {$s_1$};
                        \node[hidden-node] (H1) [above right of=S1] {};
                        \node[state-node]  (S2) [below right of=H1] {$s_2$};
                        \node[hidden-node] (H2) [above right of=S2] {};
                        \node[state-node]  (S3) [below right of=H2] {$s_3$};
                        \node[hidden-node] (H3) [above right of=S3] {};
                        \node[state-node]  (S4) [below right of=H3] {$s_4$};
                        \node[]            (HN) [above right of=S4, xshift=0.5cm, yshift=0.3cm] {$\cdots$};
                        \node[state-node]  (SN) [below right of=HN, xshift=0.5cm, yshift=-0.3cm] {$s_n$};
                        \node[]            (SH) [below of=HN] {$\cdots$};
                        
                        \draw[->, bend left, out=40, in=150] (H1) to node {} (S2);
                        \draw[->, bend left, out=40, in=150] (H2) to node {} (S3);
                        \draw[->, bend left, out=40, in=150] (H3) to node {} (S4);
                        \draw[->, bend left, out=45, in=150] (HN) to node[reward-label] {$r_n$} (SN);
                        
                        \draw[-, bend left, out=30, in=140]  (S1) to node[action-label, pos=1.0] {$a_1$} (H1);
                        \draw[-, bend left, out=30, in=140]  (S2) to node[action-label, pos=1.0] {$a_2$} (H2);
                        \draw[-, bend left, out=30, in=140]  (S3) to node[action-label, pos=1.0] {$a_3$} (H3);
                        \draw[-, bend left, out=30, in=135]  (S4) to node[action-label] {$a_4$} (HN);
                    \end{tikzpicture}
                    $s_1,\ a_1,\ s_2,\ a_2,\ s_3,\ a_3,\ s_4,\ a_4,\ \dots\,\ \boxed{\mathbf{r_n}},\ s_n$
                    \caption{Nesse problema, as recompensas são recebidas após várias ações serem realizadas}
                    \label{diag:sparse-rewards}
                \end{figure}
                
        \subsection{Retorno}

            Como dito na \emph{hipótese da recompensa} anteriormente, o que desejamos é receber a maior recompensa possível. Considerando o objetivo como um valor numérico, definimos a soma das recompensas recebidas a partir de um tempo $t$ como o \emph{retorno} $G_t$:
            \begin{equation}
                G_t \ = \ R_{t+1} + R_{t+2} + R_{t+3} + \cdots\ .
            \end{equation}

            \subsubsection{Tarefas episódicas}

                Algumas tarefas são finitas e, portanto, terminam em um certo tempo. Consideramos o tempo final, ou \emph{terminal}, como $T$. Assim, um \emph{estado terminal} é representado por $S_T$ e a recompensa recebida ao chegar nesse estado é $R_T$.
                
                Para esses casos, dizemos as tarefas são \emph{episódicas}. Um \emph{episódio} é uma interação do agente com o ambiente que inicia em $t = 1$ e termina em $T$, onde $T$ pode ser diferente em episódios diferentes. Dessa forma, sabemos que nesses casos o retorno recebido a partir de $t$ é finito:
                \begin{equation}
                \begin{split}
                    G_t & \ = \ R_{t+1} + R_{t+2} + R_{t+3} + \cdots + R_T \\
                    & \ = \ \sum_{k=t+1}^{T} R_k\ .
                \end{split}
                \end{equation}

            \subsubsection{Tarefas contínuas}
        
                Também existem tarefas que a interação nunca acaba, que são chamadas de tarefas \emph{contínuas}. Nesses casos, o tempo final seria infinito, i.e., $T = \infty$. 

                Nesse casos, o retorno pode ser infinito:
                \begin{equation}
                    G_t = R_{t+1} + R_{t+2} + R_{t+3} + \cdots\ ,
                \end{equation}
                de modo que não conseguiremos maximizá-lo em um dado momento, pois ele sempre crescerá, independentemente da trajetória seguida pelo agente. Para resolver essa questão, utilizamos um conceito adicional chamado de \emph{desconto}.
        
                \paragraph{Desconto} \hspace{0pt}

                    O desconto, $\gamma$, com $0 \leq \gamma \leq 1$, é um valor que pode diminuir o valor da recompensa esperada ao longo do tempo. Essa redução do valor das recompensas tem dois propósitos.

                    \begin{itemize}
                        \item \textbf{Propósito intuitivo}: considerar que recompensas imediatas são mais importantes do que as futuras. Se pensarmos que a recompensa imediata $R_t$ será recebida com certeza, e quais ações serão possíveis no estado seguinte, temos uma boa ideia de qual será a recompensa seguinte $R_{t+1}$. Mas teremos uma noção menor de quais podem ser as recompensas para $R_{t+2}$. Da mesma forma, a recompensa $R_{t+3}$ é ainda mais nebulosa. Assim, quanto mais no futuro pensamos, menor é a nossa certeza a respeito da recompensa recebida. O desconto seria, nesse sentido, uma medida de incerteza, pois diminui o valor das recompensas futuras quanto mais a distância aumenta.
                        \item \textbf{Propósito formal}: transformar retornos infinitos em finitos. Se multiplicarmos as recompensas por valores cada vez menores, eventualmente a soma convergirá para um valor finito.
                    \end{itemize}

                    Dessa forma, podemos utilizar o desconto para termos o \emph{retorno descontado} de uma tarefa e, com $\gamma < 1$, o retorno finito de uma tarefa contínua. Para isso, fazemos as seguintes mudanças: a recompensa imediata não é descontada, a recompensa seguinte será descontada, a recompensa seguinte será descontada ao quadrado e assim por diante, como mostrado em \eqref{eq:discounted-rewards}.
                    \begin{equation}
                    \label{eq:discounted-rewards}
                    \begin{split}                    
                        G_t & \ = \ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \\
                        & \ = \ \sum_{k=0}^{\infty} \gamma^k R_{t + k + 1}\
                    \end{split}
                \end{equation}

            \subsubsection{Notação unificada}

                Uma alternativa para unificar a notação de retorno de tarefas episódicas e contínuas através do desconto é escrevê-lo como:
                \begin{equation}
                    G_t \ = \ \sum_{k=t+1}^{T} \gamma^{k-t-1} R_k\ ,
                \end{equation}
                em que $T$ é finito e $\gamma \leq 1$ para tarefas episódicas, e $T = \infty$ e $\gamma < 1$ para tarefas contínuas.
           
    \section{Política}
    
        Relembrando os estudos de multi-armed bandits, vimos a diferença entre tarefas não-associativas, cujos valores se mantêm constantes a cada iteração, associativas, cujos valores das ações mudam com o tempo, e o problema completo do aprendizado por reforço. Para o último, que agora representamos como um MDP, mencionamos a necessidade de mapear as informações atuais com as ações possíveis.

        Nesse contexto, precisamos encontrar uma relação entre um estado e as ações possíveis a partir dele. Em especial, devemos definir uma probabilidade de executar cada ação possível para cada estado. A função que faz o mapeamento entre os pares estao e ação, $(s, a)$, é chamada de \emph{política}. 
        
        Formalmente, a política é definida como uma distribuição de probabilidade entre as ações possíveis em um estado. Ela pode ser vista como uma função $\pi(s, a)$ cujo valor é a probabilidade do agente executar a ação $a$ uma que vez qle encontra-se no estado $s$. Na prática, a política nos diz qual ação executar a seguir considerando o conjunto de informações que temos no momento, o estado. Por exemplo, uma política ``execute a primeira ação'' seria definida por:
        \begin{equation}
            \pi(s, a) =
            \begin{cases}
                1, & \text{se}\ a = a_1 \mid A(s) = \{a_1, a_2, \dots, a_n\}\\
                0, & \text{c.c.}
            \end{cases}
            .
        \end{equation}
    
    \section{Funções de valor}
        
        Para avaliar os diferentes estados e ações, precisamos mensurá-los de alguma maneira. Para isso, definimos \emph{funções de valor} para eles.

        \subsection{Função de valor do estado}
        
            A \emph{função de valor do estado}, $v_{\pi}(s)$ é o retorno esperado a partir de um estado $s$ seguindo uma política $\pi$:
            \begin{equation}
            \label{eq:state-value}
            \begin{split}
                v_{\pi}(s) & \ = \ \mathbb{E}_{\pi}[G_t \mid S_t = s] \\
                & \ = \ \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} \mid S_t = s] \\
                & \ = \ \sum_{a} \pi(s,a) \sum_{s'} \sum_{r} p(s, a, r, s') \Big[r + \gamma \mathbb{E}_{\pi}[ G_{t+1} \mid S_{t+1} = s'] \Big]\\
                & \ = \ \sum_{a} \pi(s,a) \sum_{s',r} p(s, a, r, s') \Big[r + \gamma \mathbb{E}_{\pi}[ G_{t+1} \mid S_{t+1} = s'] \Big]\\
                & \ = \ \sum_{a} \pi(s,a) \sum_{s', r} p(s, a, r, s') [r + \gamma v_{\pi}(s')]\ .
            \end{split}
            \end{equation}
            Consideramos $t$ como um tempo qualquer e que o valor de um estado terminal é sempre $0$.

            A equação \eqref{eq:state-value} é a \emph{equação de Bellman} para $v_{\pi}$, que tem esse nome por causa de Richard Bellman, que as definiu em seus trabalhos de \emph{programação dinâmica}. Ela considera a relação entre o valor do estado atual $s$ e o valor dos estados subsequentes $s'$. Observe que para chegarmos ao valor de $s$ precisamos conhecer o valor de $s'$. Esse tipo de equação em que o valor atual depende de valores ``futuros'' é bastante característica de problemas de programação dinâmica e das equações de Bellman.
        
        \subsection{Função de valor da ação}
        
            De maneira similar, a \emph{função de valor da ação}, $q_{\pi}(s, a)$, é o retorno esperado ao sair de um estado $s$, executando uma ação $a$ e a partir daí seguir uma política $\pi$:
            \begin{equation}
            \label{eq:action-value}
            \begin{split}
                q_{\pi}(s, a) & \ = \ \mathbb{E}[G_t \mid S_t = s, A_t = a] \\
                & \ = \ \mathbb{E}[R_{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a] \\
                & \ = \ \sum_{s',r} p(s, a, r, s') \Big[r + \gamma \mathbb{E}_{\pi}[ G_{t+1} \mid S_{t+1} = s', A_{t+1} = a'] \Big]\\
                & \ = \ \sum_{s',r} p(s, a, r, s') [r + \gamma q_{\pi}(s', a')]\ .
            \end{split}
            \end{equation}

            Da mesma forma, \eqref{eq:action-value} é a \emph{equação de Bellman} para $q_{\pi}$.
        
    \section{Otimalidade}
    
        Uma vez que agora temos maneiras de mensurar estados e ações através das funções de valor, $v_{\pi}(s)$ e $q_{\pi}(s, a)$, podemos utilizar esse valores para tomar decisões que nos dê o maior retorno possível. Para tanto, o agente precisa encontrar estratégias que atinjam esse resultado. Satisfazendo isso, teremos os valores \emph{ótimos} da política e das funções de valor.

        \subsection{Política ótima}
        
            Apesar de existirem infinitas políticas para uma tarefa, somente um conjunto delas nos levará a trajetória de maior retorno possível. Por definição, uma política que nos leva uma trajetória de maior retorno possível em uma tarefa é chamada de \emph{política ótima}.

            Consideramos que uma política $\pi$ é melhor ou igual do que outra política $\pi'$ se o seu valor esperado é maior ou igual do que o valor esperado da outra para todos os estados. Ou seja:
            \begin{equation*}
                \pi \geq \pi'\ \text{se e somente se}\ v_{\pi}(s) \geq v_{\pi'}(s)\ \text{para todo}\ s \in S.
            \end{equation*}
            
            É importante notar que sempre existe ao menos uma política que é melhor ou igual do que todas as outras políticas, a qual chamamos de \emph{política ótima}, $\pi_*$. Se houver mais de uma política com valores máximos, todas elas são chamadas de políticas ótimas.
        
        \subsection{Função ótima de valor do estado}
        
            Conhecendo a definição de política ótima, podemos definir funções de valor ótimas de maneira semelhante. No caso da função de valor do estado, podemos definir uma \emph{função ótima de valor do estado}, $v_*$, através da política que dá o maior valor de estado possível:
            \begin{equation}
                v_*(s) = \max_{\pi} v_{\pi}(s)\ \forall s \in S.
            \end{equation}

            Uma vez que a política ótima nos dirá qual ação executar, podemos ainda definir a \emph{função ótima de valor do estado} através da escolha da ação que nos dá o maior retorno esperado:
            \begin{equation}
            \label{eq:optimal-state-value}
            \begin{split}
                v_*(s) & \ = \ \max_{a} \mathbb{E}_{\pi_*} [G_t \mid S_t = s, A_t = a] \\
                & \ = \ \max_{a} \mathbb{E}_{\pi_*} [R_{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a] \\
                & \ = \ \max_{a} \mathbb{E} [R_{t+1} + \gamma v_*(S_{t+1}) \mid S_t = s, A_t = a] \\
                & \ = \ \max_{a} \sum_{s', r} p(s, a, r, s') [r + \gamma v_*(s')]\ .
            \end{split}
            \end{equation}

            A equação \eqref{eq:optimal-state-value} é chamada de \emph{equação de otimalidade de Bellman} para $v_*$. 
        
        \subsection{Função ótima de valor da ação}
        
            De maneira similar a \emph{função ótima de valor da ação} também pode ser definida através da política que dá o maior valor da ação:
            \begin{equation}
                q_*(s, a) = \max_{\pi} q_{\pi}(s, a)\ \forall s \in S\ \text{e}\ \forall a \in A(s).
            \end{equation}

            Além disso, a \emph{equação de otimalidade de Bellman} para $q_*$ é dada por:
            \begin{equation}
            \label{eq:optimal-action-value}
            \begin{split}
                q_*(s, a) & \ = \ \mathbb{E} [R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, A_{t+1}) \mid S_t = s, A_t = a] \\
                & \ = \ \sum_{s', r} p(s, a, r, s') [r + \gamma \max_{a'} q_*(s', a')]\ .
            \end{split}
            \end{equation}

            Esses conjuntos de equações são maneiras ótimas de resolver problemas de aprendizado por reforço em que temos o modelo completo do problema. Para esses casos, podemos utilizar diretamente \emph{programação dinâmica}, como veremos a seguir.

\end{document}