\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}

\usepackage{indentfirst}

\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{subcaption}

\usepackage{amsmath,amssymb}

\input{pb_rlstyle_tikzgraph.tex}

% text spacing
\linespread{1.3}
\setlength{\parindent}{4em}
\setlength{\parskip}{0.75em}


\newcommand{\todo}[1]{ --\textcolor{red}{\textbf{#1}}--}
%\newcommand{\todo}[1]{}


% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        \vskip 1em
        {\large \textbf{\@author} \par}
        \vskip 1em
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother


\title{Notas de estudo - Aprendizado por Reforço}
\author{Parte 03 - Processo de Decisão de Markov (MDP)}
\date{Paulo Bruno de Sousa Serafim - Out/19 - Jan/20}


\begin{document}

\maketitle

    \section{Caracterização da interação agente-ambiente}
    
        Inicialmente, vamos recapitular o problema de Aprendizado por Reforço. Um agente interage com o ambiente. No começo ele encontra-se em um estado, então executa uma ação no ambiente, que lhe dá uma recompensa e o leva para outro estado. 
    
        \subsection{Definções}
            
            \subsubsection{Agente}
            
                Entidade 
            
            \subsubsection{Ambiente}
            
                
            
            \subsubsection{Estado}
            \subsubsection{Ação}
            \subsubsection{Recompensa}
            \subsubsection{Trajetória}
            
                
            
        \subsection{Diagrama de interação}
        
            \textcolor{red}{a recompensa $R_t+1$ deve ser recebida antes de $S_t$}
        
            \begin{figure}[ht]
                \centering
                \rlinteraction
                \caption{Diagrama de interação clássico de aprendizado por reforço}
                \label{diag:classical-rl}
            \end{figure}
            
                \section{Recompensas imediatas e tardias}
    
        ... é chamado de \emph{distribuição de recompensas}
    
        \subsection{Recompensas densas}
        
            \begin{figure}[ht]
                \centering
                \begin{tikzpicture}[-,>=stealth', auto, node distance=2.0cm, thick]
                    \node[state-node]  (S1) {$s_1$};
                    \node[hidden-node] (H1) [above right of=S1] {};
                    \node[state-node]  (S2) [below right of=H1] {$s_2$};
                    \node[hidden-node] (H2) [above right of=S2] {};
                    \node[state-node]  (S3) [below right of=H2] {$s_3$};
                    \node[hidden-node] (H3) [above right of=S3] {};
                    \node[state-node]  (S4) [below right of=H3] {$s_4$};
                    \node[]            (HN) [above right of=S4] {$\cdots$};
                    \node[state-node]  (SN) [below right of=HN] {$s_n$};
                    \node[]            (SH) [below of=HN, yshift=0.5cm] {$\cdots$};
                    
                    \draw[-, bend left, out=30, in=140]  (S1) to node[action-label, pos=0.6] {$a_1$} (H1);
                    \draw[->, bend left, out=40, in=150] (H1) to node[reward-label, pos=0.4] {$r_2$} (S2);
                    \draw[-, bend left, out=30, in=140]  (S2) to node[action-label, pos=0.6] {$a_2$} (H2);
                    \draw[->, bend left, out=40, in=150] (H2) to node[reward-label, pos=0.4] {$r_3$} (S3);
                    \draw[-, bend left, out=30, in=140]  (S3) to node[action-label, pos=0.6] {$a_3$} (H3);
                    \draw[->, bend left, out=40, in=150] (H3) to node[reward-label, pos=0.4] {$r_4$} (S4);
                    \draw[-, bend left, out=30, in=135]  (S4) to node[action-label] {$a_4$} (HN);
                    \draw[->, bend left, out=45, in=150] (HN) to node[reward-label] {$r_n$} (SN);
                \end{tikzpicture}
                $s_1,\ a_1,\ \boxed{\mathbf{r_2}},\ s_2,\ a_2,\ \boxed{\mathbf{r_3}},\ s_3,\ a_3,\ \boxed{\mathbf{r_4}},\ s_4,\ a_4,\ \dots\,\ \boxed{\mathbf{r_n}},\ s_n$
                \caption{As recompensas são recebidas imediatamente após as ações serem realizadas}
                \label{diag:immediate-rewards}
            \end{figure}
            
            temos um total de n recompensas, o que caracteriza um \todo{alguma coisa} de recompensas \emph{densas}.
        
        
        \subsection{Recompensas esparsas}
        
            \begin{figure}[ht]
                \centering
                \begin{tikzpicture}[-,>=stealth', auto, node distance=1.5cm, thick]
                    \node[state-node]  (S1) {$s_1$};
                    \node[hidden-node] (H1) [above right of=S1] {};
                    \node[state-node]  (S2) [below right of=H1] {$s_2$};
                    \node[hidden-node] (H2) [above right of=S2] {};
                    \node[state-node]  (S3) [below right of=H2] {$s_3$};
                    \node[hidden-node] (H3) [above right of=S3] {};
                    \node[state-node]  (S4) [below right of=H3] {$s_4$};
                    \node[]            (HN) [above right of=S4, xshift=0.5cm, yshift=0.3cm] {$\cdots$};
                    \node[state-node]  (SN) [below right of=HN, xshift=0.5cm, yshift=-0.3cm] {$s_n$};
                    \node[]            (SH) [below of=HN] {$\cdots$};
                    
                    \draw[->, bend left, out=40, in=150] (H1) to node {} (S2);
                    \draw[->, bend left, out=40, in=150] (H2) to node {} (S3);
                    \draw[->, bend left, out=40, in=150] (H3) to node {} (S4);
                    \draw[->, bend left, out=45, in=150] (HN) to node[reward-label] {$r_n$} (SN);
                    
                    \draw[-, bend left, out=30, in=140]  (S1) to node[action-label, pos=1.0] {$a_1$} (H1);
                    \draw[-, bend left, out=30, in=140]  (S2) to node[action-label, pos=1.0] {$a_2$} (H2);
                    \draw[-, bend left, out=30, in=140]  (S3) to node[action-label, pos=1.0] {$a_3$} (H3);
                    \draw[-, bend left, out=30, in=135]  (S4) to node[action-label] {$a_4$} (HN);
                \end{tikzpicture}
                $s_1,\ a_1,\ s_2,\ a_2,\ s_3,\ a_3,\ s_4,\ a_4,\ \dots\,\ \boxed{\mathbf{r_n}},\ s_n$
                \caption{As recompensas são recebidas após várias ações serem realizadas}
                \label{diag:delayed-rewards}
            \end{figure}
            
            Temos somente uma única recompensa ao fim do episódio, o que caracteriza um \todo{alguma coisa, olhar no sutton} de recompensas \emph{esparsas}.

            
    \section{Definição de MDP}
    
        Esse tipo de interação apresentado acima é formalizado matematicamente como um \emph{Processo de decisão de Markov} (MDP). \textcolor{red}{Os papéis do agente e ambiente são diferentes}.
    
        \subsection{Elementos de um MDP}
    
            \subsubsection{Estado}
            
                \begin{tikzpicture}
                    \node[state-node] (R) {$s_1$};
                \end{tikzpicture}
                
            \subsubsection{Ação}
            
                \begin{tikzpicture}
                    \node[action-node] (R) {$a_1$};
                \end{tikzpicture}
                
            \subsubsection{\textcolor{red}{Probabilidade de Transição de Estado}}
            
                $p_1$ \\
            
                Dessa forma, a distribuição de probabilidade $p$ caracteriza a \emph{dinâmica} de um MDP.
            
                Nota: duas notações são comuns ao para se referir à probabilidade:
                
                \begin{enumerate}
                    \item $p(s', r \mid s, a)$, que pode ser lida como ``chegar no estado $s'$ recebendo uma recompensa $r$ dado que está no estado $s$ e executa a ação $a$'';
                    \item $p(s, a, r, s')$, que pode ser lida como ``está no estado $s$, executa a ação $a$, recebe a recompensa $r$ e chega no estado $s'$.
                \end{enumerate}
                
            \subsubsection{Recompensa}
        
                \begin{tikzpicture}
                    \node[reward-node] (R) {$r_1$};
                \end{tikzpicture}
                
        \subsection{Diagrama de interação mais adequado a outros casos práticos}
        
            \subsubsection{Nota sobre \emph{observabilidade parcial}}

                Normalmente, assumimos que as entradas recebidas do ambiente contêm no estado todas as informações necessárias. Nem sempre isso é verdade, alguns aspectos importantes do estado podem não ser observáveis. Por exemplo, em um jogo cuja visão da tela é considerada como o ``estado'', inimigos atrás da câmera não são observados e portanto não fazem parte do ``estado'' de entrada. Nesses casos, a \emph{Propriedade de Markov} não é completamente satisfeita. 
                
                Esse tipo de problema caracteriza-se como um \emph{Processo de Decisão de Markov Parcialmente Observável (POMDP)}. De fato um estado contém todas as informações sobre o sistema, por isso nesse tipo de problema consideramos uma variação do estado chamada de \emph{observação}. Uma observação é uma função do estado que podem ou não se sobrepor.
                
                Tarefas próximas ao mundo real são quase exclusivamente \emph{POMDPs} e portanto têm grande importância. A abordagem utilizada para resolver ou mitigar os problemas relacionados aos \emph{POMDPs} será tratada mais adiante.

            \subsubsection{Origem da recompensa}
            
                Embora o agente interaja primariamente com o ambiente, a recompensa recebida não necessariamente vem dele. Por exemplo, um outro personagem/entidade presente na tarefa pode dar recompensas ao agente. Dessa forma, é importante notar que embora a recompensa seja na maioria das vezes dadas pelo ambiente, isso não é necessariamente verdade.

            \subsubsection{Diagrama}

            \begin{figure}[ht]
                \centering
                \rlinteractionpomdp
                \caption{Diagrama de interação de aprendizado por reforço considerando observabilidade parcial}
                \label{diag:po-rl}
            \end{figure}

        \subsection{Características de um MDP}
        
            \subsubsection{MDP's finitos e infinitos}
            
            \subsubsection{Propriedade de Markov}
            
            \subsubsection{Cadeia de Markov}
        
    \section{Visão gráfica de um MDP}
    
        Graficamente, um MDP é representado como um grafo direcionado em que os nós são pares estados e ações, e as arestas indicam o estado a ser alcançado:
    
        \begin{figure}[ht]
            \centering
            \mdpbig
            \caption{Exemplo de um MDP com diversas interações}
            \label{diag:mdp-big}
        \end{figure}
        
    \section{Exemplo de como representar um problema via MDP}
    
        \textcolor{red}{Escadas de Hogwarts}
    
    \section{Objetivos}
    
        \subsection{Hipótese da recompensa}
    
        \subsection{Retorno}

            \begin{equation}
                G_t \ = \ R_{t+1} + R_{t+2} + R_{t+3} + \cdots
            \end{equation}

            \subsubsection{Tarefas episódicas}

                \begin{equation}
                \begin{split}
                    G_t & \ = \ R_{t+1} + R_{t+2} + R_{t+3} + \cdots + R_T \\
                    & \ = \ \sum_{k=t+1}^{T} R_k
                \end{split}
                \end{equation}

            \subsubsection{Tarefas contínuas}
        
                \begin{equation}
                \begin{split}
                    G_t & \ = \ R_{t+1} + R_{t+2} + R_{t+3} + \cdots \\
                    & \ = \ \sum_{k=0}^{\infty} \gamma^k R_{t + k + 1}
                \end{split}
                \end{equation}
        
        \subsection{Desconto}
        
            Propósito intuitivo: recompensa imediatas são mais importantes do que as futuras.
            
            Propósito formal: transformar retornos infinitos em finitos.
        
    \section{Política}
    
        Definição de política.
    
        \section{Política ótima}
        
    
    \section{Funções de valor}
        
        \subsection{Função estado-valor}
        
            \begin{equation}
            \begin{split}
                v_{\pi}(s) & \ = \ \mathbb{E}[G_t \mid S_t = s] \\
                & \ = \ \mathbb{E}[R_{t+1} + \gamma G_{t+1} \mid S_t = s] \\
                & \ = \ \mathbb{E}[R_{t+1} + \gamma v_{\pi}(s') \mid S_t = s] \\
                & \ = \ \sum_{a} \pi(s,a) \sum_{s', r} p(s, a, r, s') [r + \gamma v_{\pi}(s')]
            \end{split}
            \end{equation}
        
        \subsection{Função ação-valor}
        
        \subsection{Otimalidade das funções de valor}
        
            \subsubsection{Valor estado}
            
                \begin{equation}
                \begin{split}
                    v_*(s) & \ = \ \max_{a} \mathbb{E}[G_t \mid S_t = s, A_t = a] \\
                    & \ = \ \max_{a} \sum_{s', r} p(s, a, r, s') [r + \gamma v_*(s')]
                \end{split}
                \end{equation}
            
            \subsubsection{Valor ação}
            
                \begin{equation}
                \begin{split}
                    q_*(s, a) & \ = \ \max_{a} \mathbb{E}[G_t \mid S_t = s, A_t = a] \\
                    & \ = \ \sum_{s', r} p(s, a, r, s') [r + \gamma \max_{a'} q_*(s', a')]
                \end{split}
                \end{equation}

\end{document}