\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}
\usepackage{hyperref}

\usepackage{indentfirst}

\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{subcaption}

\usepackage{amsmath,amssymb}

\input{pb_rlstyle_tikzgraph.tex}

% text spacing
\linespread{1.3}
\setlength{\parindent}{4em}
\setlength{\parskip}{0.75em}


\newcommand{\todo}[1]{ --\textcolor{red}{\textbf{#1}}--}
%\newcommand{\todo}[1]{}


% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        {\large \textbf{\@author} \par}
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother


\title{Notas de estudo - Aprendizado por Reforço}
\author{Parte 03 - Processo de Decisão Markoviano e Funções de Valor}
\date{Paulo Bruno Serafim - Mar/20}


\begin{document}

\maketitle

    \section{Caracterização da interação agente-ambiente}
    
        Inicialmente, vamos recapitular o problema de Aprendizado por Reforço. Um agente interage com o ambiente. No começo ele encontra-se em um estado, então executa uma ação no ambiente, que lhe dá uma recompensa e o leva para outro estado. 
    
        \subsection{Definções}
            
            \subsubsection{Agente}
            
                Entidade 
            
            \subsubsection{Ambiente}
            
                O ambiente é a entidade a qual o agente interage. Em uma representação tradicional, ele é o responsável por apresentar o estado atual, dar as recompensas e realizar a transição de estados após o agente executar uma ação. Em representações mais complexas, o ambiente pode conter outras entidades em si, ou não ser o responsável por todas as tarefas mencionadas. Entretanto, nesse estudo vamos nos ater à representação tradicional.
            
            \subsubsection{Estado}
            \subsubsection{Ação}
            \subsubsection{Recompensa}
            
                A recompensa é um sinal escalar possivelmente dada ao agente após a realização de uma ação. Tradicionalmente, a recompensa é passada pelo ambiente, mas em representações mais complexas podem haver entidades dentro do ambiente responsáveis por dar a recompensa. 
            
            \subsubsection{Trajetória}
            
                A interação básica do agente com o ambiente acontece da seguinte forma: o agente encontra-se no estado $S_t$, executa uma ação $A_t$, recebe uma recompensa $R_{t+1}$, vai para um estado $S_{t+1}$, executa uma ação $A_{t+1}$ e assim por diante:

                $S_t,\ A_t,\ R_{t+1},\ S_{t+1},\ A_{t+1},\ R_{t+2},\ S_{t+2},\ A_{t+2},\ R_{t+3},\ S_{t+3},\ A_{t+3},\ \dots$
                
                Essa sequência que representa a interação de um agente é chamada de \emph{trajetória}, como ilustrada na Figura~\ref{fig:trajectory}.

                \begin{figure}[ht]
                    \centering
                    \begin{tikzpicture}[-,>=stealth', auto, node distance=2.0cm, thick]
                        \node[state-node]  (S1) {$s_1$};
                        \node[hidden-node] (H1) [above right of=S1] {};
                        \node[state-node]  (S2) [below right of=H1] {$s_2$};
                        \node[hidden-node] (H2) [above right of=S2] {};
                        \node[state-node]  (S3) [below right of=H2] {$s_3$};
                        \node[hidden-node] (H3) [above right of=S3] {};
                        \node[state-node]  (S4) [below right of=H3] {$s_4$};
                        
                        \draw[-, bend left, out=30, in=140]  (S1) to node[action-label, pos=0.6] {$a_1$} (H1);
                        \draw[->, bend left, out=40, in=150] (H1) to node[reward-label, pos=0.4] {$r_2$} (S2);
                        \draw[-, bend left, out=30, in=140]  (S2) to node[action-label, pos=0.6] {$a_2$} (H2);
                        \draw[->, bend left, out=40, in=150] (H2) to node[reward-label, pos=0.4] {$r_3$} (S3);
                        \draw[-, bend left, out=30, in=140]  (S3) to node[action-label, pos=0.6] {$a_3$} (H3);
                        \draw[->, bend left, out=40, in=150] (H3) to node[reward-label, pos=0.4] {$r_4$} (S4);
                    \end{tikzpicture}
                    \caption{Trajetória da interação do agente com o ambiente ao longo de quatro estados}
                    \label{fig:trajectory}
                \end{figure}
                
        \subsection{Diagrama de interação}
        
            \textcolor{red}{a recompensa $R_t+1$ deve ser recebida antes de $S_t$}
        
            \begin{figure}[ht]
                \centering
                \rlinteraction
                \caption{Diagrama de interação clássico de aprendizado por reforço}
                \label{diag:classical-rl}
            \end{figure}
            
                \section{Recompensas imediatas e tardias}
    
        ... é chamado de \emph{distribuição de recompensas}
    
        \subsection{Recompensas densas}
        
            \begin{figure}[ht]
                \centering
                \begin{tikzpicture}[-,>=stealth', auto, node distance=2.0cm, thick]
                    \node[state-node]  (S1) {$s_1$};
                    \node[hidden-node] (H1) [above right of=S1] {};
                    \node[state-node]  (S2) [below right of=H1] {$s_2$};
                    \node[hidden-node] (H2) [above right of=S2] {};
                    \node[state-node]  (S3) [below right of=H2] {$s_3$};
                    \node[hidden-node] (H3) [above right of=S3] {};
                    \node[state-node]  (S4) [below right of=H3] {$s_4$};
                    \node[]            (HN) [above right of=S4] {$\cdots$};
                    \node[state-node]  (SN) [below right of=HN] {$s_n$};
                    \node[]            (SH) [below of=HN, yshift=0.5cm] {$\cdots$};
                    
                    \draw[-, bend left, out=30, in=140]  (S1) to node[action-label, pos=0.6] {$a_1$} (H1);
                    \draw[->, bend left, out=40, in=150] (H1) to node[reward-label, pos=0.4] {$r_2$} (S2);
                    \draw[-, bend left, out=30, in=140]  (S2) to node[action-label, pos=0.6] {$a_2$} (H2);
                    \draw[->, bend left, out=40, in=150] (H2) to node[reward-label, pos=0.4] {$r_3$} (S3);
                    \draw[-, bend left, out=30, in=140]  (S3) to node[action-label, pos=0.6] {$a_3$} (H3);
                    \draw[->, bend left, out=40, in=150] (H3) to node[reward-label, pos=0.4] {$r_4$} (S4);
                    \draw[-, bend left, out=30, in=135]  (S4) to node[action-label] {$a_4$} (HN);
                    \draw[->, bend left, out=45, in=150] (HN) to node[reward-label] {$r_n$} (SN);
                \end{tikzpicture}
                $s_1,\ a_1,\ \boxed{\mathbf{r_2}},\ s_2,\ a_2,\ \boxed{\mathbf{r_3}},\ s_3,\ a_3,\ \boxed{\mathbf{r_4}},\ s_4,\ a_4,\ \dots\,\ \boxed{\mathbf{r_n}},\ s_n$
                \caption{As recompensas são recebidas imediatamente após as ações serem realizadas}
                \label{diag:immediate-rewards}
            \end{figure}
            
            temos um total de n recompensas, o que caracteriza um \todo{alguma coisa} de recompensas \emph{densas}.
        
        
        \subsection{Recompensas esparsas}
        
            \begin{figure}[ht]
                \centering
                \begin{tikzpicture}[-,>=stealth', auto, node distance=1.5cm, thick]
                    \node[state-node]  (S1) {$s_1$};
                    \node[hidden-node] (H1) [above right of=S1] {};
                    \node[state-node]  (S2) [below right of=H1] {$s_2$};
                    \node[hidden-node] (H2) [above right of=S2] {};
                    \node[state-node]  (S3) [below right of=H2] {$s_3$};
                    \node[hidden-node] (H3) [above right of=S3] {};
                    \node[state-node]  (S4) [below right of=H3] {$s_4$};
                    \node[]            (HN) [above right of=S4, xshift=0.5cm, yshift=0.3cm] {$\cdots$};
                    \node[state-node]  (SN) [below right of=HN, xshift=0.5cm, yshift=-0.3cm] {$s_n$};
                    \node[]            (SH) [below of=HN] {$\cdots$};
                    
                    \draw[->, bend left, out=40, in=150] (H1) to node {} (S2);
                    \draw[->, bend left, out=40, in=150] (H2) to node {} (S3);
                    \draw[->, bend left, out=40, in=150] (H3) to node {} (S4);
                    \draw[->, bend left, out=45, in=150] (HN) to node[reward-label] {$r_n$} (SN);
                    
                    \draw[-, bend left, out=30, in=140]  (S1) to node[action-label, pos=1.0] {$a_1$} (H1);
                    \draw[-, bend left, out=30, in=140]  (S2) to node[action-label, pos=1.0] {$a_2$} (H2);
                    \draw[-, bend left, out=30, in=140]  (S3) to node[action-label, pos=1.0] {$a_3$} (H3);
                    \draw[-, bend left, out=30, in=135]  (S4) to node[action-label] {$a_4$} (HN);
                \end{tikzpicture}
                $s_1,\ a_1,\ s_2,\ a_2,\ s_3,\ a_3,\ s_4,\ a_4,\ \dots\,\ \boxed{\mathbf{r_n}},\ s_n$
                \caption{As recompensas são recebidas após várias ações serem realizadas}
                \label{diag:delayed-rewards}
            \end{figure}
            
            Temos somente uma única recompensa ao fim do episódio, o que caracteriza um \todo{alguma coisa, olhar no sutton} de recompensas \emph{esparsas}.

    \section{Definição de MDP}
    
        Esse tipo de interação apresentado acima é formalizado matematicamente como um \emph{Processo de decisão de Markov} (MDP). \textcolor{red}{Os papéis do agente e ambiente são diferentes}.
    
        \subsection{Elementos de um MDP}
    
            \subsubsection{Estado}
            
                \begin{figure}[ht]
                    \centering
                    \begin{tikzpicture}
                        \node[state-node] (R) {$s_1$};
                    \end{tikzpicture}
                    \caption{Nos nossos diagramas, um estado é representado por um círculo branco com borda preta}
                    \label{diag:state-node}
                \end{figure}
                
            \subsubsection{Ação}
            
                \begin{figure}[ht]
                    \centering
                    \begin{tikzpicture}
                        \node[action-node] (R) {$a_1$};
                    \end{tikzpicture}
                    \caption{Nos nossos diagramas, uma ação é representada por um círculo preto com texto branco}
                    \label{diag:action-node}
                \end{figure}
                
            \subsubsection{\textcolor{red}{Probabilidade de Transição de Estado}}
            
                \begin{figure}[ht]
                    \centering
                    $p_1$\\
                    \caption{Nos nossos diagramas, uma probabilidade é representada por um texto preto sem borda}
                    \label{diag:prob-label}
                \end{figure}
            
                Dessa forma, a distribuição de probabilidade $p$ caracteriza a \emph{dinâmica} de um MDP.
            
                Nota: duas notações são comuns ao para se referir à probabilidade:
                
                \begin{enumerate}
                    \item $p(s', r \mid s, a)$, que pode ser lida como ``chegar no estado $s'$ recebendo uma recompensa $r$ dado que está no estado $s$ e executa a ação $a$'';
                    \item $p(s, a, r, s')$, que pode ser lida como ``está no estado $s$, executa a ação $a$, recebe a recompensa $r$ e chega no estado $s'$.
                \end{enumerate}
                
            \subsubsection{Recompensa}
        
                \begin{figure}[ht]
                    \centering
                    \begin{tikzpicture}
                        \node[reward-node] (R) {$r_1$};
                    \end{tikzpicture}
                    \caption{Nos nossos diagramas, uma recompensa é representada por um retângulo branco com borda preta}
                    \label{diag:reward-label}
                \end{figure}
        
        \subsection{Características de um MDP}
        
            \subsubsection{MDP's finitos e infinitos}
            
            \subsubsection{Propriedade de Markov}
            
            \subsubsection{Cadeia de Markov}
        
    \section{Visão gráfica de um MDP}
    
        Graficamente, um MDP é representado como um grafo direcionado em que os nós são pares estados e ações, e as arestas indicam o estado a ser alcançado:
    
        \begin{figure}[ht]
            \centering
            \mdpbig
            \caption{Exemplo de um MDP com diversas interações}
            \label{diag:mdp-big}
        \end{figure}
        
    \section{Exemplo de como representar um problema via MDP}
    
        \textcolor{red}{Escadas de Hogwarts}
    
    \section{Objetivos}
    
        \subsection{Hipótese da recompensa}
    
            Informalmente, o propósito de um agente é receber o máximo de recompensa possível em tarefa, como afirmada na \emph{hipótese da recompensa}, em tradução livre:
            \begin{center}
            \noindent\fbox{%
                \parbox{45em}{%
                    \vskip 0.25em
                    \parindent=2em\indent
                    \emph{Tudo o que queremos dizer com objetivos e propósitos pode ser bem entendido como a maximização do valor esperado da soma cumulativa de um sinal escalar recebido (chamado de recompensa)}
                    \vskip 0.25em
                }%
            }%
            \end{center}
            
            Observação: a hipótese acima foi sugerida por Michael Littman para os autores Sutton e Barto em comunicação pessoal, como eles explicam nas notas do capítulo 3 do livro. Uma discussão inicial promovida por Sutton é encontrada no endereço:
            
            \url{http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html}
        
        \subsection{Retorno}

            \begin{equation}
                G_t \ = \ R_{t+1} + R_{t+2} + R_{t+3} + \cdots
            \end{equation}

            \subsubsection{Tarefas episódicas}

                \begin{equation}
                \begin{split}
                    G_t & \ = \ R_{t+1} + R_{t+2} + R_{t+3} + \cdots + R_T \\
                    & \ = \ \sum_{k=t+1}^{T} R_k
                \end{split}
                \end{equation}

            \subsubsection{Tarefas contínuas}
        
                \begin{equation}
                \begin{split}
                    G_t & \ = \ R_{t+1} + R_{t+2} + R_{t+3} + \cdots \\
                    & \ = \ \sum_{k=0}^{\infty} \gamma^k R_{t + k + 1}
                \end{split}
                \end{equation}
        
        \subsection{Desconto}
        
            Propósito intuitivo: recompensa imediatas são mais importantes do que as futuras.
            
            Propósito formal: transformar retornos infinitos em finitos.
        
    \section{Política}
    
        Dentro desse contexto, precisamos encontrar uma relação entre um estados e as suas ação possíveis. Em especial, devemos definir a probabilidade de executar cada ação em cada estado. A função que faz o mapeamento entre os pares estao e ação, $(s, a)$, é chamada de \emph{política}. 
        
        Formalmente, a política é definida como uma distribuição de probabilidade entre as ações possíveis em um estado. Na prática, a política nos diz qual ação será executada a seguir, considerando o estado atual.
    
        \section{Política ótima}
        
    
    \section{Funções de valor}
        
        \subsection{Função estado-valor}
        
            \begin{equation}
            \begin{split}
                v_{\pi}(s) & \ = \ \mathbb{E}[G_t \mid S_t = s] \\
                & \ = \ \mathbb{E}[R_{t+1} + \gamma G_{t+1} \mid S_t = s] \\
                & \ = \ \mathbb{E}[R_{t+1} + \gamma v_{\pi}(s') \mid S_t = s] \\
                & \ = \ \sum_{a} \pi(s,a) \sum_{s', r} p(s, a, r, s') [r + \gamma v_{\pi}(s')]
            \end{split}
            \end{equation}
        
        \subsection{Função ação-valor}
        
            \todo{retirar o v da equação abaixo}

            \begin{equation}            
            \begin{split}
                q_{\pi}(s, a) & \ = \ \mathbb{E}[G_t \mid S_t = s, A_t = a] \\
                & \ = \ \mathbb{E}[R_{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a] \\
                & \ = \ \mathbb{E}[R_{t+1} + \gamma v_{\pi}(s') \mid S_t = s, A_t = a] \\
                & \ = \ \sum_{s', r} p(s, a, r, s') [r + \gamma v_{\pi}(s')]
            \end{split}
            \end{equation}
        
        \subsection{Otimalidade das funções de valor}
        
            \subsubsection{Valor estado}
            
                \begin{equation}
                \begin{split}
                    v_*(s) & \ = \ \max_{a} \mathbb{E}[G_t \mid S_t = s, A_t = a] \\
                    & \ = \ \max_{a} \sum_{s', r} p(s, a, r, s') [r + \gamma v_*(s')]
                \end{split}
                \end{equation}
            
            \subsubsection{Valor ação}
            
                \begin{equation}
                \begin{split}
                    q_*(s, a) & \ = \ \max_{a} \mathbb{E}[G_t \mid S_t = s, A_t = a] \\
                    & \ = \ \sum_{s', r} p(s, a, r, s') [r + \gamma \max_{a'} q_*(s', a')]
                \end{split}
                \end{equation}

\end{document}