\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}

\usepackage{amsmath,amssymb}

\input{pb_rlstyle_tikzgraph.tex}

% text spacing
\linespread{1.3}
\setlength{\parindent}{4em}
\setlength{\parskip}{0.75em}


% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        %\vskip 0.5em
        {\large \textbf{\@author} \par}
        %\vskip 0.5em
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother


\title{Notas de estudo - Aprendizado por Reforço}
\author{Parte 04 - Programação Dinâmica}
\date{Paulo Bruno de Sousa Serafim - Out/19 - Abr/20}


\begin{document}

\maketitle

    \section{Introdução}

        Anteriormente vimos que problemas de aprendizado por reforço podem ser modelados como um Processo de Decisão Markoviano (MDP). A essa representação damos o nome de \emph{modelo}. Quando temos informação sobre todos os aspectos do modelo, dizemos que conhecemos/sabemos/temos o \emph{modelo completo}.

        Embora os os problemas de aprendizado por reforço possam ser representados como MDPs, na maioria das vezes não teremos todas as informações necessárias para construir essa representação. Entre outras formas, o aprendizado pode ser caracterizado, pelo nível de conhecimento do modelo. Dessa forma, podemos utilizar técnicas específicas para os casos de termos ou não o modelo completo.

        Em um modelo completo conhecemos de antemão todos os estados, ações, recompensas e distribuições de probabilidade. Nesses casos, utilizamos técnicas de aprendizado \emph{baseadas no modelo}. Por outro lado, se não temos todas essas informações, utilizamos técnicas de aprendizado \emph{livres de modelo}.

        \subsection{Exemplo de modelo completo}
    
            Considere o MDP abaixo representado pelo diagrama da Figura~\ref{diag:model-based}. Considere que todas os rótulos ($s_i$, $a_i$, $p_i$, $r_i$) presentes são informações conhecidas. Nesse caso, sabemos todas as informações necessárias sobre o modelo: todos os estados existentes, todas as ações possíveis, todas as recompensas e, principalmente, as probabilidades de transição.
            
            \begin{figure}[ht]
                \centering
                \mdpthreestate
                \caption{Uma vez que conhecemos todos os estados, ações, recompensas e distribuições de probabilidade, temos um modelo completo.}
                \label{diag:model-based}
            \end{figure}
            
            Assim, o diagrama da Figura~\ref{diag:model-based} é um exemplo de um modelo completo. Sabendo todas as informações sobre o modelo, nós podemos estimar um valor para o estado $s_1$ de acordo com as equações de Bellman, como veremos adiante.
            
        \subsection{Exemplo de modelo incompleto}
    
            Apesar de ser muito semelhante ao diagrama da Figura~\ref{diag:model-based}, a Figura~\ref{diag:model-free} nos indica que não conhecemos as probabilidades de transição. Dessa forma, por não sabermos todas as informações, esse é um exemplo de modelo incompleto. 

            \begin{figure}[ht]
                \centering
                \mdpthreestatenoprobs
                \caption{Apesar de conhecermos todos os etados, ações e recompensas, não sabemos as distribuições de probabilidade, o que é suficiente para caracterizar um problema livre de modelo.}
                \label{diag:model-free}
            \end{figure}

            Nos estudos seguintes veremos maneiras de resolver problemas livres de modelo. Nesse momento nos concentraremos no estudo de uma maneira de resolver problemas em que temos o modelo completo, \emph{Programação Dinâmica}.
    
    \section{Programação Dinâmica}
    
        Antes de entrarmos em aspectos mais técnicos, a programação dinâmica é um método de otimização matemático utilizado inicialmente para resolver problemas de controle. Ela foi desenvolvida por Richard Bellman [1920-1984] na década de 1950. Foi Bellman quem definiu o conceito de Processo de Decisão Markoviano e o utilizou em suas soluções para problemas de otimização de controle. Somente nas décadas finais do século XX é que os trabalhos de Bellman foram considerados como uma parte do que hoje conhecemos como Aprendizado por Reforço.

        Recaptulando dos nossos estudos anteriores, o objetivo final em problemas de aprendizado por reforço é encontrar uma política ótima, $\pi_*$, para o agente. Utilizando essa política, temos a garantia de que a valor esperado da recompensa total recebida será o maior possível. E são as \emph{equações de otimalidade de Bellman} que nos dão essa solução.
    
        \subsection{Equações de otimalidade de Bellman}

            A primeira equação que vamos avaliar é a \emph{equação de otimalidade de Bellman para o valor do estado}, dada de acordo com~\eqref{eq:optimal-state-value}. Ela nos dá o valor esperado das recompensas que receberemos a partir de um estado $s$ e seguindo a política ótima $\pi_*$ a partir dele. Observe que em~\eqref{eq:optimal-state-value} consideramos que o agente sempre vai escolher a ação maximal, pois ele visa obter o maior valor esperado.
            \begin{equation}
                \begin{aligned}
                    v_{\pi_*}(s) 
                    & \ = \ \max_a \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi_*}(S_{t+1}) \mid S_t = s, A_t = a \right] \\
                    & \ = \ \max_a \sum_{r, s'} p(s, a, r, s') \left[ r + \gamma v_{\pi_*}(s') \right]
                \end{aligned}
                \label{eq:optimal-state-value}
            \end{equation}
            
            A segunda equação é a \emph{equação de otimalidade de Bellman para o valor da ação} e é dada de acordo com~\eqref{eq:optimal-action-value}. Ela nos dá o valor esperado das recompensas que vamos receber a partir de um estado $s$, executando uma ação $a$ e a partir daí executar a ação de maior valor esperado seguindo a política ótima $\pi_*$.
            \begin{equation}
                \begin{aligned}
                    q_{\pi_*}(s,a) 
                    & \ = \ \mathbb{E} \left[ R_{t+1} + \gamma \max_{a'} q_{\pi_*}(S_{t+1}, a') \bigm\vert S_t = s, A_t = a \right] \\
                    & \ = \ \sum_{r, s'} p(s, a, r, s') \left[ r + \gamma \max_{a'}q_{\pi_*}(s', a') \right]
                \end{aligned}
                \label{eq:optimal-action-value}
            \end{equation}

            Com essas equações, podemos avaliar um estado e um par estado-ação, desde que tenhamos todas as informações necessárias do modelo. Note que nos casos ótimos consideramos a ação escolhida como sendo a que retorna o maior valor. 
            
            Caso não tenhamos a política ótima ou estejamos seguindo outra política $\pi$ qualquer, poderíamos usar as funções de valor para avaliar essa política. É o que faremos a seguir. No caso da Programação Dinâmica, utilizamos uma abordagem em duas etapas: o primeiro será a avaliar a política e depois iremos melhorá-la.
            
    \section{Avaliação da política}
    
        Também conhecido como o \emph{problema da predição}, a \emph{avaliação de uma política} consiste em computar o valor de um estado seguindo uma determinada política $\pi$. Como vimos anteriormente, o valor de um estado $s$ seguindo uma política $\pi$ é dada pela \emph{equação de Bellman} do valor do estado:
        \begin{equation}
            \label{eq:state-value}
            \begin{aligned}
                v_{\pi}(s) 
                & \ = \ \mathbb{E}_{\pi} \left[ G_t \mid S_t = s \right] \\
                & \ = \ \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right] \\
                & \ = \ \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s \right] \\
                & \ = \ \sum_a \pi(s, a) \sum_{r, s'} p(s, a, r, s') \Big[ r + \gamma v_{\pi}(s') \Big] .
            \end{aligned}
        \end{equation}
    
        Dessa forma, conhecendo todas as informações presentes em~\eqref{eq:state-value}, podemos resolver essa equação e encontrar o valor de $v_{\pi}(s)$. Uma maneira de fazer isso é utilizar um sistema de equações.
    
        \subsection{Exemplo de cálculo por sistemas de equações}
        
            Considere para nosso exemplo o MDP representado pelo diagrama da Figura~\ref{diag:model-based}. De início, vamos avaliar cada estado de acordo com a equação de Bellman para o valor do estado. Por exemplo, para o estado $s_1$ temos:
            \begin{equation}
                \label{eq:equation-system}
                \begin{aligned}
                    v_\pi(s_1) 
                        &= \pi(s_1, a_1) \cdot p(s_1, a_1, r_1, s_2) \cdot \Big[ r_1 + \gamma \cdot v_\pi(s_2) \Big] \\
                        &+ \pi(s_1, a_1) \cdot p(s_1, a_1, r_2, s_2) \cdot \Big[ r_2 + \gamma \cdot v_\pi(s_2) \Big] \\
                        &+ \pi(s_1, a_2) \cdot p(s_1, a_2, r_3, s_2) \cdot \Big[ r_3 + \gamma \cdot v_\pi(s_2) \Big] \\
                        &+ \pi(s_1, a_2) \cdot p(s_1, a_2, r_4, s_3) \cdot \Big[ r_4 + \gamma \cdot v_\pi(s_3) \Big] \\
                        &+ \pi(s_1, a_3) \cdot p(s_1, a_3, r_5, s_3) \cdot \Big[ r_5 + \gamma \cdot v_\pi(s_3) \Big] .
                \end{aligned}
            \end{equation}
        
            \emph{Observação: poderíamos considerar todos os estados, ações e transições em~\eqref{eq:equation-system}. Por exemplo, poderíamos pôr um termo para uma transição de $s_1$ a $s_3$. Entretanto, essa transição teria probabilidade 0 de ocorrer. Por isso omitimos esses termos e só pusemos as transições que podem de fato acontecer.}
        
            De maneira análoga, podemos facilmente encontrar as equações de avaliação para os outros estados. Note que as únicas icógnitas em~\eqref{eq:state-value} são os valores dos outros estados. Assim, ao final, teremos $n$ equações com $n$ icógnitas. Uma vez que resolvermos esse sistemas, teremos todos os valores dos estados. Embora, como vimos, esse valor possa ser encontrado partir da resolução de um sistema linear nos estados, essa abordagem pode ser muito custosa. Dessa forma, aqui estamos interessados em computá-lo iterativamente. 
        
        \subsection{Avaliação iterativa da política}
        
            Para o caso em que queremos melhorar a estimativa do valor esperado de um estado iterativamente a estratégia é começar com um valor inicial, $v_{\pi_{0}}$ e melhorar a estimativa do valor atual $v_{\pi_{k+1}}$ a cada iteração $k$.
            A equação que vamos utilizar é muito parecida com a Equação de Bellman para $v_{\pi}(s)$. 
            Entretanto substituiremos o valor estado do estado seguinte $s'$, que não conhecemos, por uma estimativa.
            Especificamente, vamos usar a estimativa calculada no passo anterior, $v_{\pi_{k}(s')}$.
            Observação: para simplificar a notação, removeremos o índice $\pi$, mas ele ainda está lá implicitamente.
            
            \begin{equation}
            \begin{split}
                v_0(s) & \ = \ estimativa inicial\\
                \downarrow\\
                v_1(s) & \ = \ \sum_{a} \pi(s, a) \sum_{r, s'} p(s, a, r, s') \Big[ r + \gamma \cdot v_0(s') \Big]\\
                \downarrow\\
                v_2(s) & \ = \ \sum_{a} \pi(s, a) \sum_{r, s'} p(s, a, r, s') \Big[ r + \gamma \cdot v_1(s') \Big]\\
                \downarrow\\
                v_3(s) & \ = \ \sum_{a} \pi(s, a) \sum_{r, s'} p(s, a, r, s') \Big[ r + \gamma \cdot v_2(s') \Big]\\
                \vdots\\
                v_{k+1}(s) & \ = \ \sum_{a} \pi(s, a) \sum_{r, s'} p(s, a, r, s') \Big[ r + \gamma \cdot v_k(s') \Big]
            \end{split}
            \end{equation}
    
            Também conhecido como o \textit{problema da predição}, a avaliação da política consiste em computar o valor de um estado seguindo uma determinada política $\pi$. Embora esse valor possa ser definido a partir da resolução de um sistema linear nos estados, isso pode ser muito custoso, dessa forma, aqui estamos interessados em computá-lo iterativamente.
                    
            A sua versão iterativa é dada por:
            
            \begin{equation}
                \begin{split}
                    v_{k+1}(s) & \ \dot{=} \, \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_k(S_{t+1}) \mid S_t = s \right] \\
                    & = \sum_a \pi(a \mid s) \sum_{s',r} p(s',r \mid s,a) \Big[ r + \gamma v_k(s') \Big]
                \end{split}
            \end{equation}
            
            Em que a aproximação inicial $v_0(s)$ pode ser escolhida arbitrariamente e o valor de um estado terminal é $0$.
        
    \section{Melhora da política}
    
        Também conhecido como o \textit{problema do controle}.
        
        \begin{equation}
            \begin{split}
                q_{\pi}(s,a) & \ \dot{=} \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s, A_t = a \right] \\
                & = \sum_{s',r} p(s', r \mid s, a) \Big[ r + \gamma v_{\pi}(s') \Big]
            \end{split}
        \end{equation}
    
    \section{Iteração da política}
    
        União da avaliação e da melhora.
        
        \begin{figure}[ht]
            \centering
            \begin{math}
                \pi_0 \xrightarrow{\ \textrm{A} \ } 
                v_{\pi_0} \xrightarrow{\ \textrm{M} \ } 
                \pi_1 \xrightarrow{\ \textrm{A} \ } 
                v_{\pi_1} \xrightarrow{\ \textrm{M} \ } 
                \pi_2 \xrightarrow{\ \textrm{A} \ } 
                \cdots \xrightarrow{\ \textrm{M} \ }
                \pi_* \xrightarrow{\ \textrm{A} \ } v_{\pi_*}
            \end{math}
            \caption{Representação da iteração da política inicial $\pi_0$ até chegar uma política ótima $\pi_*$}
            \label{fig:policy-iteration}
        \end{figure}
        
    \section{Iteração do valor}
    
        \begin{equation}
            \begin{split}
                v_{k+1}(s) & \  \dot{=} \ \max_a \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) \mid S_t = s, A_t = a] \\
                & = \ \max_a \sum_{s',r} p(s', r \mid s, a) \Big[ r + \gamma v_k(s') \Big]
            \end{split}
        \end{equation}
    
    \section{Iteração de Política Generalizada}
    
        \emph{Iteração de Política Generalizada} (GPI) é o termo que abrange todas as maneiras de encontrar uma política ótima de maneira iterativa através de sucessivas avaliações e melhoras, em que a cada passo estaremos cada vez mais próximos dos valores ótimos. Esse processo é ilustrado na Figura~\ref{diag:generalized-policy-iteration}. 
        
        \vspace{5mm}
        \begin{figure}[ht]
            \centering
            \begin{tikzpicture}[->,>=latex,shorten >=1pt,auto,node distance=2.8cm, thick]
                \tikzstyle{state}=[fill=none,draw=none,text=black]
                
                \node[state] (A) {\Large$\boldsymbol{\pi}$};
                \node[state] (B) [right of=A, xshift=0.75cm] {\Large$\mathbf{V}$};
                \node[state] (DOTS) [below of=A, xshift=1.75cm, yshift=0.5cm] {\textbf{\vdots}};
                \node[state] (A2) [below of=A, yshift=-0.5cm] {\Large$\boldsymbol{\pi_{*}}$};
                \node[state] (B2) [below of=B, yshift=-0.5cm] {\Large$\mathbf{v_{*}}$};
            
                \draw[bend left=70] (A) to node[above] {evaluation} (B);
                \draw[bend left=60, draw=none] (A) to node[below] {$V 	\rightsquigarrow v_{\pi}$} (B);
                \draw[bend left=70] (B) to node[below] {improvement} (A);
                \draw[bend left=60, draw=none] (B) to node[above] {\small$\pi \rightsquigarrow$ greedy($V$)} (A);
                
                \draw[transform canvas={yshift=0.15cm}] (A2) to node[] {} (B2);
                \draw[transform canvas={yshift=-0.15cm}] (B2) to node[] {} (A2);
            \end{tikzpicture}
            \caption{Representação da dinâmica de atualização da política na abordagem de GPI. A convergência ocorre quando encontrarmos uma política ótima $\pi_*$.}
            \label{diag:generalized-policy-iteration}
        \end{figure}
        
\end{document}