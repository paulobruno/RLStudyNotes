\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}

\usepackage{amsmath,amssymb}

\input{pb_rlstyle_tikzgraph.tex}

% text spacing
\linespread{1.3}
\setlength{\parindent}{4em}
\setlength{\parskip}{0.75em}


% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        %\vskip 0.5em
        {\large \textbf{\@author} \par}
        %\vskip 0.5em
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother


\title{Notas de estudo - Aprendizado por Reforço}
\author{Parte 04 - Programação Dinâmica}
\date{Paulo Bruno de Sousa Serafim - Out/19 - Abr/20}


\begin{document}

\maketitle

    \section{Introdução}

        Anteriormente vimos que problemas de aprendizado por reforço podem ser modelados como um Processo de Decisão Markoviano (MDP). A essa representação damos o nome de \emph{modelo}. Quando temos informação sobre todos os aspectos do modelo, dizemos que conhecemos/sabemos/temos o \emph{modelo completo}.

        Embora os os problemas de aprendizado por reforço possam ser representados como MDPs, na maioria das vezes não teremos todas as informações necessárias para construir essa representação. Entre outras formas, o aprendizado pode ser caracterizado, pelo nível de conhecimento do modelo. Dessa forma, podemos utilizar técnicas específicas para os casos de termos ou não o modelo completo.

        Em um modelo completo conhecemos de antemão todos os estados, ações, recompensas e distribuições de probabilidade. Nesses casos, utilizamos técnicas de aprendizado \emph{baseadas no modelo}. Por outro lado, se não temos todas essas informações, utilizamos técnicas de aprendizado \emph{livres de modelo}.

        \subsection{Exemplo de modelo completo}
    
            Considere o MDP abaixo representado pelo diagrama da Figura~\ref{diag:model-based}. Considere que todas os rótulos ($s_i$, $a_i$, $p_i$, $r_i$) presentes são informações conhecidas. Nesse caso, sabemos todas as informações necessárias sobre o modelo: todos os estados existentes, todas as ações possíveis, todas as recompensas e, principalmente, as probabilidades de transição.
            
            \begin{figure}[ht]
                \centering
                \mdpthreestate
                \caption{Uma vez que conhecemos todos os estados, ações, recompensas e distribuições de probabilidade, temos um modelo completo.}
                \label{diag:model-based}
            \end{figure}
            
            Assim, o diagrama da Figura~\ref{diag:model-based} é um exemplo de um modelo completo. Sabendo todas as informações sobre o modelo, nós podemos estimar um valor para o estado $s_1$ de acordo com as equações de Bellman, como veremos adiante.
            
        \subsection{Exemplo de modelo incompleto}
    
            Apesar de ser muito semelhante ao diagrama da Figura~\ref{diag:model-based}, a Figura~\ref{diag:model-free} nos indica que não conhecemos as probabilidades de transição. Dessa forma, por não sabermos todas as informações, esse é um exemplo de modelo incompleto. 

            \begin{figure}[ht]
                \centering
                \mdpthreestatenoprobs
                \caption{Apesar de conhecermos todos os etados, ações e recompensas, não sabemos as distribuições de probabilidade, o que é suficiente para caracterizar um problema livre de modelo.}
                \label{diag:model-free}
            \end{figure}

            Nos estudos seguintes veremos maneiras de resolver problemas livres de modelo. Nesse momento nos concentraremos no estudo de uma maneira de resolver problemas em que temos o modelo completo, \emph{Programação Dinâmica}.
    
    \section{Programação Dinâmica}
    
        Antes de entrarmos em aspectos mais técnicos, a programação dinâmica é um método de otimização matemático utilizado inicialmente para resolver problemas de controle. Ela foi desenvolvida por Richard Bellman [1920-1984] na década de 1950. Foi Bellman quem definiu o conceito de Processo de Decisão Markoviano e o utilizou em suas soluções para problemas de otimização de controle. Somente nas décadas finais do século XX é que os trabalhos de Bellman foram considerados como uma parte do que hoje conhecemos como Aprendizado por Reforço.

        Recaptulando dos nossos estudos anteriores, o objetivo final em problemas de aprendizado por reforço é encontrar uma política ótima, $\pi_*$, para o agente. Utilizando essa política, temos a garantia de que a valor esperado da recompensa total recebida será o maior possível. E são as \emph{equações de otimalidade de Bellman} que nos dão essa solução.
    
        \subsection{Equações de otimalidade de Bellman}

            A primeira equação que vamos avaliar é a \emph{equação de otimalidade de Bellman para o valor do estado}, dada de acordo com~\eqref{eq:optimal-state-value}. Ela nos dá o valor esperado das recompensas que receberemos a partir de um estado $s$ e seguindo a política ótima $\pi_*$ a partir dele. Observe que em~\eqref{eq:optimal-state-value} consideramos que o agente sempre vai escolher a ação maximal, pois ele visa obter o maior valor esperado.
            \begin{equation}
                \begin{aligned}
                    v_{\pi_*}(s) 
                    & \ = \ \max_a \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi_*}(S_{t+1}) \mid S_t = s, A_t = a \right] \\
                    & \ = \ \max_a \sum_{r, s'} p(s, a, r, s') \left[ r + \gamma v_{\pi_*}(s') \right]
                \end{aligned}
                \label{eq:optimal-state-value}
            \end{equation}
            
            \subsubsection{Função de valor-ação}
        
                \begin{equation}
                    \begin{split}
                        q_*(s,a) & = \mathbb{E} \left[ R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') \bigm\vert S_t = s, A_t = a \right] \\
                        & = \sum_{s', r} p(s',r \mid s,a) \left[ r + \gamma \max_{a'}q_*(s', a') \right]
                    \end{split}
                \end{equation}
                
            Com essas equações, podemos avaliar um estado e um par estado-ação, desde que tenhamos todas as informações necessárias do modelo. Note que nos casos ótimos consideramos a ação escolhida como sendo a que retorna o maior valor. Caso não tenhamos a política ótima ou estejamos seguindo outra política $\pi$ qualquer, poderíamos usar as funções de valor para avaliar essa política. É o que faremos a seguir.
            
    \section{Avaliação de política}
    
        \subsection{Exemplo de cálculo por sistemas de equações}
        
            \begin{equation}
                \begin{split}
                    v_\pi(s_1) &= \pi(a_1 \mid s_1) \cdot p(s_2, r_1 \mid s_1, a_1) \cdot \Big[ r_1 + \gamma \cdot v_\pi(s_2) \Big] \\
                               &+ \pi(a_1 \mid s_1) \cdot p(s_2, r_2 \mid s_1, a_1) \cdot \Big[ r_2 + \gamma \cdot v_\pi(s_2) \Big] \\
                               &+ \pi(a_2 \mid s_1) \cdot p(s_2, r_3 \mid s_1, a_2) \cdot \Big[ r_3 + \gamma \cdot v_\pi(s_2) \Big] \\
                               &+ \pi(a_2 \mid s_1) \cdot p(s_3, r_4 \mid s_1, a_2) \cdot \Big[ r_4 + \gamma \cdot v_\pi(s_3) \Big] \\
                               &+ \pi(a_3 \mid s_1) \cdot p(s_3, r_5 \mid s_1, a_3) \cdot \Big[ r_5 + \gamma \cdot v_\pi(s_3) \Big]
                \end{split}
            \end{equation}
        
        \subsection{Avaliação iterativa da política}
        
            Para o caso em que queremos melhorar a estimativa do valor esperado de um estado iterativamente a estratégia é começar com um valor inicial, $v_{\pi_{0}}$ e melhorar a estimativa do valor atual $v_{\pi_{k+1}}$ a cada iteração $k$.
            A equação que vamos utilizar é muito parecida com a Equação de Bellman para $v_{\pi}(s)$. 
            Entretanto substituiremos o valor estado do estado seguinte $s'$, que não conhecemos, por uma estimativa.
            Especificamente, vamos usar a estimativa calculada no passo anterior, $v_{\pi_{k}(s')}$.
            Observação: para simplificar a notação, removeremos o índice $\pi$, mas ele ainda está lá implicitamente.
            
            \begin{equation}
            \begin{split}
                v_0(s) & \ = \ estimativa inicial\\
                \downarrow\\
                v_1(s) & \ = \ \sum_{a} \pi(s, a) \sum_{r, s'} p(s, a, r, s') \Big[ r + \gamma \cdot v_0(s') \Big]\\
                \downarrow\\
                v_2(s) & \ = \ \sum_{a} \pi(s, a) \sum_{r, s'} p(s, a, r, s') \Big[ r + \gamma \cdot v_1(s') \Big]\\
                \downarrow\\
                v_3(s) & \ = \ \sum_{a} \pi(s, a) \sum_{r, s'} p(s, a, r, s') \Big[ r + \gamma \cdot v_2(s') \Big]\\
                \vdots\\
                v_{k+1}(s) & \ = \ \sum_{a} \pi(s, a) \sum_{r, s'} p(s, a, r, s') \Big[ r + \gamma \cdot v_k(s') \Big]
            \end{split}
            \end{equation}
    
            Também conhecido como o \textit{problema da predição}, a avaliação da política consiste em computar o valor de um estado seguindo uma determinada política $\pi$. Embora esse valor possa ser definido a partir da resolução de um sistema linear nos estados, isso pode ser muito custoso, dessa forma, aqui estamos interessados em computá-lo iterativamente.
            
            O valor de um estado $s$ seguindo uma política $\pi$ é dado por:
            
            \begin{equation}
                \begin{split}
                    v_{\pi}(s) & \ \dot{=} \, \mathbb{E}_{\pi} \left[ G_t \mid S_t = s \right] \\
                    & = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right] \\
                    & = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s \right] \\
                    & = \sum_a \pi(a \mid s) \sum_{s',r} p(s',r \mid s,a) \Big[ r + \gamma v_{\pi}(s') \Big]
                \end{split}
            \end{equation}
                    
            A sua versão iterativa é dada por:
            
            \begin{equation}
                \begin{split}
                    v_{k+1}(s) & \ \dot{=} \, \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_k(S_{t+1}) \mid S_t = s \right] \\
                    & = \sum_a \pi(a \mid s) \sum_{s',r} p(s',r \mid s,a) \Big[ r + \gamma v_k(s') \Big]
                \end{split}
            \end{equation}
            
            Em que a aproximação inicial $v_0(s)$ pode ser escolhida arbitrariamente e o valor de um estado terminal é $0$.
        
    \section{Melhora da política}
    
        Também conhecido como o \textit{problema do controle}.
        
        \begin{equation}
            \begin{split}
                q_{\pi}(s,a) & \ \dot{=} \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s, A_t = a \right] \\
                & = \sum_{s',r} p(s', r \mid s, a) \Big[ r + \gamma v_{\pi}(s') \Big]
            \end{split}
        \end{equation}
    
    \section{Iteração da política}
    
        União da avaliação e da melhora.
        
        \begin{center}
            \begin{math}
                \pi_0 \xrightarrow{\ \textrm{E} \ } 
                v_{\pi_0} \xrightarrow{\ \textrm{I} \ } 
                \pi_1 \xrightarrow{\ \textrm{E} \ } 
                v_{\pi_1} \xrightarrow{\ \textrm{I} \ } 
                \pi_2 \xrightarrow{\ \textrm{E} \ } 
                \cdots \xrightarrow{\ \textrm{I} \ }
                \pi_* \xrightarrow{\ \textrm{E} \ } v_{*}
            \end{math}
        \end{center}
        
    \section{Iteração do valor}
    
        \begin{equation}
            \begin{split}
                v_{k+1}(s) & \  \dot{=} \ \max_a \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) \mid S_t = s, A_t = a] \\
                & = \ \max_a \sum_{s',r} p(s', r \mid s, a) \Big[ r + \gamma v_k(s') \Big]
            \end{split}
        \end{equation}
    
    \section{Iteração de Política Generalizada (GPI)}
    
        \begin{center}
            \begin{tikzpicture}[->,>=latex,shorten >=1pt,auto,node distance=2.8cm, thick]
                \tikzstyle{state}=[fill=none,draw=none,text=black]
                
                \node[state] (A) {\Large$\boldsymbol{\pi}$};
                \node[state] (B) [right of=A, xshift=0.75cm] {\Large$\mathbf{V}$};
                \node[state] (DOTS) [below of=A, xshift=1.75cm, yshift=0.5cm] {\textbf{\vdots}};
                \node[state] (A2) [below of=A, yshift=-0.5cm] {\Large$\boldsymbol{\pi_{*}}$};
                \node[state] (B2) [below of=B, yshift=-0.5cm] {\Large$\mathbf{v_{*}}$};
            
                \draw[bend left=70] (A) to node[above] {evaluation} (B);
                \draw[bend left=60, draw=none] (A) to node[below] {$V 	\rightsquigarrow v_{\pi}$} (B);
                \draw[bend left=70] (B) to node[below] {improvement} (A);
                \draw[bend left=60, draw=none] (B) to node[above] {\small$\pi \rightsquigarrow$ greedy($V$)} (A);
                
                \draw[transform canvas={yshift=0.15cm}] (A2) to node[] {} (B2);
                \draw[transform canvas={yshift=-0.15cm}] (B2) to node[] {} (A2);
            \end{tikzpicture}
        \end{center}
        
        
\end{document}