\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}

\usepackage{hyperref}
\usepackage{indentfirst}

\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{subcaption}

\usepackage{amsmath,amssymb}
\DeclareMathOperator*{\argmax}{argmax}

\input{pb_rlstyle_tikzgraph.tex}

% text spacing
\linespread{1.3}
\setlength{\parindent}{4em}
\setlength{\parskip}{0.75em}


% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        {\large \textbf{\@author} \par}
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother


\title{Notas de estudo - Aprendizado por Reforço}
\author{Parte 04 - Programação Dinâmica}
\date{Paulo Bruno de Sousa Serafim - Out/19 - Abr/20}


\begin{document}

\maketitle

    \section{Introdução}

        Anteriormente vimos que problemas de Aprendizado por Reforço podem ser modelados como um Processo de Decisão Markoviano (MDP). A essa representação damos o nome de \emph{modelo}. Quando temos informação sobre todos os aspectos do modelo, dizemos que conhecemos/sabemos/temos o \emph{modelo completo}.

        Embora os os problemas de Aprendizado por Reforço possam ser representados como MDPs, na maioria das vezes não teremos todas as informações necessárias para construir essa representação. Entre outras formas, o aprendizado pode ser caracterizado, pelo nível de conhecimento do modelo. Dessa forma, podemos utilizar técnicas específicas para os casos de termos ou não o modelo completo.

        Em um modelo completo conhecemos de antemão todos os estados, ações, recompensas e distribuições de probabilidade. Nesses casos, utilizamos técnicas de aprendizado \emph{baseadas no modelo}. Por outro lado, se não temos todas essas informações, utilizamos técnicas de aprendizado \emph{livres de modelo}.

        \subsection{Exemplo de modelo completo}
    
            Considere o MDP abaixo representado pelo diagrama da Figura~\ref{diag:model-based}. Considere que todas os rótulos ($s_i$, $a_i$, $p_i$, $r_i$) presentes são informações conhecidas. Nesse caso, sabemos todas as informações necessárias sobre o modelo: todos os estados existentes, todas as ações possíveis, todas as recompensas e, principalmente, as probabilidades de transição.
            
            \begin{figure}[ht]
                \centering
                \mdpthreestate
                \caption{Uma vez que conhecemos todos os estados, ações, recompensas e distribuições de probabilidade, temos um modelo completo.}
                \label{diag:model-based}
            \end{figure}
            
            Assim, o diagrama da Figura~\ref{diag:model-based} é um exemplo de um modelo completo. Sabendo todas as informações sobre o modelo, nós podemos estimar um valor para o estado $s_1$ de acordo com as equações de Bellman, como veremos adiante.
            
        \subsection{Exemplo de modelo incompleto}
    
            Apesar de ser muito semelhante ao diagrama da Figura~\ref{diag:model-based}, a Figura~\ref{diag:model-free} nos indica que não conhecemos as probabilidades de transição. Dessa forma, por não sabermos todas as informações, esse é um exemplo de modelo incompleto. 

            \begin{figure}[ht]
                \centering
                \mdpthreestatenoprobs
                \caption{Apesar de conhecermos todos os etados, ações e recompensas, não sabemos as distribuições de probabilidade, o que é suficiente para caracterizar um problema livre de modelo.}
                \label{diag:model-free}
            \end{figure}

            Nos estudos seguintes veremos maneiras de resolver problemas livres de modelo. Nesse momento nos concentraremos no estudo de uma maneira de resolver problemas em que temos o modelo completo, \emph{Programação Dinâmica}.
    
    \section{Programação Dinâmica}
    
        Antes de entrarmos em aspectos mais técnicos, a programação dinâmica é um método de otimização matemático utilizado inicialmente para resolver problemas de controle. Ela foi desenvolvida por Richard Bellman [1920-1984] na década de 1950. Foi Bellman quem definiu o conceito de Processo de Decisão Markoviano e o utilizou em suas soluções para problemas de otimização de controle. Somente nas décadas finais do século XX é que os trabalhos de Bellman foram considerados como uma parte do que hoje conhecemos como Aprendizado por Reforço.

        Recaptulando dos nossos estudos anteriores, o objetivo final em problemas de Aprendizado por Reforço é encontrar uma política ótima, $\pi_*$, para o agente. Utilizando essa política, temos a garantia de que a valor esperado da recompensa total recebida será o maior possível. E são as \emph{equações de otimalidade de Bellman} que nos dão essa solução.
    
        \subsection{Equações de otimalidade de Bellman}

            A primeira equação que vamos avaliar é a \emph{equação de otimalidade de Bellman para o valor do estado}, dada de acordo com~\eqref{eq:optimal-state-value}. Ela nos dá o valor esperado das recompensas que receberemos a partir de um estado $s$ e seguindo a política ótima $\pi_*$ a partir dele. Observe que em~\eqref{eq:optimal-state-value} consideramos que o agente sempre vai escolher a ação maximal, pois ele visa obter o maior valor esperado.
            \begin{equation}
                \begin{aligned}
                    v_{\pi_*}(s) 
                    & \ = \ \max_a \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi_*}(S_{t+1}) \mid S_t = s, A_t = a \right] \\
                    & \ = \ \max_a \sum_{r, s'} p(s, a, r, s') \left[ r + \gamma v_{\pi_*}(s') \right]
                \end{aligned}
                \label{eq:optimal-state-value}
            \end{equation}
            
            A segunda equação é a \emph{equação de otimalidade de Bellman para o valor da ação} e é dada de acordo com~\eqref{eq:optimal-action-value}. Ela nos dá o valor esperado das recompensas que vamos receber a partir de um estado $s$, executando uma ação $a$ e a partir daí executar a ação de maior valor esperado seguindo a política ótima $\pi_*$.
            \begin{equation}
                \begin{aligned}
                    q_{\pi_*}(s,a) 
                    & \ = \ \mathbb{E} \left[ R_{t+1} + \gamma \max_{a'} q_{\pi_*}(S_{t+1}, a') \bigm\vert S_t = s, A_t = a \right] \\
                    & \ = \ \sum_{r, s'} p(s, a, r, s') \left[ r + \gamma \max_{a'}q_{\pi_*}(s', a') \right]
                \end{aligned}
                \label{eq:optimal-action-value}
            \end{equation}

            Com essas equações, podemos avaliar um estado e um par estado-ação, desde que tenhamos todas as informações necessárias do modelo. Note que nos casos ótimos consideramos a ação escolhida como sendo a que retorna o maior valor. 
            
            Caso não tenhamos a política ótima ou estejamos seguindo outra política $\pi$ qualquer, poderíamos usar as funções de valor para avaliar essa política. É o que faremos a seguir. No caso da Programação Dinâmica, utilizamos uma abordagem em duas etapas: o primeiro será a avaliar a política e depois iremos melhorá-la.
            
    \section{Avaliação da política}
    
        Também conhecido como o \emph{problema da predição}, a \emph{avaliação de uma política} consiste em computar o valor de um estado seguindo uma determinada política $\pi$. Como vimos anteriormente, o valor de um estado $s$ seguindo uma política $\pi$ é dada pela \emph{equação de Bellman} do valor do estado:
        \begin{equation}
            \label{eq:state-value}
            \begin{aligned}
                v_{\pi}(s) 
                & \ = \ \mathbb{E}_{\pi} \left[ G_t \mid S_t = s \right] \\
                & \ = \ \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right] \\
                & \ = \ \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s \right] \\
                & \ = \ \sum_a \pi(s, a) \sum_{r, s'} p(s, a, r, s') \Big[ r + \gamma v_{\pi}(s') \Big] .
            \end{aligned}
        \end{equation}
    
        Dessa forma, conhecendo todas as informações presentes em~\eqref{eq:state-value}, podemos resolver essa equação e encontrar o valor de $v_{\pi}(s)$. Uma maneira de fazer isso é utilizar um sistema de equações.
    
        \subsection{Exemplo de cálculo por sistemas de equações}
        
            Considere para nosso exemplo o MDP representado pelo diagrama da Figura~\ref{diag:model-based}. De início, vamos avaliar cada estado de acordo com a equação de Bellman para o valor do estado. Por exemplo, para o estado $s_1$ temos:
            \begin{equation}
                \label{eq:equation-system}
                \begin{aligned}
                    v_\pi(s_1) 
                        &= \pi(s_1, a_1) \cdot p(s_1, a_1, r_1, s_2) \cdot \Big[ r_1 + \gamma \cdot v_\pi(s_2) \Big] \\
                        &+ \pi(s_1, a_1) \cdot p(s_1, a_1, r_2, s_2) \cdot \Big[ r_2 + \gamma \cdot v_\pi(s_2) \Big] \\
                        &+ \pi(s_1, a_2) \cdot p(s_1, a_2, r_3, s_2) \cdot \Big[ r_3 + \gamma \cdot v_\pi(s_2) \Big] \\
                        &+ \pi(s_1, a_2) \cdot p(s_1, a_2, r_4, s_3) \cdot \Big[ r_4 + \gamma \cdot v_\pi(s_3) \Big] \\
                        &+ \pi(s_1, a_3) \cdot p(s_1, a_3, r_5, s_3) \cdot \Big[ r_5 + \gamma \cdot v_\pi(s_3) \Big] .
                \end{aligned}
            \end{equation}
        
            \emph{Observação: poderíamos considerar todos os estados, ações e transições em~\eqref{eq:equation-system}. Por exemplo, poderíamos pôr um termo para uma transição de $s_1$ a $s_3$. Entretanto, essa transição teria probabilidade 0 de ocorrer. Por isso omitimos esses termos e só pusemos as transições que podem de fato acontecer.}
        
            De maneira análoga, podemos facilmente encontrar as equações de avaliação para os outros estados. Note que as únicas icógnitas em~\eqref{eq:state-value} são os valores dos outros estados. Assim, ao final, teremos $n$ equações com $n$ icógnitas. Uma vez que resolvermos esse sistemas, teremos todos os valores dos estados. Embora, como vimos, esse valor possa ser encontrado partir da resolução de um sistema linear nos estados, essa abordagem pode ser muito custosa. Dessa forma, aqui estamos interessados em computá-lo iterativamente. 
        
        \subsection{Avaliação iterativa da política}
        
            Uma maneira computacionalmente mais adequada ao cálculo do valor dos estados é atualizar o valor da estimativa ao longo de iterações. A estratégia é começar com um valor inicial, $v_{\pi_{0}}$ e melhorar a estimativa do valor atual $v_{\pi_{k+1}}$ a cada iteração $k$. A equação que vamos utilizar é muito parecida com a Equação de Bellman para $v_{\pi}(s)$~\eqref{eq:state-value}. Entretanto, substituiremos o valor  do estado seguinte $s'$, que não conhecemos, por uma estimativa. Especificamente, vamos utilizar a estimativa calculada no passo anterior, $v_{\pi_{k}(s')}$:
            \begin{equation}
            \label{eq:state-value-update}
            \begin{split}
                v_0(s) & \ = \ \text{estimativa inicial} \\
                \downarrow\\
                v_1(s) & \ = \ \sum_{a} \pi(s, a) \sum_{r, s'} p(s, a, r, s') \Big[ r + \gamma \cdot v_0(s') \Big]\\
                \downarrow\\
                v_2(s) & \ = \ \sum_{a} \pi(s, a) \sum_{r, s'} p(s, a, r, s') \Big[ r + \gamma \cdot v_1(s') \Big]\\
                \downarrow\\
                v_3(s) & \ = \ \sum_{a} \pi(s, a) \sum_{r, s'} p(s, a, r, s') \Big[ r + \gamma \cdot v_2(s') \Big]\\
                \vdots
            \end{split}
            \end{equation}
    
            Considerando a estimativa a cada iteração $k$, como em~\eqref{eq:state-value-update}, podemos facilmente definir uma versão iterativa para~\eqref{eq:state-value}. Ela é dada pela regra de atualização~\eqref{eq:iterative-evaluation}.
            \begin{equation}
                \begin{aligned}
                    v_{k+1}(s) & \ = \, \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_k(S_{t+1}) \mid S_t = s \right] \\
                    & = \sum_a \pi(s, a) \sum_{r, s'} p(s, a, r, s') \Big[ r + \gamma v_k(s') \Big] .
                \end{aligned}
                \label{eq:iterative-evaluation}
            \end{equation} 
            Em que a aproximação inicial $v_0(s)$ pode ser escolhida arbitrariamente e o valor de um estado terminal é $0$.

            Observe que estamos atualizando a estimativa da iteração atual $k+1$ baseados na estimativa da iteração anterior $k$. Essa ``estimativa pela estimativa'' caracteriza o \emph{bootstrapping}, como mencionado na nota anterior.
        
    \section{Melhora da política}
    
        A segunda parte da tarefa de encontrar a política ótima em um problema é a \emph{melhora de uma política}, também conhecida como o \textit{problema do controle}. Ela é se refere ao processo de utilizar o valor de um estado para determinar o valor das ações. Agora que nós já temos uma maneira de avaliar uma política, calculando o valor de um estado, podemos utilizá-la para analizar se a política que estamos seguindo é boa ou não. 
       
        Considere que estamos em um estado $s$. A política atual $\pi$ nos dirá a probabilidade de selecionar cada uma das ações possíveis em $s$. Suponha que em vez de seguir $\pi$ nós definimos que uma certa ação $a$ será executada com probabilidade $1$, indenpendentemente das probabilidades dadas por $\pi$. O que nós faremos é construir uma nova política $\pi'$ que é igual à $\pi$ exceto quando estamos no estado s:
        \begin{equation*}
            \pi'(S_t, A_t) =
            \begin{cases}
                \hfil 1 & \text{se}\ S_t = s, A_t = a \\
                \hfil 0 & \text{se}\ S_t = s, A_t \neq a \\
                \pi(S_t, A_t) & \text{c.c.}
            \end{cases}
            .
        \end{equation*}

        Lembre-se que o valor de uma ação é calculada como a recompensa esperada ao executar essa ação a partir de um estado $s$ e seguindo uma política $\pi$, ou seja, a \emph{equação de Bellman} para o valor de uma ação~\eqref{eq:action-value}. Uma vez que já sabemos a ação que deverá ser executada, $a$, não precisamos mais considerar a distribuição de probabilidade de cada uma delas. Assim, diferentemente da equação de valor do estado~\eqref{eq:state-value}, não temos um termo com o somatório de todas as probabilidades para cada ação, $\sum_a \pi(s, a)$.
        \begin{equation}
            \label{eq:action-value}
            \begin{aligned}
                q_{\pi}(s,a) & \ = \ \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s, A_t = a \right] \\
                & \ = \ \sum_{r, s'} p(s, a, r, s') \Big[ r + \gamma v_{\pi}(s') \Big]
            \end{aligned}
        \end{equation}

        Em conjunto com~\eqref{eq:action-value}, podemos avaliar se a escolha de $a$ melhorou ou não a recompensa total esperada. Se $q_{\pi'}(s, a) > v_{\pi}(s)$ (note que usamos $\pi'$ em $q$ e $\pi$ em $v$), podemos afirmar que sim, foi melhor selecionar a ação $a$ do que seguir a política $\pi$. Nesse caso, dizemos que a política $\pi'$ é melhor do que $\pi$.

        Mais ainda, se em vez de escolher \emph{uma} ação que será melhor do que seguir a política atual, podemos selecionar \emph{a} melhor ação entre todas as possibilidades. Ou seja, seguimos a política $\pi$ até chegarmos em $s$ e então selecionamos a ação de maneira gulosa:
        \begin{equation*}
            A_{greedy} \ = \ \argmax_a q_{\pi}(s, a) \ .
        \end{equation*}

        Observe que seguir a política $\pi'$ nunca será pior do que seguir a política $\pi$. Se a probabilidade de selecionar a ação gulosa $a$ de acordo com $\pi$ for menor do que $1$, ou seja, se $\pi(s, a) < 1$, então $\pi'$ será melhor do que $\pi$. Caso $\pi(s, a) = 1$, então $\pi' = \pi$ e $\pi$ já nos dará a melhor ação para o estado $s$.

        Se generalizarmos esse procedimento para todos os estados, selecionaremos as melhores ações possíveis em cada um deles. E quando $\pi' = \pi$ em todos os estados, não há mais possibilidade de melhora, assim tanto $\pi'$ quanto $\pi$ serão políticas ótimas. Esses resultados são provados pelo \emph{teoreama da melhora de política}, cuja ideia é mostrada em~\eqref{eq:policy-improvement-theorem}.
        
        \emph{Observação: aqui abusaremos um pouco da notação de política. Se uma política $\pi$ retorna probabilidade $1$ para uma única ação $a$ e probabilidade $0$ para todas as outras ações possíveis em um estado $s$, ela é uma política \emph{determinística} e dizemos que $a = \pi(s)$.}
        \begin{equation}
            \label{eq:policy-improvement-theorem}
            \begin{split}
                v_{\pi}(s) 
                    & \ \le q_{\pi}(s, \pi'(s)) \\
                    & \ = \mathbb{E} \big[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s, A_t = \pi'(s) \big] \\
                    & \ = \mathbb{E}_{\pi'} \big[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \mid S_t = s \big] \\
                    & \ \le \mathbb{E}_{\pi'} \big[ R_{t+1} + \gamma q_{\pi}(S_{t+1}, \pi'(S_{t+1})) \mid S_t = s \big] \\
                    & \ = \mathbb{E}_{\pi'} \Big[ R_{t+1} + \gamma \mathbb{E} \big[ R_{t+2} + \gamma v_{\pi} (S_{t+2}) \mid S_{t+1}, A_{t+1} = \pi'(S_{t+1}) \big] \mid S_t = s \Big] \\
                    & \ = \mathbb{E}_{\pi'} \big[ R_{t+1} + \gamma R_{t+2} + \gamma^2 v_{\pi}(S_{t+2}) \mid S_t = s \big] \\
                    & \ \le \mathbb{E}_{\pi'} \big[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 v_{\pi}(S_{t+3}) \mid S_t = s \big] \\
                    & \ \vdots \\
                    & \ \le \mathbb{E}_{\pi'} \big[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \cdots \mid S_t = s \big] \\
                    & \ = v_{\pi'}(s) .
            \end{split}
        \end{equation}
    
        Agora temos uma maneira de avaliar uma política, calculando o valor dos estados, e uma maneira de melhorar a política, selecionando as melhores ações em cada estado. Assim, podemos juntar os dois métodos e obter uma política cada vez melhor. É exatamente o que faremos a seguir, utilizando um processo chamado de \emph{Iteração da Política}.
    
    \section{Iteração da política}
    
        Uma vez que temos maneiras de realizar tanto a avaliação quanto a melhora de uma política, i.e., encontrar o valor dos estados e escolher a melhor ação em cada um deles, podemos unir essas duas abordagens e utilizá-las para chegar à política ótima. Essa junção é chamada de \emph{iteração da política}.
        
        A intuição por trás da iteração da política é bem simples. Se conseguimos melhorar a estimativa do valor dos estados (utilizando avaliação da política), podemos usar essas informações para melhor escolher as ações a serem executadas nesses estados (utilizando melhora da política). Repetindo esse processo, teremos estimativas cada vez melhores e melhores escolhas de ação até chegarmos na política ótima. Essa dinâmica é ilustrada na Figura~\ref{fig:policy-iteration}.
        
        \begin{figure}[ht]
            \centering
            \begin{math}
                \pi_0 \xrightarrow{\ \textrm{A} \ } 
                v_{\pi_0} \xrightarrow{\ \textrm{M} \ } 
                \pi_1 \xrightarrow{\ \textrm{A} \ } 
                v_{\pi_1} \xrightarrow{\ \textrm{M} \ } 
                \pi_2 \xrightarrow{\ \textrm{A} \ } 
                \cdots \xrightarrow{\ \textrm{M} \ }
                \pi_* \xrightarrow{\ \textrm{A} \ } v_{\pi_*}
            \end{math}
            \caption{Representação da iteração da política inicial $\pi_0$ até chegar uma política ótima $\pi_*$}
            \label{fig:policy-iteration}
        \end{figure}
        
        Essa abordagem produz uma sequência de políticas em que cada uma delas é certamente melhor que anterior caso ainda não tenhamos uma política ótima, ou igual a anterior se já chegamos em uma política ótima. Como o MDP é finito, temos um número finito de políticas, de modo que essa estratégia chegará eventualmente a uma política ótima.
        
        Observe que a cada iteração temos uma rodada de avaliação e uma de melhora. A etapa de avaliação, por sua vez, consiste em uma série de iterações até a convergência. Dessa forma, o processo todo pode demorar muito e acabar por não ser factível na prática. Uma maneira de simplificar o processo é truncar a fase de avaliação antes da convergência. É o que veremos a seguir.

    \section{Iteração do valor}

        Normalmente, na prática, paramos a atualização do valor dos estados (avaliação da política) quando a diferença entre as estimativas $v_{k+1}$ e $v_k$ é muito pequena. Entretanto, podemos parar a atualização antes disso e ainda ter a garantia de que convergiremos a uma política ótima.

        Uma opção é parar a atualização o mais cedo possível, de modo a gastar o mínimo de tempo com cada etapa. Dessa forma, poderíamos rodar um único passo de atualização da estimativa do valor do estado e em seguida realizar a melhora da política de acordo com o valor da ação. Na prática, realizar a melhora da política após somente uma iteração da avaliação significa combiná-las em uma regra de atualização com um único passo.

        Primeiro, partiremos da regra de atualização iterativa do valor do estado~\eqref{eq:iterative-evaluation}. Como estamos realizando a melhora da política, em vez de considerar todas as ações possíveis consideramos somente a ação que maximiza o valor esperado das recompensas recebidas. Assim, substituimos o termo de soma das probabilidades de cada ação pela escolha da ação que nos dá o maior valor, como visto em~\eqref{eq:value-iteration}.
        
        \begin{equation}
            \begin{aligned}
                v_{k+1}(s) & \ = \ \max_a \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) \mid S_t = s, A_t = a] \\
                & = \ \max_a \sum_{r, s'} p(r, s' \mid s, a) \Big[ r + \gamma v_k(s') \Big]
            \end{aligned}
            \label{eq:value-iteration}
        \end{equation}
        
        Outra forma de ver como chegamos em~\eqref{eq:value-iteration} é partir da equação de otimalidade de Bellman para o valor do estado~\eqref{eq:optimal-state-value}. Nesse caso, definimos uma regra de atualização para ela de maneira análoga ao que fizemos em~\eqref{eq:iterative-evaluation}. Na verdade, essas duas formas de enxergar a iteração do valor fazem parte de uma família de abordagens chamada de \emph{Iteração de Política Generalizada}.
    
    \section{Iteração de Política Generalizada}
    
        \emph{Iteração de Política Generalizada} (GPI) é o termo que abrange todas as maneiras de encontrar uma política ótima de maneira iterativa através de sucessivas avaliações e melhoras, em que a cada passo estaremos cada vez mais próximos dos valores ótimos. Esse processo é ilustrado na Figura~\ref{diag:generalized-policy-iteration}. 
        
        \vspace{5mm}
        \begin{figure}[ht]
            \centering
            \begin{tikzpicture}[->,>=latex,shorten >=1pt,auto,node distance=2.8cm, thick]
                \tikzstyle{state}=[fill=none,draw=none,text=black]
                
                \node[state] (A) {\Large$\boldsymbol{\pi}$};
                \node[state] (B) [right of=A, xshift=0.75cm] {\Large$\mathbf{V}$};
                \node[state] (DOTS) [below of=A, xshift=1.75cm, yshift=0.5cm] {\textbf{\vdots}};
                \node[state] (A2) [below of=A, yshift=-0.5cm] {\Large$\boldsymbol{\pi_{*}}$};
                \node[state] (B2) [below of=B, yshift=-0.5cm] {\Large$\mathbf{v_{*}}$};
            
                \draw[bend left=70] (A) to node[above] {evaluation} (B);
                \draw[bend left=60, draw=none] (A) to node[below] {$V 	\rightsquigarrow v_{\pi}$} (B);
                \draw[bend left=70] (B) to node[below] {improvement} (A);
                \draw[bend left=60, draw=none] (B) to node[above] {\small$\pi \rightsquigarrow$ greedy($V$)} (A);
                
                \draw[transform canvas={yshift=0.15cm}] (A2) to node[] {} (B2);
                \draw[transform canvas={yshift=-0.15cm}] (B2) to node[] {} (A2);
            \end{tikzpicture}
            \caption{Representação da dinâmica de atualização da política na abordagem de GPI. A convergência ocorre quando encontrarmos uma política ótima $\pi_*$.}
            \label{diag:generalized-policy-iteration}
        \end{figure}
        
        Iniciamos o processo com duas estimativas iniciais para a política $\pi$ e o valor de cada estado $V$. À medida que as atualizações vão ocorrendo (avaliação e melhora da política), os valores vão convergindo. No momento em que eles convergem, teremos a política ótima $\pi_*$ e os valores ótimos dos estados $v_*$. Se continuássemos com as rodadas de avaliação e melhora, nada mudaria.
        
        Utilizando GPI, encontraremos uma política ótima, que é o nosso objetivo final. Entretanto, todos os processos da abordagem utilizando Programação Dinâmica descritos até aqui só foram possíveis porque tínhamos o modelo completo. Caso não tenhamos o modelo completo, precisamos utilizar outras formas de aprendizado. Veremos a seguir uma abordagem que utiliza amostragem aleatória: \emph{Métodos de Monte Carlo}.
        
\end{document}