\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}

\usepackage{amsmath,amssymb}

\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{subcaption}

\input{pb_rlstyle_tikzgraph.tex}


% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        \vskip 1em
        {\large \textbf{\@author} \par}
        \vskip 1em
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother


\title{Notas de estudo - Aprendizado por Reforço}
\author{Parte 05 - Métodos de Monte-Carlo}
\date{Paulo Bruno de Sousa Serafim - Out/19 - Jan/20}


\begin{document}

\maketitle

    \section{Métodos \emph{baseados em modelos} vs. Métodos \emph{livres de modelo}}
    
    
            \begin{figure}[h]
                \centering
                \begin{subfigure}{.3\linewidth}
                    \centering
                    \begin{tikzpicture}[-,>=stealth', auto, node distance=1.5cm, thick]
                        \node[state-node]  (SimpleBanditS1) {$s$};
                        \node[reward-node] (SimpleBanditR1) [below of=SimpleBanditS1, xshift=-1.0cm] {$r$};
                        \node[reward-node] (SimpleBanditR2) [below of=SimpleBanditS1, xshift=0.0cm]  {$r$};
                        \node[reward-node] (SimpleBanditR3) [below of=SimpleBanditS1, xshift=1.0cm]  {$r$};
                        
                        \draw[bend right] (SimpleBanditS1) to node[left, pos=0.8] {$p$} (SimpleBanditR1);
                        \draw             (SimpleBanditS1) to node[pos=0.8] {$p$} (SimpleBanditR2);
                        \draw[bend left]  (SimpleBanditS1) to node[right, pos=0.8] {$p$} (SimpleBanditR3);
                        
                        \draw[hidden-edge, bend right] (SimpleBanditS1) to node[action-label, pos=0.4] {$a$} (SimpleBanditR1);
                        \draw[hidden-edge]             (SimpleBanditS1) to node[action-label, pos=0.4] {$a$} (SimpleBanditR2);
                        \draw[hidden-edge, bend left]  (SimpleBanditS1) to node[action-label, pos=0.4] {$a$} (SimpleBanditR3);
                    \end{tikzpicture}
                    \caption{Modelo conhecido.}
                \end{subfigure}
                \begin{subfigure}{.3\linewidth}
                    \centering
                    \begin{tikzpicture}[-,>=stealth', auto, node distance=1.5cm, thick]
                        \node[state-node]  (SimpleBanditS1) {$s$};
                        \node[reward-node] (SimpleBanditR1) [below of=SimpleBanditS1, xshift=-1.0cm] {$r$};
                        \node[reward-node] (SimpleBanditR2) [below of=SimpleBanditS1, xshift=0.0cm]  {$r$};
                        \node[reward-node] (SimpleBanditR3) [below of=SimpleBanditS1, xshift=1.0cm]  {$r$};
                        
                        \draw[bend right] (SimpleBanditS1) to node[left, pos=0.8] {$?$} (SimpleBanditR1);
                        \draw             (SimpleBanditS1) to node[pos=0.8] {$?$} (SimpleBanditR2);
                        \draw[bend left]  (SimpleBanditS1) to node[right, pos=0.8] {$?$} (SimpleBanditR3);
                        
                        \draw[hidden-edge, bend right] (SimpleBanditS1) to node[action-label, pos=0.4] {$a$} (SimpleBanditR1);
                        \draw[hidden-edge]             (SimpleBanditS1) to node[action-label, pos=0.4] {$a$} (SimpleBanditR2);
                        \draw[hidden-edge, bend left]  (SimpleBanditS1) to node[action-label, pos=0.4] {$a$} (SimpleBanditR3);
                    \end{tikzpicture}                    
                    \caption{Modelo desconhecido.}
                \end{subfigure}
                \caption{Teste}
            \end{figure}


    \section{Introdução}
    
        Mostrar grafos de MDP's sem probabilidades e com próximos estados desconhecidos.
        
        \subsection{Monte-Carlo ``primeira-visita''}
        
        \subsection{Monte-Carlo ``cada-visita''}
    
    \section{Estimativas dos valores das ações}
    
        Relembrando dos estudos de \emph{Multi-Armed Bandits}, considere o caso de uma máquina com uma única alavanca (uma ação). Como poderíamos estimar o valor esperado dessa única ação sem conhecer a distribuição de probabilidades? 
        
        Uma maneira de fazer isso é executar a ação várias vezes e a partir daí fazer estimativas. Quando o número de ações tende ao infinito, o valor da estimativa tende ao valor esperado. Em algum ponto intermediário teremos uma boa estimativa do valor esperado. Dessa forma, teremos uma boa estimativa do valor esperado em um aprendizado livre de modelo.

        Agora considere um problema modelado por um MDP, como o da Figura~\ref{}. Em cada estado múltiplas ações podem ser possíveis, de modo que começar de um estado não seria suficiente para explorarmos o modelo de maneira satisfatória. Para isso, devemos levar em consideração também as executadas a partir de um estado. Assim, contamos como partida os pares de estado e ação $(s, a)$.
        
        Observe que um jogo como xadrez tem sempre o mesmo estado inicial. Dessa forma, começando pelo único estado inicial teríamos uma tendência (\emph{bias}) para alguns pares $(s, a)$. Uma maneira de contornar esse problema é usar como partida qualquer par de $(s, a)$ e não somente o estado estado inicial.
    
        Explicar o que é \textit{exploring starts}
    
    \section{Controle Monte-Carlo}
    
    \section{Iteração de Política Generalizada}
    
        \begin{center}
            \begin{math}
                \pi_0 \xrightarrow{\ \textrm{E} \ } 
                q_{\pi_0} \xrightarrow{\ \textrm{I} \ } 
                \pi_1 \xrightarrow{\ \textrm{E} \ } 
                q_{\pi_1} \xrightarrow{\ \textrm{I} \ } 
                \pi_2 \xrightarrow{\ \textrm{E} \ } 
                \cdots \xrightarrow{\ \textrm{I} \ }
                \pi_* \xrightarrow{\ \textrm{E} \ } q_{*}
            \end{math}
        \end{center}
    
    \section{Métodos \textit{On-policy} vs. \textit{Off-policy}}
    
    \section{Árvore de Busca de Monte-Carlo (MCTS)}
    
        \subsection{Técnicas de \textit{Rollout}}
        
        \subsection{Algoritmo}
        
        \subsection{AlphaGo}
        
            Um exemplo recente do uso de simulações de monte Carlo em Aprendizado por Reforço foi o alphago. Nele, houve um uso híbrido de um aprendizado supervisionado (verificar) e um algoritmo baseado em simulações de Monte Carlo chamado MCTS.

            Assim, sendo um bom exemplo ilustrutativo, além de ter seu uso ainda hoje, vamos descrever o que é o algoritmo MCTS.

\end{document}
