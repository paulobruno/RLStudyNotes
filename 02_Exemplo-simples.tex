\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}

\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{subcaption}

\usepackage{amsmath,amssymb}
\DeclareMathOperator*{\argmax}{argmax}

\input{pb_rlstyle_tikzgraph.tex}

% text spacing
\linespread{1.3}
\setlength{\parindent}{4em}
\setlength{\parskip}{0.75em}


\newcommand{\todo}[1]{ --\textcolor{red}{\textbf{#1}}--}
%\newcommand{\todo}[1]{}


% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        %\vskip 0.5em
        {\large \textbf{\@author} \par}
        %\vskip 0.5em
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother


\title{Notas de estudo - Aprendizado por Reforço}
\author{Parte 02 - Problemas \emph{Multi-Armed Bandits}}
\date{Paulo Bruno Serafim - Fev/20}


\begin{document}

\maketitle

    \section{\textit{Multi-armed bandit}}
    
        \subsection{\emph{One-armed bandit}}
        
            Vamos considerar o caso mais simples, em que só temos um estado e uma ação. Apesar da ação ser única, ela pode dar recompensas diferentes com probabilidades diferentes. Na literatura, esse problema é conhecido como \emph{one-armed bandit}, cuja tradução para o português é "máquina caça-níquel". Assim, executar a ação seria equivalente a puxar a alavanca de uma caça-níquel e receber alguma recompensa de acordo com uma distribuição de probabilidade interna da máquina.

            Por exemplo, suponha que ao executar a ação temos $80\%$ de chance receber $0$, $19\%$ de receber $2$ e $1\%$ de receber $50$. Ao longo dessas notas, representaremos esses problemas através de diagramas que mostrem estados, ações, recompensas e probabilidades. Nesse exemplo, o diagrama é:
            
            \begin{center}
            \begin{tikzpicture}[-,>=stealth', auto, node distance=1.5cm, thick]
                \node[state-node]  (SimpleBanditS1) {$s$};
                \node[action-node] (SimpleBanditA1) [below of=SimpleBanditS1, yshift=0.5cm] {$a$};
                \node[reward-node] (SimpleBanditR1) [below of=SimpleBanditA1, xshift=-1.5cm] {$0$};
                \node[reward-node] (SimpleBanditR2) [below of=SimpleBanditA1, xshift=0.0cm]  {$2$};
                \node[reward-node] (SimpleBanditR3) [below of=SimpleBanditA1, xshift=1.5cm]  {$50$};
                
                \draw[] (SimpleBanditS1) to node[] {} (SimpleBanditA1);
                \draw[bend right] (SimpleBanditA1) to node[left] {$80\%$} (SimpleBanditR1);
                \draw             (SimpleBanditA1) to node[left]     {$19\%$} (SimpleBanditR2);
                \draw[bend left]  (SimpleBanditA1) to node[]      {$1\%$} (SimpleBanditR3);
            \end{tikzpicture}
            \end{center}

            O que queremos saber é: qual é o \emph{valor} da ação $a$? Por definição, o valor de uma ação é o valor esperado das recompensas recebidas quando a ação escolhida $A$ é $a$:
            \begin{equation}
            \begin{split}
                q_*(a) & \ = \ \mathbb{E}[R \mid A = a] \\
                & \ = \ \sum_{r}{} p(r) \cdot r
            \end{split}
            \end{equation}        
            Para o exemplo anterior, temos:            
            \begin{equation*}
            \begin{split}
                q_*(a) & \ = \ \sum_{r}{} p(r) \cdot r \\
                & = \ 0.8 \cdot 0 + 0.19 \cdot 2 + 0.01 \cdot 50 \\
                & = \ 0.88
            \end{split}
            \end{equation*}
            
        \subsection{\emph{Multi-armed bandit}}
    
            Vamos agora considerar o caso em que estamos diante de múltiplas máquinas caça-níques. Nesse caso, escolher uma máquina significa ir até ela (estado) e puxar a sua alavanca (ação), de modo que estaremos em um único estado por vez. Esse problema é equivalente a ter um único estado, mas múltiplas ações possíveis. A dinâmica de interação agora envolve também escolher uma ação a ser executada. Portanto, precisamos nos atentar com qual ação executar imediatamente e quais as recompensas esperadas, ou seja, o seu valor. Se existirem $k$ ações possíveis, podemos executar qualquer uma delas e receberemos alguma recompensa de acordo com uma distribuição de probabilidade. O diagrama a seguir representa essa dinâmica:
        
            \begin{center}
                \simplebandit
            \end{center}
                    
            Note que nesse caso podemos considerar que estamos diante de uma máquina caça-níquel com múltiplas alavancas, ou seja, uma \emph{multi-armed bandit}. Esse é um problema bem conhecido e muito estudado na literatura, muitas vezes referido somente como \emph{MAB}. Imagine que você vá jogar em uma caça-níquel com múltiplas alavancas. Cada máquina dará resultados diferentes com probabilidades diferentes, mas você tem um documento contendo as recompensas possíveis e suas probabilidades para cada máquina. Você só pode jogar uma única vez. Qual máquina você escolheria?
        
            A escolha ótima, i.e., a escolha racional que potencialmente dará o maior valor, é aquela cujo retorno esperado é o maior. Em outras palavras, você deve escolher a ação cujo valor seja o maior. 

            Por exemplo, considere o problema:

            \begin{center}
            \begin{tikzpicture}[-, >=stealth', auto, node distance=1.5cm, thick]
                \node[state-node] (S1) {$s$};
                \node[action-node] (A1) [below of=S1, xshift=-3.5cm] {$a_1$};
                \node[action-node] (A2) [below of=S1, xshift=0.0cm]  {$a_2$};
                \node[action-node] (A3) [below of=S1, xshift=3.5cm]  {$a_3$};                
                \node[reward-node] (A1R1) [below of=A1, xshift=-1.5cm] {$0$};
                \node[reward-node] (A1R2) [below of=A1, xshift=0.0cm]  {$2$};
                \node[reward-node] (A1R3) [below of=A1, xshift=1.5cm]  {$50$};
                \node[reward-node] (A2R1) [below of=A2, xshift=-1.0cm] {$0$};
                \node[reward-node] (A2R2) [below of=A2, xshift=1.0cm]  {$5$};
                \node[reward-node] (A3R1) [below of=A3, xshift=-1.5cm] {$0.5$};
                \node[reward-node] (A3R2) [below of=A3, xshift=0.0cm]  {$1$};
                \node[reward-node] (A3R3) [below of=A3, xshift=1.5cm]  {$2$};
                
                \draw[bend right] (S1) to node[] {} (A1);
                \draw[]           (S1) to node[] {} (A2);
                \draw[bend left]  (S1) to node[] {} (A3);

                \draw[bend right]    (A1) to node[left]  {$0.80$} (A1R1);
                \draw                (A1) to node[left]  {$0.19$} (A1R2);
                \draw[bend left]     (A1) to node[right] {$0.01$} (A1R3);
                \draw[bend right]    (A2) to node[right] {$0.9$}  (A2R1);
                \draw[bend left]     (A2) to node[right] {$0.1$}  (A2R2);
                \draw[bend right]    (A3) to node[left]  {$0.5$}  (A3R1);
                \draw                (A3) to node[left]  {$0.3$}  (A3R2);
                \draw[bend left]     (A3) to node[right] {$0.2$}  (A3R3);
            \end{tikzpicture}
            \end{center}
            Qual ação escolher, $a_1$, $a_2$ ou $a_3$? Naturalmente, vamos calcular o valor de cada ação e a partir deles tomar alguma decisão. A decisão natural é escolher a ação com maior valor, que é chamada de ação \emph{gulosa}:
            \begin{equation}
                A_{greedy} \ = \ \argmax_a q_*(a)
            \end{equation}
            
            Calculando os valores das ações do exemplo anterior, temos:
            \begin{equation*}
            \begin{split}
                q_*(a_1) \ = \ 0.88 \\
                q_*(a_2) \ = \ 0.50 \\
                q_*(a_3) \ = \ 0.95
            \end{split}
            \end{equation*}
            Portanto, a ação gulosa é $a_3$.
    
        \subsection{Estimativa do valor da ação}

            Até agora todos as probabilidades e recompensas eram conhecidas. Se pensarmos em um caso mais real, ao encontrarmos $k$ máquinas saberíamos que temos $k$ ações possíveis, mas sem conhecer as probabilidades e recompensas de cada uma delas. De que modo poderíamos descobrir qual é a melhor ação após interagir com elas?

            Inicialmente, não temos nenhuma informação sobre o valor, então qualquer ação é iguamente boa. Para descobrir informações de cada ação teríamos que executá-las algumas vezes e, a partir daí, ter alguma informação para definir uma estratégia de escolha adequada. Vamos definir essa estimativa como a média das recompensas recebidas até o tempo $t$ após executar uma ação, dada por:
            \begin{equation}
            \label{eq:estimativa-acao}
                Q_t(a) \ = \ \frac{\sum\limits_{i=1}^{t-1} R_i(a)}{N_{t-1}(a)}
            \end{equation}
            onde $r_i(a)$ é a recompensa recebida por executar $a$ no tempo $i$, ou $0$ se $a$ não foi executada em $i$, e $N_{t-1}(a)$ é o número de vezes que $a$ foi executada até o tempo $t-1$. Assim, à medida que as interações forem ocorrendo, teremos uma estimativa melhor do valor real de uma ação. Com $t$ tendendo ao infinito, $Q_t$ tende ao valor real $q_*$.

            Considere um MAB com duas ações. Definimos uma estratégia em que as três primeiras jogadas serão na ação $a_1$ e as três jogadas seguinte serão em $a_2$, de forma a obter algum informação do valor delas. Para cada passo vamos calcular o valor estimado de cada ação. Suponha que obtivemos os seguintes resultados:
            \begin{tabbing}
                \hspace{4.0cm}\=~~~~~~~~~~~~ \=~~~~~~~~~~~~ \=~~~~~~~~~~~~ \=~~~~~~~~~~~~~~~~~~~ \= \kill
                \> $k = 1$   \> $a = 1$   \> $r = 1$   \> $Q_1(a_1) = 1$           \> $Q_1(a_2) = 0$\\
                \> $k = 2$   \> $a = 1$   \> $r = 3$   \> $Q_2(a_1) = 2$           \> $Q_2(a_2) = 0$\\
                \> $k = 3$   \> $a = 1$   \> $r = 1$   \> $Q_3(a_1) = \frac{5}{3}$ \> $Q_3(a_2) = 0$\\
                \> $k = 4$   \> $a = 2$   \> $r = 0$   \> $Q_4(a_1) = \frac{5}{3}$ \> $Q_4(a_2) = 0$\\
                \> $k = 5$   \> $a = 2$   \> $r = 0$   \> $Q_5(a_1) = \frac{5}{3}$ \> $Q_5(a_2) = 0$\\
                \> $k = 6$   \> $a = 2$   \> $r = 0$   \> $Q_6(a_1) = \frac{5}{3}$ \> $Q_6(a_2) = 0$
            \end{tabbing}
            O que podemos inferir desses dados é que em $a_1$ duas recompensas são possíveis, $1$ com $\frac{2}{3}$ de probabilidade e $3$ com $\frac{1}{3}$ de probabilidade, de modo que $Q_6(a_1) = \frac{5}{3}$. Já para $a_2$ temos recompensa $0$ com $100\%$ de probabilidade, de modo que $Q_6(a_2) = 0$. Portanto, a ação gulosa nesse caso seria $a_1$.
            
            Suponha que jogamos mais quatro vezes, duas em $a_1$ e duas em $a_2$, e obtivemos os resultados:
            \begin{tabbing}
                \hspace{4.0cm}\=~~~~~~~~~~~~ \=~~~~~~~~~~~~ \=~~~~~~~~~~~~ \=~~~~~~~~~~~~~~~~~~~ \= \kill
                \> $k = 7$   \> $a = 1$   \> $r = 1$   \> $Q_7(a_1) = 1.5$  \> $Q_7(a_2) = 0$\\
                \> $k = 8$   \> $a = 1$   \> $r = 3$   \> $Q_8(a_1) = 1.8$  \> $Q_8(a_2) = 0$\\
                \> $k = 9$   \> $a = 2$   \> $r = 1000$\> $Q_9(a_1) = 1.8$  \> $Q_9(a_2) = 250$\\
                \> $k = 10$  \> $a = 2$   \> $r = 0$   \> $Q_{10}(a_1) = 1.8$ \> $Q_{10}(a_2) = 200$
            \end{tabbing}
            Os novos valores das estimativas são:
            \begin{equation*}
            \begin{split}
                Q_{10}(a_1) \ = \ 1.8\\
                Q_{10}(a_2) \ = \ 200
            \end{split}
            \end{equation*}
            Portanto, a ação gulosa nesse caso seria $a_2$. 
            
            A título de informação, a configuração exata que eu pensei desse MAB é:
            \begin{center}
            \begin{tikzpicture}[-, >=stealth', auto, node distance=1.5cm, thick]
                \node[state-node] (S1) {$s$};
                \node[action-node] (A1) [below of=S1, xshift=-2.5cm] {$a_1$};
                \node[action-node] (A2) [below of=S1, xshift=2.5cm]  {$a_2$};

                \node[reward-node] (A1R1) [below of=A1, xshift=-1.0cm] {$3$};
                \node[reward-node] (A1R2) [below of=A1, xshift=1.0cm]  {$1$};
                \node[reward-node] (A2R1) [below of=A2, xshift=-1.0cm] {$0$};
                \node[reward-node] (A2R2) [below of=A2, xshift=1.0cm]  {$1000$};
                
                \draw[bend right] (S1) to node[] {} (A1);
                \draw[bend left]  (S1) to node[] {} (A2);

                \draw[bend right]    (A1) to node[left]  {$0.25$} (A1R1);
                \draw[bend left]     (A1) to node[right] {$0.75$} (A1R2);
                \draw[bend right]    (A2) to node[left]  {$0.99$} (A2R1);
                \draw[bend left]     (A2) to node[right] {$0.01$} (A2R2);
            \end{tikzpicture}
            \end{center}
            com:
            \begin{equation*}
            \begin{split}
                q_*(a_1) \ = \ 0.25 \cdot 3 + 0.75 \cdot 1 = 1.5 \\
                q_*(a_2) \ = \ 0.99 \cdot 0 + 0.01 \cdot 1000 = 10
            \end{split}                
            \end{equation*}

            A estratégia que definimos de executar a ação $a_1$ nas primeiras $n$ jogadas e a ação $a_2$ nas $n$ jogadas seguintes, ou, de maneira mais geral, escolher ações sem se basear na estimativa e em seguida definir as ações gulosas, pode parecer boa. Contudo, um dos problemas dessa estratégia é o que acontece se uma recompensa tiver baixa probabilidade, como em $a_2$. 

            Não seria difícil acontecer de uma recompensa com $1\%$ de probabilidade não ocorrer nas primeiras 15 execuções, por exemplo. Assim, poderíamos acreditar que $q_*(a_1) > q_*(a_2)$, o que não seria verdade. Dessa forma, sempre executar a ação gulosa após poucas estimativas iniciais, ou seja, explorar o resultado já conhecido, pode não ser bom. Uma maneira de contornar esse problema seria eventualmente executar ações de menor estimativa, ou seja, prospectar novos resultados, de modo que teríamos mais chances de ver que $q_*(a_2) > q_*(a_1)$.

            O exemplo anterior ilustra bem o quão importante é ter mais informações sobre cada estado para não fazer escolhas inadequadas. Além disso, evidencia a dúvida de quando parar de prospectar informações e passar a explorar os resultados de maneira gulosa. Na literatura, esse problema é conhecido como o \emph{Dilema Prospecção vs. Exploração}.
    
    \section{Dilema Prospecção vs. Exploração}
    
        \noindent
        Pegando emprestado um exemplo que não lembro mais onde vi, imagine que você goste bastante de uma lanchonete que tem um ótimo hambúrguer. Você pode continuar sempre indo na mesma lanchonete. Você sempre vai comer um ótimo hambúrguer, mas pode ser que exista outra lanchonete que tenha um hambúrguer ainda melhor e você não vai saber. Você poderia decidir, por exemplo, nunca repetir uma lanchonete, assim você conheceria ótimas lanchonetes com ótimos hambúrguers, mas algumas vezes iria comer sanduíches ruins.
        
        Esse exemplo ilustra o dilema entre sempre escolher uma boa opção familiar (a mesma lanchonete), ou seja, \textbf{explorar} aquilo que já é conhecido, e \textbf{prospectar} (mudar de lanchonete), ou seja, testar outras opções ainda não visitadas. 
    
        \subsection{Nota de tradução}
        
            \begin{itemize}
                \item \textit{``Exploration''} = Prospecção
                \item \textit{``Exploitation''} = Exploração
            \end{itemize}

            Em inglês esse problema é conhecido como ``Exploration vs. Exploitation Dilemma''. O termo em português comum tanto para ``exploration'', quanto para ``exploitation'' é ``exploração''. Eu gosto de traduzir ``exploration'', a busca por informações novas, como ``prospecção'' e ``exploitation'', tirar proveito das informações conhecidas, como ``exploração''.
                
        \subsection{Estratégias de balanceamento}
        
            As estratégias para lidar com o dilema anterior se baseiam em executar ações de maneira gulosa na maior parte do tempo, mas ainda executar ações não-gulosas algumas vezes.
        
            \subsubsection{Estratégias $\boldsymbol\varepsilon$}
            
                \begin{itemize}
                    \item \textbf{$\boldsymbol\varepsilon$-greedy}: executa a ação gulosa em uma porcentagem $(1-\varepsilon)$ e executa uma ação aleatória $\varepsilon$. Ex.: seja $\varepsilon = 0.1$, a ação a ser executada terá $90\%$ de chance de ser gulosa e $10\%$ de ser aleatória.
                    \item \textbf{$\boldsymbol\varepsilon$-first}: para um conjunto N de ações, executa $(1 - \varepsilon)N$ ações gulosas e executa $(\varepsilon N)$ ações não-gulosas. Ex.: sejam $\varepsilon = 0.2$ e $N = 10$, 8 ações serão gulosas e 2 ações serão aleatórias.
                    \item \textbf{$\boldsymbol\varepsilon$-decay}: No início do aprendizado não há ainda uma boa distribuição das recompensas, de modo que os valores tendem a ser menos confiáveis, assim faz sentido prospectar as ações. Após algumas iterações, a recompensa já terá sido mais distribuída, de modo que faz sentido termos uma fase de exploração maior. Após várias iterações a distribuição de recompensa será bem mais confiável, logo a fase de exploração deverá ser máxima. Essa estratégia é representada usando um $\varepsilon_{I}$ alto nas primeiras $I$ iterações (possivelmente $1$), um $\varepsilon_{F}$ bem baixo a partir da iteração $F$ e um $\varepsilon$ que diminui (decai) de $\varepsilon_{I}$ a $\varepsilon_{F}$ entre $I$ e $F$. O tipo de decaimento mais comum é o linear, representado na Figura \ref{diag:epsilon-decay}, com $\varepsilon_{I} = 1.0$, $\varepsilon_{F} = 0.1$, $I = 5$ e $F = 30$.
                    
                    \begin{figure}[ht]
                        \centering
                        \begin{tikzpicture}
                            \begin{axis}[
                                height=4.5cm,
                                width=6.0cm,
                        		xlabel=\textbf{\normalsize Épocas},
                        		ylabel=\Large$\boldsymbol{\varepsilon}$,
                        		xmin=0, xmax=50,
                        		ymin=0.0, ymax=1.1,
                        		xtick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                        		ytick={0.0, 0.2, 0.4, 0.6, 0.8, 1.0},
                        		tick align=center,
                        		xtick pos=bottom,
                        		ytick pos=left,
                        		yticklabel style={/pgf/number format/.cd, fixed, fixed zerofill, precision=1},
                        		font=\tiny, 
                                no markers,
                                every axis plot/.append style={ultra thick},
                            ]
                                \addplot[color=black] coordinates {
                            		( 0, 1.0)
                            		( 5, 1.0)
                            		(30, 0.1)
                            		(50, 0.1)
        	                    };
                            \end{axis}
                        \end{tikzpicture}
                        
                        \caption{$\varepsilon$ começa em $1.0$ e decai linearmente até $0.1$.}
                        \label{diag:epsilon-decay}
                    \end{figure}
                \end{itemize}
            
        \subsubsection{Limite de Confiança Superior (UCB)}
        
            Outra estratégia bastante utilizada leva em consideração o quanto cada ação não-gulosa foi escolhida/testada. Em vez de escolher uma ação não-gulosa de acordo uma mesma probabilidade, vamos escolher uma ação não-gulosa de acordo com o seu \emph{potencial} em ser ótima.
            
            Dois fatores influenciam a estimativa de uma ação. O primeiro, naturalmente, é o valor estimado e o segundo é a certeza que temos de que o primeiro é uma boa estimativa. Por exemplo, estimativas com uma grande incerteza associada podem ser bem maiores ou bem menores do que o estimado.
            
            Assim, podemos adicionar um termo que considere também a incerteza do valor estimado, de modo que a escolha de uma ação no tempo $t$, $A_t$, passe a ser baseada em:            
            \begin{equation}
                A_t \ = \ \argmax_a \Big[ Q_t(a) + Incerteza(Q_a) \Big]
            \end{equation}           
            
            Note que se $Incerteza(Q_a) = 0$, então $A_t$ é o valor real da ação $a$. Se $Incerteza(Q_a) > 0$, então o valor de $a$ pode ser maior do que $Q_t(a)$, mas no máximo $A_t$. Portanto, o termo de incerteza define o limite superior do valor verdadeiro de $a$. No caso do Limite de Confiança Superior (\emph{Upper Confidence Bound}, UCB), a incerteza é dada por:            
            \begin{equation}
                Incerteza(Q_a) \ = \ c \cdot \sqrt{\frac{\ln{t}}{N_t(a)}}
            \end{equation}
            
            E, portanto, a escolha de uma ação é dada por:            
            \begin{equation}
                A_t \ \dot{=} \ \argmax_a \left[ Q_t(a) + c \cdot \sqrt{\frac{\ln{t}}{N_t(a)}} \ \right]
            \end{equation}            
            \noindent
            onde $c$ é o \emph{nível de confiança} e $N_t(a)$ é o número de vezes que a ação $a$ foi escolhida até o tempo $t$. Observe que quando $a$ for escolhida, o valor da incerteza diminui, pois $N_t(a)$ aumenta mais do que $\ln{t}$, e se $a$ não for escolhida o valor da incerteza aumenta, pois $N_t(a)$ se mantém e $\ln{t}$ aumenta.
            
    \section{Versão incremental}

        Lembrando de \eqref{eq:estimativa-acao}, uma maneira ingênua que poderíamos utilizar no cálculo da estimativa $Q_t(a)$ pode ser guardar todos os valores obtidos até o momento, calcular a soma dos valores e dividir por pelo número de vezes em que a ação foi executada. Em termos de implementação, teríamos um inteiro para guardar $N_t(a)$ e um array de valores para todos os $Q_t(a)$. Naturalmente, esse array crescerá a cada iteração, de modo que o uso de memória aumentará com o tempo. Não seria particularmente uma boa solução.
        
        Uma maneira de evitar usar um array de tamanho variável e manter o uso de memória constante é guardar a soma de $Q_t(a)$, $Soma_t(a)$ e atualizá-la a cada iteração:
        \begin{equation}
        \label{eq:estimativa-soma}
        \begin{split}
            Soma_{t+1}(a) & \ = \ Soma_t(a) + Q_t(a) \\
            Q_{t+1}(a) & \ = \ \frac{Soma_{t+1}(a)}{N_{t+1}(a)}  \\
        \end{split}
        \end{equation}

        Para simplificar a notação, vamos considerar somente uma única ação, de modo que $(a)$ se torna redundante, e somente as vezes em que ela foi executada, $n$. Assim, o cálculo da estimativa para essa ação após ter sido executada $n-1$ vezes é:
        \begin{equation}
        \begin{split}
            Q_n & \ \dot{=} \ \frac{R_1 + R_2 + \cdots + R_{n-1}}{n - 1}\\
            & \dot{=} \frac{1}{n-1} \sum_{i=1}^{n-1} R_i
        \end{split}
        \end{equation}
        
        De \eqref{eq:estimativa-soma}, temos uma versão iterativa que utiliza memória constante. Entretanto, uma vez que o valor de $Q_t(a)$ já é guardado, poderíamos atualizá-lo diretamente, sem a necessidade da soma $Soma_t(a)$:
        
        \begin{subequations}
        \begin{align}
            Q_{n+1} & \ \dot{=} \ \frac{1}{n} \sum_{i=1}^{n} R_i\\
            & = \ \frac{1}{n} \Bigg( R_n + \sum_{i=1}^{n-1} R_i \Bigg) \nonumber\\
            & = \ \frac{1}{n} R_n + \frac{1}{n} \sum_{i=1}^{n-1} R_i \nonumber\\
            & = \ \frac{1}{n} R_n + \frac{1}{n} \frac{(n-1)}{(n-1)} \sum_{i=1}^{n-1} R_i \nonumber\\
            & = \ \frac{1}{n} R_n + \frac{(n-1)}{n} \frac{1}{(n-1)} \sum_{i=1}^{n-1} R_i \nonumber\\
            & = \ \frac{1}{n} R_n + \frac{(n-1)}{n} Q_n \nonumber\\
            & = \ \frac{1}{n} R_n + \Bigg( \frac{n}{n} - \frac{1}{n} \Bigg) Q_n \nonumber\\
            & = \ \frac{1}{n} R_n + Q_n - \frac{1}{n} Q_n \nonumber\\
            \label{eq:estimativa-qn}
            & = \ Q_n + \frac{1}{n} \Big[ R_n - Q_n \Big]
        \end{align}
        \end{subequations}
        
        A forma de \eqref{eq:estimativa-qn} segue um padrão muito comum a várias técnicas de aprendizado por reforço:
        \begin{equation}
            NovaEstimativa \leftarrow AntigaEstimativa + TamanhoPasso \Big[ Objetivo - AntigaEstimativa \Big]
        \end{equation}
            
    \section{Problemas não-estacionários}

        Em um problema \emph{estacionário} os valores reais das ações se mantêm os mesmos ao longo de todas as iterações. Já em um problema \emph{não-estacionário} os valores das ações mudam ao longo do tempo, seja por uma mudança nas probabilidades, seja por uma mudança nas recompensas, ou por ambos.
        
        Por exemplo, um simples problema não-estacionário pode ser visto no diagrama \ref{diag:non-stationary} abaixo. Note que $p_1 \neq p_2$ e $q_1 \neq q_2$, caso contrário seria um problema estacionário. 
        \begin{figure}[h]
            \centering
            \begin{subfigure}{.3\linewidth}
                \centering
                \begin{tikzpicture}[-,>=stealth', auto, node distance=1.5cm, thick]
                    \node[state-node]  (S1) {$s$};
                    \node[reward-node] (R1) [below of=S1, xshift=-1.0cm] {$r_1$};
                    \node[reward-node] (R2) [below of=S1, xshift=1.0cm]  {$r_2$};
                    
                    \draw[bend right] (S1) to node[left, pos=0.8] {$p_1$} (R1);
                    \draw[bend left]  (S1) to node[right, pos=0.8] {$q_1$} (R2);
                    
                    \draw[hidden-edge, bend right] (S1) to node[action-label] {$a_1$} (R1);
                    \draw[hidden-edge, bend left]  (S1) to node[action-label] {$a_2$} (R2);
                \end{tikzpicture}
                \caption{Tempo $t = n$}
            \end{subfigure}
            \begin{subfigure}{.3\linewidth}
                \centering
                \begin{tikzpicture}[-,>=stealth', auto, node distance=1.5cm, thick]
                    \node[state-node]  (S1) {$s$};
                    \node[reward-node] (R1) [below of=S1, xshift=-1.0cm] {$r_1$};
                    \node[reward-node] (R2) [below of=S1, xshift=1.0cm]  {$r_2$};
                    
                    \draw[bend right] (S1) to node[left, pos=0.8] {$p_2$} (R1);
                    \draw[bend left]  (S1) to node[right, pos=0.8] {$q_2$} (R2);
                    
                    \draw[hidden-edge, bend right] (S1) to node[action-label] {$a_1$} (R1);
                    \draw[hidden-edge, bend left]  (S1) to node[action-label] {$a_2$} (R2);
                \end{tikzpicture}
                \caption{Tempo $t = n+1$}
            \end{subfigure}
            \caption{Exemplo de problema não-estacionário}
            \label{diag:non-stationary}
        \end{figure}

        No caso dos problemas não-estacionários, o cálculo da estimativa em \eqref{eq:estimativa-qn} não é adequada, pois ela parte do fato que os valores das ações serão sempre os mesmos. Em um problema que muda com o tempo, faz sentido dar mais importância às informações mais recentes. Para tanto, em vez de usar um passo baseado no número de vezes que a ação foi executada e que diminui cada vez que isso acontece, $\frac{1}{n}$, podemos utilizar um passo constante, $\alpha$. O novo cálculo da estimativa será:
        \begin{equation}
            Q_{n+1} \ \dot{=} \ Q_n + \alpha \Big[ R_n - Q_n \Big]
        \end{equation}

        Observe que esse novo cálculo implica em uma média ponderada das recompensas recebidas e da estimativa inicial, em que as recompensas mais recentes terão mais peso:
        \begin{subequations}
        \begin{align}
            Q_{n+1} & \ \dot{=} \ Q_n + \alpha \Big[ R_n - Q_n \Big] \nonumber\\
            & \ = \ Q_n + \alpha R_n - \alpha Q_n \nonumber\\
            & \ = \ \alpha R_n + (1 - \alpha) Q_n \nonumber\\
            & \ = \ \alpha R_n + (1 - \alpha) \Big[ \alpha R_{n-1} + (1 - \alpha) Q_{n-1} \Big] \nonumber\\
            & \ = \ \alpha R_n + \alpha (1 - \alpha) R_{n-1} + (1 - \alpha)^2 Q_{n-1} \nonumber\\
            & \ = \ \alpha R_n + \alpha (1 - \alpha) R_{n-1} + (1 - \alpha)^2 \Big[ \alpha R_{n-2} + (1 - \alpha) Q_{n-2} \Big] \nonumber\\
            & \ = \ \alpha R_n + \alpha (1 - \alpha) R_{n-1} + \alpha (1 - \alpha)^2 R_{n-2} + (1 - \alpha)^3 Q_{n-3} \nonumber\\
            & \ = \ (1 - \alpha)^3 Q_{n-3} + \alpha R_n + \alpha (1 - \alpha) R_{n-1} + \alpha (1 - \alpha)^2 R_{n-2} \nonumber\\
            & \ = \ (1 - \alpha)^n Q_1 + \alpha R_n + \alpha (1 - \alpha) R_{n-1} + \cdots + \alpha (1 - \alpha)^{n-1} R_1 \nonumber\\
            & \ = \ (1 - \alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n - i} R_i
        \end{align}
        \end{subequations}
        Essa nova estimativa recebe o nome de \emph{média ponderada recente}.
        
        %\subsection{Escolha dos valores iniciais}
            
            %\subsubsection{Valores otimistas}
            
    \section{Tarefas Não-Associativas, Associativas e o Problema Completo do Aprendizado por Reforço}

        \subsection{Nonassociative}
            
            Todos os exemplos vistos nessa nota foram de tarefas \emph{não-associativas}. Uma tarefa \emph{não-associativa} é uma tarefa em que não há mudança de configuração no ambiente. Assim, após uma ação ser escolhida, o agente recebe a recompensa adequada e volta para o início do problema. Por exemplo:            
            \begin{center}
                \simplebandit
            \end{center}
            
            Para resolver um problema não-associativo podemos utilizar as técnicas de avaliação das ações vistas anteriormente.
    
        \subsection{Associative}
        
            Em um problema \emph{associativo} o ambiente pode mudar após uma ação ser escolhida. Nesse caso, existem diversas configurações de ambientes diferentes e uma delas será aquela a ser avaliada no momento. Observe que a escolha de uma ação não influencia na configuração encontrada na iteração seguinte.
    
            \begin{center}
                \associativebandits
            \end{center}
            
            Para resolver esse tipo de problema não podemos utilizar as técnicas de avaliação da função de valor da ação, pois o ambiente pode mudar completamente na iteração seguinte. Dessa forma, é necessário que haja uma informação adicional, chamada de \emph{contexto}, sobre qual ambiente está sendo jogado agora, para que possamos fazer um mapeamento de cada um deles e assim conseguir uma resposta ótima. Esse tipo de mapeamento se chama \emph{política} e será explorado no futuro.

            Por exemplo, considere que há uma máquina que pode assumir três configurações diferentes, $MAB_1$, $MAB_2$ e $MAB_3$, cada um com duas ações possíveis. Suponha ainda que nós sabemos o valor de cada ação de cada configuração:
            \begin{equation*}
            \begin{split}
                q_{*,1}(a_1) & = 5\\
                q_{*,1}(a_2) & = 1\\
                q_{*,2}(a_1) & = 2\\
                q_{*,2}(a_2) & = 10\\
                q_{*,3}(a_1) & = 0.5\\
                q_{*,3}(a_2) & = 0.5
            \end{split}
            \end{equation*}
            No $MAB_1$ deveríamos executar a ação $a_1$. No $MAB_2$ deveríamos executar a ação $a_2$. No $MAB_3$ poderíamos executar qualquer ação.
            
            Se não soubermos em qual $MAB$ estamos no momento, saber os valores das ações não adianta muito. Entretanto, se tivermos informações adicionais, ou seja, um contexto, sobre o $MAB$ atual, poderíamos executar a ação gulosa para ele. Digamos que cada um dos $MAB$ mostre uma cor diferente em um display. A informação de cores representa um contexto que nos dá informação suficiente para identificar a configuração atual e então executar as ações de maior valor.
            
        \subsection{Full RL Problem}
        
            O \emph{problema completo de aprendizado por reforço} é um passo além de um não-associativo, pois cada ação levará a um determinado ``ambiente''. 
        
            \begin{center}
                \fullrldiagram
            \end{center}
            
            Nesse caso, não precisamos de uma informação adicional sobre o ambiente/estado em que estamos, já que essa informação pode ser encontrada diretamente a partir dos pares (estado, ação). Mas ainda é necessário mapear as melhores ações a partir de cada estado e para isso utilizaremos uma \emph{política}. É justamente sobre o problema completo do aprendizado por reforço que trataremos a seguir. Mais especificamente sobre sua formalização como um \emph{Processo de Decisão de Markov}.

\end{document}
