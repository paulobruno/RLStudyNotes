\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2.5cm]{geometry}

\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{subcaption}

\usepackage{amsmath,amssymb}
\DeclareMathOperator*{\argmax}{argmax}

\input{pb_rlstyle_tikzgraph.tex}

% text spacing
\linespread{1.3}
\setlength{\parindent}{4em}
\setlength{\parskip}{0.75em}


\newcommand{\todo}[1]{ --\textcolor{red}{\textbf{#1}}--}
%\newcommand{\todo}[1]{}


% PB: redefinir maketitle
\makeatletter
\def\@maketitle
{
    \begin{flushleft}
        \let \footnote \thanks
        {\Large \textbf{\@title} \par}
        %\vskip 0.5em
        {\large \textbf{\@author} \par}
        %\vskip 0.5em
        {\large \textit{\@date}}
    \end{flushleft}
    \par
    \vskip 1.5em
}
\makeatother


\title{Notas de estudo - Aprendizado por Reforço}
\author{Parte 02 - Problemas \emph{Multi-Armed Bandits}}
\date{Paulo Bruno Serafim - Fev/20}


\begin{document}

\maketitle

    \section{\textit{Multi-armed bandit}}
    
        \subsection{\emph{One-armed bandit}}
        
            Vamos considerar o caso mais simples, em que só temos um estado e uma ação. Apesar da ação ser única, ela pode dar recompensas diferentes com probabilidades diferentes. Na literatura, esse problema é conhecido como \emph{one-armed bandit}, cuja tradução para o português é "máquina caça-níquel". Assim, executar a ação seria equivalente a puxar a alavanca de uma caça-níquel e receber alguma recompensa de acordo com uma distribuição de probabilidade interna da máquina.

            Por exemplo, suponha que ao executar a ação temos $80\%$ de chance receber $0$, $19\%$ de receber $2$ e $1\%$ de receber $50$. Ao longo dessas notas, representaremos esses problemas através de diagramas que mostrem estados, ações, recompensas e probabilidades. Nesse exemplo, o diagrama é:
            
            \begin{center}
            \begin{tikzpicture}[-,>=stealth', auto, node distance=1.5cm, thick]
                \node[state-node]  (SimpleBanditS1) {$s$};
                \node[action-node] (SimpleBanditA1) [below of=SimpleBanditS1, yshift=0.5cm] {$a$};
                \node[reward-node] (SimpleBanditR1) [below of=SimpleBanditA1, xshift=-1.5cm] {$0$};
                \node[reward-node] (SimpleBanditR2) [below of=SimpleBanditA1, xshift=0.0cm]  {$2$};
                \node[reward-node] (SimpleBanditR3) [below of=SimpleBanditA1, xshift=1.5cm]  {$50$};
                
                \draw[] (SimpleBanditS1) to node[] {} (SimpleBanditA1);
                \draw[bend right] (SimpleBanditA1) to node[left] {$80\%$} (SimpleBanditR1);
                \draw             (SimpleBanditA1) to node[left]     {$19\%$} (SimpleBanditR2);
                \draw[bend left]  (SimpleBanditA1) to node[]      {$1\%$} (SimpleBanditR3);
            \end{tikzpicture}
            \end{center}

            O que queremos saber é: qual é o \emph{valor} da ação $a$? Por definição, o valor de uma ação é o valor esperado das recompensas recebidas quando a ação escolhida $A$ é $a$:
            \begin{equation}
            \begin{split}
                q_*(a) & \ = \ \mathbb{E}[R \mid A = a] \\
                & \ = \ \sum_{r}{} p(r) \cdot r
            \end{split}
            \end{equation}        
            Para o exemplo anterior, temos:            
            \begin{equation*}
            \begin{split}
                q_*(a) & \ = \ \sum_{r}{} p(r) \cdot r \\
                & = \ 0.8 \cdot 0 + 0.19 \cdot 2 + 0.01 \cdot 50 \\
                & = \ 0.88
            \end{split}
            \end{equation*}
            
        \subsection{\emph{Multi-armed bandit}}
    
            Vamos agora considerar o caso em que estamos diante de múltiplas máquinas caça-níques. Nesse caso, escolher uma máquina significa ir até ela (estado) e puxar a sua alavanca (ação), de modo que estaremos em um único estado por vez. Esse problema é equivalente a ter um único estado, mas múltiplas ações possíveis. A dinâmica de interação agora envolve também escolher uma ação a ser executada. Portanto, precisamos nos atentar com qual ação executar imediatamente e quais as recompensas esperadas, ou seja, o seu valor. Se existirem $k$ ações possíveis, podemos executar qualquer uma delas e receberemos alguma recompensa de acordo com uma distribuição de probabilidade. O diagrama a seguir representa essa dinâmica:
        
            \begin{center}
                \simplebandit
            \end{center}
                    
            Note que nesse caso podemos considerar que estamos diante de uma máquina caça-níquel com múltiplas alavancas, ou seja, uma \emph{multi-armed bandit}. Esse é um problema bem conhecido e muito estudado na literatura, muitas vezes referido somente como \emph{MAB}. Imagine que você vá jogar em uma caça-níquel com múltiplas alavancas. Cada máquina dará resultados diferentes com probabilidades diferentes, mas você tem um documento contendo as recompensas possíveis e suas probabilidades para cada máquina. Você só pode jogar uma única vez. Qual máquina você escolheria?
        
            A escolha ótima, i.e., a escolha racional que potencialmente dará o maior valor, é aquela cujo retorno esperado é o maior. Em outras palavras, você deve escolher a ação cujo valor seja o maior. 

            Por exemplo, considere o problema:

            \begin{center}
            \begin{tikzpicture}[-, >=stealth', auto, node distance=1.5cm, thick]
                \node[state-node] (S1) {$s$};
                \node[action-node] (A1) [below of=S1, xshift=-3.5cm] {$a_1$};
                \node[action-node] (A2) [below of=S1, xshift=0.0cm]  {$a_2$};
                \node[action-node] (A3) [below of=S1, xshift=3.5cm]  {$a_3$};                
                \node[reward-node] (A1R1) [below of=A1, xshift=-1.5cm] {$0$};
                \node[reward-node] (A1R2) [below of=A1, xshift=0.0cm]  {$2$};
                \node[reward-node] (A1R3) [below of=A1, xshift=1.5cm]  {$50$};
                \node[reward-node] (A2R1) [below of=A2, xshift=-1.0cm] {$0$};
                \node[reward-node] (A2R2) [below of=A2, xshift=1.0cm]  {$5$};
                \node[reward-node] (A3R1) [below of=A3, xshift=-1.5cm] {$0.5$};
                \node[reward-node] (A3R2) [below of=A3, xshift=0.0cm]  {$1$};
                \node[reward-node] (A3R3) [below of=A3, xshift=1.5cm]  {$2$};
                
                \draw[bend right] (S1) to node[] {} (A1);
                \draw[]           (S1) to node[] {} (A2);
                \draw[bend left]  (S1) to node[] {} (A3);

                \draw[bend right]    (A1) to node[left]  {$0.80$} (A1R1);
                \draw                (A1) to node[left]  {$0.19$} (A1R2);
                \draw[bend left]     (A1) to node[right] {$0.01$} (A1R3);
                \draw[bend right]    (A2) to node[right] {$0.9$}  (A2R1);
                \draw[bend left]     (A2) to node[right] {$0.1$}  (A2R2);
                \draw[bend right]    (A3) to node[left]  {$0.5$}  (A3R1);
                \draw                (A3) to node[left]  {$0.3$}  (A3R2);
                \draw[bend left]     (A3) to node[right] {$0.2$}  (A3R3);
            \end{tikzpicture}
            \end{center}
            Qual ação escolher, $a_1$, $a_2$ ou $a_3$? Naturalmente, vamos calcular o valor de cada ação e a partir deles tomar alguma decisão. A decisão natural é escolher a ação com maior valor, que é chamada de ação \emph{gulosa}:
            \begin{equation}
                A_{greedy} \ = \ \argmax_a q_*(a)
            \end{equation}
            
            Calculando os valores das ações do exemplo anterior, temos:
            \begin{equation*}
            \begin{split}
                q_*(a_1) \ = \ 0.88 \\
                q_*(a_2) \ = \ 0.50 \\
                q_*(a_3) \ = \ 0.95
            \end{split}
            \end{equation*}
            Portanto, a ação gulosa é $a_3$.
    
        \subsection{Estimativa do valor da ação}

            Até agora todos as probabilidades e recompensas eram conhecidas. Se pensarmos em um caso mais real, ao encontrarmos $k$ máquinas saberíamos que temos $k$ ações possíveis, mas sem conhecer as probabilidades e recompensas de cada uma delas. De que modo poderíamos descobrir qual é a melhor ação após interagir com elas?

            Inicialmente, não temos nenhuma informação sobre o valor, então qualquer ação é iguamente boa. Para descobrir informações de cada ação teríamos que executá-las algumas vezes e, a partir daí, ter alguma informação para definir uma estratégia de escolha adequada. Vamos definir essa estimativa como a média das recompensas recebidas até o tempo $t$ após executar uma ação, dada por:
            \begin{equation}
            \label{eq:estimativa-acao}
                Q_t(a) \ = \ \frac{\sum\limits_{i=1}^{t-1} R_i(a)}{N_{t-1}(a)}
            \end{equation}
            onde $r_i(a)$ é a recompensa recebida por executar $a$ no tempo $i$, ou $0$ se $a$ não foi executada em $i$, e $N_{t-1}(a)$ é o número de vezes que $a$ foi executada até o tempo $t-1$. Assim, à medida que as interações forem ocorrendo, teremos uma estimativa melhor do valor real de uma ação. Com $t$ tendendo ao infinito, $Q_t$ tende ao valor real $q_*$.

            Considere um MAB com duas ações. Definimos uma estratégia em que as três primeiras jogadas serão na ação $a_1$ e as três jogadas seguinte serão em $a_2$, de forma a obter algum informação do valor delas. Para cada passo vamos calcular o valor estimado de cada ação. Suponha que obtivemos os seguintes resultados:
            \begin{tabbing}
                \hspace{4.0cm}\=~~~~~~~~~~~~ \=~~~~~~~~~~~~ \=~~~~~~~~~~~~ \=~~~~~~~~~~~~~~~~~~~ \= \kill
                \> $k = 1$   \> $a = 1$   \> $r = 1$   \> $Q_1(a_1) = 1$           \> $Q_1(a_2) = 0$\\
                \> $k = 2$   \> $a = 1$   \> $r = 3$   \> $Q_2(a_1) = 2$           \> $Q_2(a_2) = 0$\\
                \> $k = 3$   \> $a = 1$   \> $r = 1$   \> $Q_3(a_1) = \frac{5}{3}$ \> $Q_3(a_2) = 0$\\
                \> $k = 4$   \> $a = 2$   \> $r = 0$   \> $Q_4(a_1) = \frac{5}{3}$ \> $Q_4(a_2) = 0$\\
                \> $k = 5$   \> $a = 2$   \> $r = 0$   \> $Q_5(a_1) = \frac{5}{3}$ \> $Q_5(a_2) = 0$\\
                \> $k = 6$   \> $a = 2$   \> $r = 0$   \> $Q_6(a_1) = \frac{5}{3}$ \> $Q_6(a_2) = 0$
            \end{tabbing}
            O que podemos inferir desses dados é que em $a_1$ duas recompensas são possíveis, $1$ com $\frac{2}{3}$ de probabilidade e $3$ com $\frac{1}{3}$ de probabilidade, de modo que $Q_6(a_1) = \frac{5}{3}$. Já para $a_2$ temos recompensa $0$ com $100\%$ de probabilidade, de modo que $Q_6(a_2) = 0$. Portanto, a ação gulosa nesse caso seria $a_1$.
            
            Suponha que jogamos mais quatro vezes, duas em $a_1$ e duas em $a_2$, e obtivemos os resultados:
            \begin{tabbing}
                \hspace{4.0cm}\=~~~~~~~~~~~~ \=~~~~~~~~~~~~ \=~~~~~~~~~~~~ \=~~~~~~~~~~~~~~~~~~~ \= \kill
                \> $k = 7$   \> $a = 1$   \> $r = 1$   \> $Q_7(a_1) = 1.5$  \> $Q_7(a_2) = 0$\\
                \> $k = 8$   \> $a = 1$   \> $r = 3$   \> $Q_8(a_1) = 1.8$  \> $Q_8(a_2) = 0$\\
                \> $k = 9$   \> $a = 2$   \> $r = 1000$\> $Q_9(a_1) = 1.8$  \> $Q_9(a_2) = 250$\\
                \> $k = 10$  \> $a = 2$   \> $r = 0$   \> $Q_{10}(a_1) = 1.8$ \> $Q_{10}(a_2) = 200$
            \end{tabbing}
            Os novos valores das estimativas são:
            \begin{equation*}
            \begin{split}
                Q_{10}(a_1) \ = \ 1.8\\
                Q_{10}(a_2) \ = \ 200
            \end{split}
            \end{equation*}
            Portanto, a ação gulosa nesse caso seria $a_2$. 
            
            A título de informação, a configuração exata que eu pensei desse MAB é:
            \begin{center}
            \begin{tikzpicture}[-, >=stealth', auto, node distance=1.5cm, thick]
                \node[state-node] (S1) {$s$};
                \node[action-node] (A1) [below of=S1, xshift=-2.5cm] {$a_1$};
                \node[action-node] (A2) [below of=S1, xshift=2.5cm]  {$a_2$};

                \node[reward-node] (A1R1) [below of=A1, xshift=-1.0cm] {$3$};
                \node[reward-node] (A1R2) [below of=A1, xshift=1.0cm]  {$1$};
                \node[reward-node] (A2R1) [below of=A2, xshift=-1.0cm] {$0$};
                \node[reward-node] (A2R2) [below of=A2, xshift=1.0cm]  {$1000$};
                
                \draw[bend right] (S1) to node[] {} (A1);
                \draw[bend left]  (S1) to node[] {} (A2);

                \draw[bend right]    (A1) to node[left]  {$0.25$} (A1R1);
                \draw[bend left]     (A1) to node[right] {$0.75$} (A1R2);
                \draw[bend right]    (A2) to node[left]  {$0.99$} (A2R1);
                \draw[bend left]     (A2) to node[right] {$0.01$} (A2R2);
            \end{tikzpicture}
            \end{center}
            com:
            \begin{equation*}
            \begin{split}
                q_*(a_1) \ = \ 0.25 \cdot 3 + 0.75 \cdot 1 = 1.5 \\
                q_*(a_2) \ = \ 0.99 \cdot 0 + 0.01 \cdot 1000 = 10
            \end{split}                
            \end{equation*}

            A estratégia que definimos de executar a ação $a_1$ nas primeiras $n$ jogadas e a ação $a_2$ nas $n$ jogadas seguintes, ou, de maneira mais geral, escolher ações sem se basear na estimativa e em seguida definir as ações gulosas, pode parecer boa. Contudo, um dos problemas dessa estratégia é o que acontece se uma recompensa tiver baixa probabilidade, como em $a_2$. 

            Não seria difícil acontecer de uma recompensa com $1\%$ de probabilidade não ocorrer nas primeiras 15 execuções, por exemplo. Assim, poderíamos acreditar que $q_*(a_1) > q_*(a_2)$, o que não seria verdade. Dessa forma, sempre executar a ação gulosa após poucas estimativas iniciais, ou seja, explorar o resultado já conhecido, pode não ser bom. Uma maneira de contornar esse problema seria eventualmente executar ações de menor estimativa, ou seja, prospectar novos resultados, de modo que teríamos mais chances de ver que $q_*(a_2) > q_*(a_1)$.

            O exemplo anterior ilustra bem o quão importante é ter mais informações sobre cada estado para não fazer escolhas inadequadas. Além disso, evidencia a dúvida de quando parar de prospectar informações e passar a explorar os resultados de maneira gulosa. Na literatura, esse problema é conhecido como o \emph{Dilema Prospecção vs. Exploração}.
    
    \section{Dilema \textit{``Exploration vs. Exploitation''}}
    
        Pegando emprestado um exemplo \todo{procurar onde vi esse exemplo, provavelmente sutton}, imagine que você goste bastante de uma lanchonete que tem um ótimo hambúrguer. Você pode continuar sempre indo na mesma lanchonete. Você sempre vai comer um ótimo hambúrguer, mas pode ser que exista outra lanchonete que tenha um hambúrguer ainda melhor, e você não vai saber. Você poderia decidir, por exemplo, nunca repetir uma lanchonete, assim você conheceria ótimas lanchonetes com ótimos hambúrguers, mas algumas vezes iria comer sanduíches ruins.
        
        Esse exemplo ilustra o dilema entre sempre escolher uma boa opção (a mesma lanchonete), ou seja, \textbf{explorar} aquilo que já é conhecido, e prospectar (mudar de lanchonete), ou seja, testar outras opções ainda não verificadas. 
    
        \subsection{Definições}
        
            \subsubsection{\textit{``Exploration''} = Prospecção}
            
            \subsubsection{\textit{``Exploitation''} = Exploração}
        
        \subsection{Estratégias}
        
            As estratégias para lidar com o dilema anterior se baseiam em executar ações de maneira gulosa na maior parte do tempo, mas ainda executar ações não-gulosas algumas vezes.
        
            \subsubsection{Estratégias $\boldsymbol\varepsilon$}
            
                \begin{itemize}
                    \item \textbf{$\boldsymbol\varepsilon$-greedy}: executa a ação gulosa em uma porcentagem $(1-\varepsilon)$ e executa uma ação aleatória $\varepsilon$.
                    \item \textbf{$\boldsymbol\varepsilon$-first}: para um conjunto N de ações, executa $(1 - \varepsilon N)$ ações gulosas e executa $(\varepsilon N)$ ações não-gulosas.
                    \item \textbf{$\boldsymbol\varepsilon$-decay}: No início do aprendizado não há ainda uma boa distribuição das recompensas, de modo que os valores tendem a ser menos confiáveis, assim faz sentido prospectar ações. Após algumas iterações a recompensa já terá sido mais distribuída, de modo que faz sentido termos uma fase de exploração maior. Após várias iterações a distribuição de recompensa será bem mais confiável, logo a fase de exploração deverá ser máxima. Essa estratégia é representada usando um $\varepsilon_{I}$ alto nas primeiras $I$ iterações (possivelmente $1$), um $\varepsilon_{F}$ bem baixo a partir da iteração $F$ e um $\varepsilon$ que diminui (decai) de $\varepsilon_{I}$ a $\varepsilon_{F}$ entre $I$ e $F$. O tipo de decaimento mais comum é o linear, representado na Figura \ref{diag:epsilon-decay}, com $\varepsilon_{I} = 1.0$, $\varepsilon_{F} = 0.1$, $I = 5$ e $F = 30$ \todo{checar se I e F devem ser 5 e 30 ou 6 e 31}.
                    
                    \begin{figure}[ht]
                        \centering
                        \begin{tikzpicture}
                            \begin{axis}[
                                height=4.5cm,
                                width=6.0cm,
                        		xlabel=\textbf{\normalsize Épocas},
                        		ylabel=\Large$\boldsymbol{\varepsilon}$,
                        		xmin=0, xmax=50,
                        		ymin=0.0, ymax=1.1,
                        		xtick={0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50},
                        		ytick={0.0, 0.2, 0.4, 0.6, 0.8, 1.0},
                        		tick align=center,
                        		xtick pos=bottom,
                        		ytick pos=left,
                        		yticklabel style={/pgf/number format/.cd, fixed, fixed zerofill, precision=1},
                        		font=\tiny, 
                                no markers,
                                every axis plot/.append style={ultra thick},
                            ]
                                \addplot[color=black] coordinates {
                            		( 0, 1.0)
                            		( 5, 1.0)
                            		(30, 0.1)
                            		(50, 0.1)
        	                    };
                            \end{axis}
                        \end{tikzpicture}
                        
                        \caption{$\varepsilon$ começa em $1.0$ e decai linearmente até $0.1$.}
                        \label{diag:epsilon-decay}
                    \end{figure}
                \end{itemize}
            
        \subsubsection{Limite de Confiança Superior (UCB)}
        
            Outra estratégia bastante utilizada leva em consideração o quanto cada ação não-gulosa foi escolhida/testada. Em vez de escolher uma ação não-gulosa de acordo uma mesma probabilidade, vamos escolher uma ação não-gulosa de acordo com o seu \emph{potencial} em ser ótima.
            
            Dois fatores influenciam a estimativa de uma ação. O primeiro, naturalmente, é o valor estimado e o segundo é a certeza que temos de que o primeiro é uma boa estimativa. Por exemplo, estimativas com uma grande incerteza associada podem ser bem maiores ou bem menores do que o estimado.
            
            Assim, podemos adicionar um termo que considere também a incerteza do valor estimado, de modo que a escolha de uma ação passe a ser baseada em:
            
            \begin{equation}
                A_t \ = \ \argmax_a \left[ Q_t(a) + Incerteza(Q_a) \right]
            \end{equation}
            
            No caso do Limite de Confiação Superior (\emph{Upper Confidence Bound}, UCB), a incerteza é dada por:
            
            \begin{equation}
                Incerteza(Q_a) \ = \ c \cdot \sqrt{\frac{\ln{t}}{N_t(a)}}
            \end{equation}
            
            E, portanto, a escolha de uma ação é dada por:
            
            \begin{equation}
                A_t \ \dot{=} \ \argmax_a \left[ Q_t(a) + c \cdot \sqrt{\frac{\ln{t}}{N_t(a)}} \ \right]
            \end{equation}
            
            \noindent
            onde $c$ é o \emph{nível de confiança} e $N_t(a)$ é o número de vezes que a ação $a$ foi escolhida até o tempo $t$. Observe que quando $a$ for escolhida, o valor da incerteza diminui, pois $N_t(a)$ e $\ln{t}$ aumentam, e se $a$ não for escolhida o valor da incerteza aumenta, pois $N_t(a)$ se mantém e $\ln{t}$ aumenta.
            
            Note que se $Incerteza(Q_a) = 0$, então $A_t$ é o valor real da ação $a$. Se $Incerteza(Q_a) > 0$, então o valor de $a$ pode ser maior do que $Q_t(a)$, mas no máximo $A_t$. Portanto, o termo de incerteza define o limite superior do valor verdadeiro de $a$.
            
    \section{Função valor-ação}
    
        Função valor-ação ótima:
    
        \begin{equation}
            q_*(a) \ \dot{=} \ \mathbb{E}[R_t \mid A_t = a]
        \end{equation}
    
        Escolha gulosa:
    
        \begin{equation}
            A_t \ \dot{=} \ \argmax_a Q_t(a)
        \end{equation}
        
        \begin{center}
        \mdpthreestate
        \end{center}
        
        Para simplificar o grafo, na maioria das vezes os nós das ações serão omitidos e as ações serão indicadas nos rótulos das arestas. Além disso, se a probabilidade for 1, ela é omitida.
        
        \subsection{Definição}
        
        \subsection{Versão incremental}
            
            \begin{equation}
                Q_n \ \dot{=} \ \frac{R_1 + R_2 + \cdots + R_{n-1}}{n - 1}
            \end{equation}
            
            \begin{equation}
            \begin{split}
                Q_{n+1} & \ \dot{=} \ \frac{1}{n} \sum_{i=1}^{n} R_i \\
                & = \ Q_n + \frac{1}{n} \Big[ R_n - Q_n \Big]
            \end{split}
            \end{equation}
            
            Ver no livro a derivação dessa equação (eq. 2.3).
            
            \begin{equation}
                NovaEstimativa \leftarrow AntigaEstimativa + TamanhoPasso \Big[ Objetivo - AntigaEstimativa \Big]
            \end{equation}
            
            \begin{equation}
            \begin{split}
                Q_{n+1} & \ \dot{=} \ Q_n + \alpha \Big[ R_n - Q_n \Big] \\
                & = \ (1 - \alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n - i} R_i
            \end{split}
            \end{equation}
            
        \subsection{Escolha dos valores iniciais}
            
            Obs.: citar a diferença entre problemas estacionários e não-estacionários
            
            \subsubsection{Valores otimistas}
            
            \subsubsection{Limite Superior de Confiança (UCB)}
            
    \section{Tarefas \todo{ou problema?, verificar texto todo} Não-Associativas, Associativas e o Problema Completo do Aprendizado por Reforço}
    
        \subsection{Nonassociative}
        
            Um problema \emph{não-associativo} é basicamente um problema em que não uma mudança de ambiente \todo{configuração seria melhor? verificar no sutton}. Assim, após uma ação ser escolhida, o agente recebe a recompensa adequada e volta para o início do problema. 
            
            \begin{center}
                \simplebandit
            \end{center}
            
            Para resolver um problema não-associativo utilizamos as técnicas de avaliação das ações discutidas anteriormente.
    
        \subsection{Associative}
        
            Em um problema \emph{associativo} o ambiente pode mudar após uma ação ser escolhida. Nesse caso, existem diversas configurações de ambientes diferentes e uma delas será aquela a ser avaliada no momento. Observe que a escolha de uma ação não influencia na configuração encontrada na iteração seguinte.
    
            \begin{center}
                \associativebandits
            \end{center}
            
            Para resolver esse tipo de problema não podemos utilizar as técnicas de avaliação da função de valor da ação, pois o ambiente pode mudar completamente na iteração seguinte. Dessa forma, é necessário que haja uma informação adicional sobre qual ambiente está sendo jogado agora, para que possamos fazer um mapeamento de cada um deles, assim podemos ter uma resposta ótima para cada. Esse tipo de mapeamento se chama \emph{política} e será explorado no futuro.
            
        \subsection{Full RL Problem}
        
            O \emph{problema completo de aprendizado por reforço} é um passo além de um não-associativo, pois cada ação levará a um determinado ``ambiente''. 
        
            \begin{center}
                \fullrldiagram
            \end{center}
            
            Nesse caso, não precisamos de uma informação adicional sobre o ambiente/estado em que estamos, já que essa informação pode ser encontrada diretamente a partir dos pares (estado, ação). Mas ainda é necessário mapear as melhores ações a partir de cada estado e para isso utilizaremos uma \emph{política}. É justamente sobre o problema completo do aprendizado por reforço que trataremos a seguir. Mais especificamente sobre sua formalização como um \emph{Processo de Decisão de Markov}.
            
        \subsection{Problema estacionário}
    
            Por exemplo, um simples problema não-estacionário pode ser visto nos diagramas abaixo. Note que $p_1 \neq p_2$ e $q_1 \neq q_2$, caso contrário seria um problema estacionário. 
    
            \begin{figure}[h]
                \centering
                \begin{subfigure}{.3\linewidth}
                    \centering
                    \begin{tikzpicture}[-,>=stealth', auto, node distance=1.5cm, thick]
                        \node[state-node]  (S1) {$s$};
                        \node[reward-node] (R1) [below of=S1, xshift=-1.0cm] {$r_1$};
                        \node[reward-node] (R2) [below of=S1, xshift=1.0cm]  {$r_2$};
                        
                        \draw[bend right] (S1) to node[left, pos=0.8] {$p_1$} (R1);
                        \draw[bend left]  (S1) to node[right, pos=0.8] {$q_1$} (R2);
                        
                        \draw[hidden-edge, bend right] (S1) to node[action-label] {$a_1$} (R1);
                        \draw[hidden-edge, bend left]  (S1) to node[action-label] {$a_2$} (R2);
                    \end{tikzpicture}
                    \caption{Momento 1}
                \end{subfigure}
                \begin{subfigure}{.3\linewidth}
                    \centering
                    \begin{tikzpicture}[-,>=stealth', auto, node distance=1.5cm, thick]
                        \node[state-node]  (S1) {$s$};
                        \node[reward-node] (R1) [below of=S1, xshift=-1.0cm] {$r_1$};
                        \node[reward-node] (R2) [below of=S1, xshift=1.0cm]  {$r_2$};
                        
                        \draw[bend right] (S1) to node[left, pos=0.8] {$p_2$} (R1);
                        \draw[bend left]  (S1) to node[right, pos=0.8] {$q_2$} (R2);
                        
                        \draw[hidden-edge, bend right] (S1) to node[action-label] {$a_1$} (R1);
                        \draw[hidden-edge, bend left]  (S1) to node[action-label] {$a_2$} (R2);
                    \end{tikzpicture}
                    \caption{Momento 2}
                \end{subfigure}
                \caption{Teste}
            \end{figure}
        
\end{document}
